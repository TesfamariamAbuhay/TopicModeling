ProcediaComputer
Computer Science
Science 00
(2009) 000â€“000
Procedia
1 (2012)
2187â€“2196

Procedia
Computer
Science
www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Models for strategy choice in random or potentially deceptive
environments
Burton Voorhees*
Faculty of Science and Technology, Athabasca University, 1 University Drive, Athabasca, Alberta, CANADA T9S 3A3

Abstract

Two simulation models are presented illustrative of decision and behavioural choice issues for complex
adaptive systems involving cognitive agents. The first illustrates the value of managed instability for
complex systems. It explores conditions under which a system able maintain itself in an unstable state
gains an advantage in behavioural flexibility offsetting the extra expense involved in maintaining that state
against instability. The second model considers the trade-off between quickness and accuracy of response
in risk/reward identification, in order to differentiate conditions under which rapid or considered responses
are favoured. The second model is currently being used to distinguish cases where cognitive agents are
preferable to non-cognitive agents and vice versa.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
âƒ
decision strategies, behavioural flexibility, virtual stability, fight/flight response

1. Introduction
In traditional scientific method systems chosen for analysis are simple. There are few variables, easily separated
into dependent and independent classes, and there are a sufficient variety of experimental and/or observational
procedures so that correlations between independent and dependent variables can be determined. In practice, the
restriction to simple systems means primary emphasis is placed on two types of system: those having only a few
variables, which can be treated directly; and those having a very large number of variables, treated
thermodynamically or statistically. In contrast, the digital computer makes possible a computational science of
complex systems having an intermediate number of variables, conceptualized in terms of interactions between
agents with learning and even self-referential capacities. One tool in this sort of study is the toy model.
Toy models play a role in complex system theorizing equivalent to the simple solvable models used in classical
physics education (1). Well-constructed toy models can serve as exemplary cases, illustrating general principles,
providing points of reference, anchoring points, and test cases for understanding. This paper discusses two toy
models illustrative of issues that arise in decision situations where different response or behavioral strategies are
available but the preferred strategy is uncertain. The first model illustrates the advantage of managed instability, of

* Corresponding author. Tel.: 1-780-708-1814.
E-mail address: burt@athabascau.ca.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 âƒ
doi:10.1016/j.procs.2010.04.245

2188

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

not committing exclusively to any one behavior in an environment subject to random fluctuations, even though this
lack of commitment comes at a cost in terms, for example, of increased mortality rate. The capacity to function in
this way is an essential requirement for a cognitive agent. The second model explores conditions that favour one of
four strategies for response to risk/reward situations in which initial appearances may be deceptive. As presented in
this paper, the second model only assumes non-cognitive agents, but current work extends this to comparison of
cognitive and non-cognitive agents with the goal of differentiating cases in which the a non-cognitive agent is to be
preferred in order to avoid having to pay the extra cost of cognition.
2. Model 1: The Value of Equivocation in Random Environments
The concept of virtual stability defines a condition in which an agent uses self-monitoring and adaptive control to
maintain itself in a state that would otherwise be unstable, thereby gaining greater behavioural flexibility [2,3,4].
This concept also applies to the case of a population that is able to persist in a fluctuating environment by
maintaining a degree of instability in its spectrum of potential genetic or behavioural responses.
Full details and results of this model are presented in [3,4]. A fixed population of agents with three distinct
population types: A, B, and C, goes through a sequence of environmental states and the relative proportion of each
population type is tracked. There are three possible environmental states; labeled A, B, and N. In environment A
population type A is favored, in environment B population type B is favored. Environment N is neutral with respect
to population types A and B while population type C is never favored.
Each environmental state has an associated transition matrix with adjustable parameters. Transitions between the
A and B population types are rare but possible, but there are no transitions from the A or B population types to the C
population type. The C population can make transitions to subtypes AC and BC, representing C population
members acting as A or B types. Environment N represents the standard background to which types A and B have
become adapted while environments A and B represent random fluctuations about this standard.
A sequence of environmental states is generated with specified proportions of A, B, and N states. In a given
environmental state, a certain number of iterations through the population are carried out in which each member of
the population changes type, remain the same type, or die depending on probabilities set in the associated transition
matrix. At the end of this process, population members who have died are redistributed according to surviving
proportions of each population type and the program moves on to the next environmental state in the sequence.
Parameters in the transition matrices are adjusted to determine values for which the A/B populations, or the C
population dominates, and to discover transition points between these cases. Typical results are shown in Figures 1.
A

B

n = 10

A

C

B

n = 11

C

Fig. 1. Convergence to A/B (n = 10) or C (n = 11) dominance.
In this figure the parameter n being varied acts as a control parameter for the C-type population. Barycentric
coordinates are used; hence each vertex in the triangle represents the state in which the population consists entirely
of the corresponding population type. The initial population distribution is located at the barycenter of the triangle.

2189

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

The n = 10 case shows convergence to an oscillation between the A and B populations with the C population type
extinct. The n = 11 case shows convergence to the C population with A and B populations extinct. In addition to
the parameter n, parameters involving numbers of iterations through the population in an environmental state, and
transition probabilities for the three population types were tested. All showed patterns similar to that of Figure 1:
there is a transitional value separating convergence to the A/B type and the C type. Parameters can also be tuned so
all three population types are viable (for n this value is about 10.55).
In addition to varying transition matrix parameters, studies were conducted in which a large number of runs
(5150) are carried out with different proportions set for the environmental states, allowing determination of how the
spectrum of environmental fluctuations influences results. Typical results of these â€œenvironmental testsâ€ are shown
in Figure 2. In this figure barycentric coordinates are also used, but they label environmental states rather than
population types so that points in the triangle give the proportions of A, B, and N states in the environmental
sequence.
N
N

A

B
n=3

A

B
n = 30

Fig. 2. Environmental test results: Points above the boundary converge to A/B, points below converge to C.
The n = 3 case in Figure 2 indicate that the C population can win so long as there are not too many neutral
environmental states. It is important to realize that the environmental states represent random fluctuations since
complex systems are able to recognize and adapt to periodic or almost periodic change either genetically, or through
forecasting and planning. In this sense, the neutral environmental states represent cases in which no fluctuation
occursâ€”it is the environmental mean to which both A and B population types have adapted.
The n = 30 case in Figure 2 may seem strange since it indicates that the C population type can dominate even if
the environmental states always favour A or B. The C type out competes the A and B population types on their
home territory. This is a result of being able to set the control parameter n large enough that the probability of a CA
or CB population member remain CA or CB after two iterations of the program is greater than the probability of an
A member remaining A or a B member remaining B over two iterations.
3. Model 2: Strategies for Risk and Reward Decisions
This model simulates agents acting in an environment in which threats and opportunities for resource acquisition
arise following specified probabilities. Further, these events may be disguised so that a threat appears as an
opportunity and vice versa. In predator-prey terms, a predator may appear as such, or may be disguised as a prey
and a prey may appear as a prey or be disguised as a predator. The situation, for example, could be that faced by a
member of a prey species that forages for food, of by a trader in a financial market. Under such conditions an agent
will face four possible cases denoted DD (danger/danger), SD (safe/danger), DS (danger/safe), and SS (safe/safe).
Here the first designation refers to appearance and the second to actuality.

2190

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

The model uses a set of four possible behavioural strategies for response in these situations. These can be agent
specific, so that an agent always uses the same strategy in competition against other agents using different strategies,
or a mixed strategy may be employed. In work presented here, mixed and pure strategy choices are allowed, but
there is no allowance for learning or judgment, strategies are fixed. The four strategies considered are:
1. HT (Hair Trigger): Immediately avoid apparent danger and immediately pursue apparent safety. In other
words, this strategy reacts only to immediate appearances.
2. SF (Safety First): Immediately avoid apparent danger and pause briefly to test apparent safety.
3. CS (Cautious Sceptic): Pause to test both apparent danger and apparent safety.
4. CO (Cautious Optimist): Pause to test apparent danger but immediately pursue apparent safety.
In the most basic terms, an agent who succumbs to danger has a high probability of becoming a meal while an
agent who pursues actual safety has a high probability of obtaining a meal. To quantify the results arising from use
of a strategy, two functions are defined:

fD (t) =

1
1 e

fS (t) = 1 

aD t

(b

+e

D cD t

1

2

)t

(1)

(b c t ) t

1  eaSt + e

S

S

2

fD(t) is the probability of being eaten by time t in a situation of actual danger and fS(t) is the probability of obtaining
a meal in a situation of actual safety. The coefficients in these functions are chosen so that fD is an increasing
function of t with range [0,1] and fS is a decreasing function with the same range. Probabilities of being eaten or of
obtaining a meal are defined by these functions according to Table 1.
Table 1. Probabilities of being eaten (fD) or of obtaining a meal (fS) for each possible strategy in each environmental
situation.
Strategy/Situation
HT
SF
CS
CO

DD
fD(t0)
fD(t0)
fD(t1)
fD(t1)

SD
1- fD(t0)
fD(t1)
fD(t1)
1- fD(t0)

DS
0
0
fS(t1)
fS(t1)

SS
fS(t0)
fS(t1)
fS(t1)
fS(t0)

Here t0 is the immediate reaction time and t1 > t0 is the time required for testing the displayed appearance. From
equation (1) this leads to the following inequalities:
fD(t0) < fD(t1)
fS(t0) > fS(t1)

(2)

Taking extra time to test appearances increases the probability of being eaten in cases of actual danger and
decreases the probability of obtaining a meal in cases of actual safety. This is counter-balanced by the loss of
opportunity arising from avoidance of apparent danger when this appearance is deceptive, and the higher probability
of being eaten if apparent safety is pursued when this appearance is deceptive.
If (q1,q2,q3,q4) is the probability vector for encountering the respective environmental situations DD, SD, DS, and
SS while (p1,p2,p3,p4) is a mixed strategy vector for choice of the respective strategies HT, SF, CS, and CO then the
probabilities PE of being eaten and PM of having a meal are given by
PE = p1{fD(t0)q1 + [1-fD(t0)]q2} + p2[fD(t0)q1 + f D(t0)q2] + â€¦ + p3fD(t1)(q1 + q2)+ q4{fD(t1)q1 + [1-fD(t0)]q2}
PM = [p1fS(t0) + p2fS(t1)]q4 + p3 fS(t1)(q3 + q4) + p4[fS(t1)q3 + f S(t0)q4]

(4)

The function R = PM â€“ PE measures the advantage or disadvantage arising from choice of a particular mixed
strategy given the environmental distribution of situations described by q.

2191

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

R = {fS(t0)q4 â€“ fD(t0)q1 â€“ [1-fD(t0)]q2}p1 + [fS(t1)q4 â€“ fD(t0)q1 â€“ fD(t0)q2]p2 + â€¦
+ [fS(t1)(q3 + q4) â€“ f D(t1)(q1 + q2)]p3 + {fS(t1)q3 + fS(t0)q4 â€“ fD(t1)q1 â€“ [1-fD(t0)]q2 }p4

(5)

Since both p and q are probability vectors, the sum of their components is 1 and values of these components are
represented in barycentric coordinates by points in the 3-simplices p-3 and q-3. Further, if q is fixed then PE, PM,
and R are linear functions of p1, p2, p3, and p4 hence the maximum and minimum values will occur at faces, edges,
or vertices of p-3. The k-th vertex of this simplex is characterized by pk = 1, pj = 0, j  k. Vertex values of these
functions are given in Table 2.
Table 2. Values of PM, PE, and R for pure strategies.
p=1
p1
p2
p3
p4

PM
fS(t0)q4
fS(t1)]q4
fS(t1)(q3 + q4)
fS(t1)q3 + fS(t0)q4

PE
fD(t0)q1 + [1-fD(t0)]q2
fD(t0)q1 + fD(t0)q2
fD(t1)(q1 + q2)
fD(t1)q1 + [1-fD(t0)]q2

R
fS(t0)q4 â€“ fD(t0)q1 â€“ [1-fD(t0)]q2
fS(t1)q4 â€“ fD(t0)q1 â€“ fD(t0)q2
fS(t1)(q3 + q4) â€“ fD(t1)(q1 + q2)
fS(t1)q3 + fS(t0)q4 â€“ fD(t1)q1 â€“ [1-fD(t0)]q2

Comparisons of values from this table leads to the following conclusions:
1. An agent utilizing the p4 (CO) strategy is most likely to be eaten and also most likely to obtain a meal. In
ecological terms, this strategy is appropriate for a species that trades increased mortality for increased nutrition.
2. An agent utilizing the p2 (SF) strategy is least likely to be eaten and least likely to obtain a meal. Ecologically,
this strategy is appropriate for a species with low nutritional requirements.
3. In most cases, ranking strategies from most to least likely to be eaten gives the order p4, p1, p3, p2
(CO,SF,CS,HT) and ranking them from most to least likely to obtain a meal gives the order p4, p3, p1, p2
(CO,CS,HT,SF). In some cases, however, the positions of p1 and p3 may change.
To be more specific, the following strict inequalities hold:
PE(p4=1) > PE(p1=1) > PE(p2=1)
PE(p4=1) > PE(p3=1) > PE(p2=1)
PM(p4=1) > PM(p1=1) > PM(p2=1)
PM(p4=1) > PM(p3=1) > PM(p2=1)

(6)

The relation between strategies p1 (HT) and p3 (CS) is more complex:



PE ( p1 = 1) is 




PM ( p1 = 1) is 



> PE ( p3 = 1) if

1  fD (t 0 )  fD (t1 ) q1
>
fD (t1 )  fD (t 0 )
q2

 PE ( p3 = 1) otherwise
< PM ( p3 = 1) if

fS (t 0 )  fS (t1 ) q3
<
fS (t1 )
q4

(7)

 PE ( p3 = 1) otherwise

Since 1 â€“ fD(t0) â€“ fD(t1) will be of the order of 1 â€“ 2 while fD(t1) â€“ fD(t0) is of order  and likewise fS(t0) â€“ fS(t1)
will be of order  and fS(t1) will be of order 1 â€“ 2 ( << 1), equations (6) and (7) indicate that PE(p1=1) < PE(P3=1)
requires q1 >> q2 while PM(p1=1) > PM(p3=1) requires q4 >> q3. That is, if there is little or no deception, so that
appearances give a good reflection of actuality, then the HT strategy provides a better return than the CS strategy.
On the other hand, if appearances are usually deceptive the CS strategy is favoured over the other three strategies.
The value of the function R is computed at each vertex of the p-3 simplex to gain an idea of the relative advantages
of each strategy.
Table 3 gives conditions for R to reach its maximum value at each vertex of the p-3 simplex.

2192

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

Table 3. Conditions for R to be maximum for specified strategies.
R max at
p1 = 1

Conditions
 1  fD (t 0 )  fD (t1 ) 
 fD (t1 )  fD (t 0 ) 
q4 > 
 q2 , q 3 < 
 q1
fS (t1 )
 fS (t 0 )  fS (t1 ) 



Strong Implications
q4 >> q2

p2 = 1

 1  fD (t 0 )  fD (t1 ) 
 fD (t1 )  fD (t 0 ) 
q4 < 
 q2 , q 3 < 
 q1
f
(t
)

f
(t
)
fS (t1 )
 S 0



S 1

q1 >> q3

p3 = 1

 1  fD (t 0 )  fD (t1 ) 
 f (t )  fD (t 0 ) 
q4 < 
q2 , q 3 >  D 1

 q1
fS (t1 )
 fS (t 0 )  fS (t1 ) 



p4 = 1

 1  fD (t 0 )  fD (t1 ) 
 fD (t1 )  fD (t 0 ) 
q4 > 
 q2 , q 3 > 
 q1
f
(t
)

f
(t
)
fS (t1 )
 S 0



S 1

q1 >> q3

q4 >> q2

This partitions the q-simplex q-3 into regions with boundaries determined by the functions fD(t) and fS(t).
Conditions for maximum values of R on p-simplex edges can also be determined. These are shown in Table 4.
Table 4. Conditions for R to be maximum on p-simplex edges.
Edge

Conditions for R Maximum on Edge

(p1,p2)

 1  fD (t 0 )  fD (t1 ) 
q4 = 
 q2
 fS (t 0 )  fS (t1 ) 

(p1,p3)

 1  fD (t 0 )  fD (t1 ) 
 fD (t1 )  fD (t 0 ) 
q4 = 
 q2 , q 3 = 
 q1
fS (t1 )
 fS (t 0 )  fS (t1 ) 



(p1,p4)

 f (t )  fD (t 0 ) 
q3 =  D 1
 q1
fS (t1 )



(p2,p3)

 f (t )  fD (t 0 ) 
q3 =  D 1
 q1
fS (t1 )



(p2,p4)

 1  fD (t 0 )  fD (t1 ) 
 fD (t1 )  fD (t 0 ) 
q4 = 
 q2 , q 3 = 
 q1
f
(t
)

f
(t
)
fS (t1 )
 S 0



S 1

(p3,p4)

 1  fD (t 0 )  fD (t1 ) 
q4 = 
 q2
 fS (t 0 )  fS (t1 ) 

Note that R(p1=1) = R(p3=1) if and only if R(p2=1) = R(p4=1), which implies that if R is constant on any face of
the p-simplex then it is constant in the interior as well, as it will also be if R(p1=1) = R(p3=1) or R(p2=1) = R(p4=1).
The pairs (p1,p3) and (p2,p4) are opposite strategies: while p 1 reacts immediately to danger and safety, p3 pauses to
test both; and while p2 reacts immediately to danger but tests safety, p4 tests danger but reacts immediately to safety.
3.1. Population Dynamics Version of the Model
Let ns, s = 1,â€¦,4 be the number of individual agents using strategy s out of a total population N = n1 + n2 + n3 +
n4. Further, assume only those agents who have had a meal can reproduce, and that population s reproduce at a rate
as. The evolutionary dynamics of the total population is modelled by the equation

2193

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

dns
= as PM (s) [1  PE (s)] ns
dt

(8)

where PM(s) and PE(s) are the probabilities of obtaining a meal, or of being eaten for strategy ps. What is of interest
are the relative proportions of each strategy in the total population: xs = ns/N.

dxs 1 dns ns dN
=

N dt N 2 dt
dt

(9)

Substituting from equation (9) and the definition of xs yields
4
dxs
= as PM (s) [1  PE (s)] xs  xs  ak PM (k) [1  PE (k)]xk
dt
k=1

(10)

which has the form of a simple replicator equation [5]. Writing the rescaled growth rate for each strategy group in
the population as rs = asPM(s)[1 â€“ PE(s)] puts equation (10) into the form

dxs
= rs xs  xs ,
dt

4

 =  rk xk

(11)

k=1

The simple form of this equation allows direct solution. Consider  as a function of time and define
t

 (t) = exp   ds

(12)

0

With this definition, equation (11) becomes

d 
dxs

= xs  rs 


dt
dt 

(13)

xs (t) = xs (0)erst (t )

(14)

with solution

Use of equations (11) and (12) together with equation (14) leads to
4
4
d
=  rk xk (0)erk t  (t ) = e (t )  rk xk (0)erk t
dt
k=1
k=1

(15)

The solution of this equation is

 4

 (t) = ln   xk (0)erk t 
 k=1

Substituting from equation (16) into equation (14) now yields

(16)

2194

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000


 

 x (0)erst  

(0)
x
s
= 4

xs (t) =  4 s


( rk rs )t 
rk t 
x
(0)e
(0)e
x
 k
  k

 k=1
  k=1


(17)

By equation (17), if for any k  s it is true that rk > rs then xs(t) goes exponentially to zero while if rs > rk for all k 
s then xs(t) goes to 1. Thus, if any mixed strategy is to persist the corresponding rescaled growth rates must be
equal, and this places conditions on the growth rates as or on the functions fD and fS. In particular, if strategies s and
k are to coexist then the condition rk = rs yields the constraint on growth rates

ak =

PM (s) [1  PE (s)]
as
PM (k) [1  PE (k)]

(18)

If all four strategies are to persist this requires that


f (t )q 	
a1 = 1 + S 1 3 
 a4
fS (t 0 )q4 

q
f (t ) 	 1  fD (t1 )q1  [1  fD (t 0 )q2 ] 
a2 =  3 + S 0 
 
 a4
q
 4 fS (t1 )   1  fD (t 0 )q1  fD (t1 )q2 
a3 =

[ f (t )q
S

1

3

+ fS (t 0 )q4 ] {1  fD (t1 )q1  [1  fD (t 0 )q2 ]}
fD (t1 ) fS (t1 )(q1 + q2 )(q3 + q4 )

(19)

a4

Results such as this might arise from an evolutionary process but equation (18) can also be satisfied if the
probabilities PM and PE are allowed to depend on the choice of strategy. If this is the case, the form of the functions
fD and fS becomes significant beyond their use to model qualitative probabilities.
3.2. The Discrete Model
In a discrete model a sequence of strategic decision are required in discrete time. Agents are presented with a
string e(L),e(L-1),â€¦,e(1),e(0) where e(t) represents the presented appearance at time-step t. For example, one such
string might be DD,DD,SD,SS,DS,SD,DD,SS,SS,DS,DD,SD,SD,DS,SS.
The environmental string is presented to the population from right to left, one state at a time and within the
presented state the entire population responds according to the mixture of strategies in the population with the
numbers of individuals playing each strategy counted as well as the number who have died (eaten or starved). Two
assumptions are added: (a) If the presented state is DD or SD then individual agents who are not eaten survive
without a meal; (b) If the presented state is DS or SS then no agents are eaten but agents who do not get a meal do
not survive. With these assumptions the population evolution equation can be written as:



  (i, e(t))N 
ni (t)
ni (t + 1) = 
( j, e(t))n j (t) 


 j


(20)

where N = n1 + n2 + n3 + n4 with n1 the number of agents playing strategy HT, n2 the number of agents playing
strategy SF, n3 the number playing strategy CS, and n4 the number playing strategy CO. N is to be kept constant as
equation (20) is iterated by adding in the number of dead in a round in proportional to the numbers of agents in each
of the four strategy subpopulations. This equation can also be written in terms of frequencies: xi(t) = ni(t)/N as:

2195

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000



(i, e(t))


xi (t + 1) = 
 xi (t)

(
j,
e(t))x
(t)
j


 j


(21)

The coefficient (i,e(t)) is defined by

 1  PE (i, e(t))
(i, e(t)) = 
P (i, e(t)
 M

e(t) = DD or SD
e(t) = DS or SS

(22)

Working with the frequency representation, iteration of equation (21) yields
t


(i, e(s))



s=0
 x (0)
xi (t + 1) = 
  t
 i

     ( j, e(t))  x j (t) 
	
 j  s=0
	

(23)

and writing this out in terms of the probabilities in Table 1 gives

 f k1 (t ) [1  fD (t 0 )]k2 fSk4 (t 0 ) 
x1 (t) =  D 0
 x1 (0)
W (t)


 f k1 (t ) f k2 (t ) f k4 (t )x (0) 
x2 (t) =  D 0 D 1 S 1 2  x2 (0)
W (t)



(24)

 f k1 +k2 (t1 ) fSk3 +k4 (t1 ) 
x3 (t) =  D
 x3 (0)
W (t)


 f k1 (t ) [1  fD (t1 )]k2 fSk3 (t1 ) fSk4 (t 0 ) 
x4 (t) =  D 1
 x4 (0)
W (t)



where k1, k2, k3, and k4 are the respective numbers of the states DD, SD, DS, and SS in the choice sequence and the
denominator W(t) is given by



t

	

  ( j, e(t))
 x (t)  W (t) = f
j

j

s=0

k1
D

(t 0 ) [1  fD (t 0 )] 2 fSk4 (t 0 )x1 (0) +
k

+ fDk1 (t 0 ) fDk2 (t1 ) fSk4 (t1 )x2 (0) + fDk1 +k2 (t1 ) fSk3 +k4 (t1 )x3 (0) +

(25)

+ f (t 0 ) [1  fD (t 0 )] f (t 0 )x1 (0)
k1
D

k2

k4
S

3.3. Initial Simulation Results
Work on the simulation model is still in progress. Initial results show that mixed strategies in a population tend to
collapse to a single strategy, depending on the initial strategy frequencies, in line with conclusions from sections 3.1.
This effect is illustrated in Figure 3.

2196

B. Voorhees / Procedia Computer Science 1 (2012) 2187â€“2196
Author name / Procedia Computer Science 00 (2010) 000â€“000

Fig. 3. Evolution of initial mixed strategies. Red = HT, Green = SF, Blue = CS, Black = CO.
5. Discussion
All agents, cognitive or not, have models of their environment. In the cognitive case the agents are able to modify
and expand these models. Cognitive agents are intended to exhibit intelligent behaviour including perceiving,
reasoning, judging, responding, and learning. For a cognitive agent to respond appropriately in real time a number
of requirements must be satisfied: there must be criteria describing a target state or goal; a set of possible
behavioural responses; channels of information input (perception); the capacities to make comparisons between
current and target states; to make reasoned judgments in selection of behaviours; to determine the extent to which
responses have been effective in satisfying the specified criteria; and, the ability to learn from this process. In short,
cognitive agents need capacities for self-monitoring and adaptive control, hence the ability to maintain virtually
stable states. At the same time, cognition is expensive and there will be conditions when that expense outweighs the
advantages that cognitive flexibility provided. This is illustrated in Model 1. Current work on Model 2 is directed
towards further exploration of conditions favouring either cognitive or non-cognitive agents.
As presented here, Model 2 assumes non-cognitive agents and explores relations between four possible response
strategies under conditions of initial uncertainty. Parameters are probability distributions over the response
strategies and the environmental states, as well as the functions defined in equation (1). Current work expands this,
considering agents able to choose responses on the basis of cues provided in the environmental sequence and/or
learning from past experiences. In later work with this model, reasoning and judgment will be included by allowing
averaging over a number of simulations as a means of choosing responses. While the model itself remains very
simple, the question addressed, of a trade off between quickness and accuracy of response, is an important one for
complex adaptive systems that must act in an uncertain environment.
Acknowledgements
Supported by NSERC Discovery Grant OGP 0024871 and NSERC Undergraduate Student Research Assistant
grants to Todd Keeler, Jospeh Senez (Model 1), and Tyrone Woods (Model 2).
References
1. Thomas Kuhn (1977) The essential tension. In Thomas Kuhn, The Essential Tension: Selected Studies in
Scientific Tradition and Change Chicago: University of Chicago Press. 225 â€“ 239.
2. Burton Voorhees (2008) Virtual stability: A principle of complex systems. In A Minai, et al, eds.)
Proceedings of the 4th International Conference on Complex Systems NY: Springer.
3. Burton Voorhees, Joseph Senez, Todd Keeler, & Martin Connors (2008) A population model of the stabilityflexibility tradeoff. Advances in Complex Systems 11(3) 443 â€“ 470.
4. Burton Voorhees (2009) Virtual stability: Constructing a simulation model. Complexity 15(2) 31 â€“ 44.
5. Martin A. Nowak (2006) Evolutionary Dynamics Cambridge, MA: Harvard University Press.

