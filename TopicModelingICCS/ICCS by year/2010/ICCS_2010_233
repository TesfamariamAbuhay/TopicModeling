Available online at www.sciencedirect.com

Procedia
Computer Science
Science 00
(2009) 000‚Äì000
Procedia
Computer
1 (2012)
2441‚Äì2448

Procedia
Computer
Science
www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Knowledge extraction from multiple criteria linear programming
classification approach
Yuejin Zhanga,b, Peng Zhangc, Lingling Zhangb, Yong Shia,d, *
a

Research Center on Fictitious Economy and Data Sciences, Chinese Academy of Sciences, 100190, Beijing, China
b
School of Management, Graduate University of Chinese Academy of Sciences, 100190, Beijing, China
c
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China
d
College of Information Science and Technology, University of Nebraska ant Omaha, NE68182, Omaha, USA

Abstract
As an effective model for classification, Multiple Criteria Linear Programming (MCLP) has been widely used in business
intelligence. However, a possible limitation of MCLP is that it generates unexplainable black-box models which can only tell us
results without reasons. To overcome this shortage, in this paper, we present a knowledge mining strategy which mines
explainable decision rules from black-box MCLP models. Firstly, we use the rough set theory to distinguish the definable set
where samples are perfectly classified, from the rough set where misclassified samples may exist. Then, to get explainable
knowledge, we present a clustering-based decision rule extraction approach to extract knowledge from the definable set, and a
rough set-based rule extraction approach to the rough set. Finally, empirical studies on real world VIP Email data sets
demonstrate that our method can effectively extract explicit rules from MCLP model with only a little lost in performance.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
‚Éù
Keywords: MCLP, rule extraction, clustering, rough set;

1. Introduction
Classification is a useful method in data mining [1-5]. Recently, many mathematical programming based
algorithms are used to do classification, such as the fuzzy set, neural networks, and linear programming. Among
them, linear programming has been initiated in classification since twenty years ago [6]. After that, linear
programming methods gradually evolve to the Multiple Criteria Linear Programming (MCLP) approach by Shi et al.
[7] to classify real world credit card data set. Compared to the traditional mathematical tools, the MCLP approach is
much more flexible by allowing decision makers to play an active part in the analysis [8]. And it has been
extensively used in a series of business applications such as credit scoring, customer classification, medical fraud
detection.
Although MCLP is able to achieve good classification accuracies, it has a very notable shortcoming that it

* Corresponding author. Tel./fax: +86-10-82680697.
E-mail address: yshi@gucas.ac.cn (Y. Shi), cindy6228@163.com (Y. Zhang), zhangpeng@ict.ac.cn (P. Zhang), zll933@163.com (L. Zhang)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ‚Éù
doi:10.1016/j.procs.2010.04.275

2442

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Y. Zhang et al./ Procedia Computer Science 00 (2010) 000‚Äì000

generates unexplainable models. For example, in credit scoring, MCLP models can tell us whether a customer is a
high-risk one, and those who have a low credit score will be rejected to get any loan. Nevertheless, to customers
who are rejected, MCLP models can‚Äôt tell us why they are assigned low credit scores. In addition, there often exists
overlapping between classes in actual classification applications, so any linear function can not entirely separate the
classes from each other. MCLP approach gives an optimal classified boundary by a compromise solution approach,
and the accuracy rate of classification. But it did not tell us with what attribute values the samples may be classified
into a wrong class.
To solve this limitation, in this paper, we present a knowledge mining strategy which mining explainable
knowledge from black-box MCLP models. Firstly, the concept of rough set is employed to distinguish the definable
set where samples are perfectly classified, from the rough set in which samples may be misclassified. Then, for the
definable set, we propose a rule extraction approach based on clustering; and for the rough set, we propose the rough
set-based rule extraction approach. By using the rule extraction approaches, we can open up the black-box to extract
usable, readable if-then rules. These rules can be easily understood by humans.
The rest of this paper is organized as follow. In the next section, we survey some related works. In the third
section, we discuss a clustering based rule extraction method. After that, we discuss another rough set based
decision rule extraction method in section four. To demonstrate our method‚Äôs performance, we do some empirical
studies in section five. Finally, we conclude this paper in section six.
2. Related Work
Multiple Criteria Linear Programming (MCLP) Model: A general problem of data classification by using
multiple criteria linear programming can be described as following: given a set of r variables or attributes in
database A ( A1 ," , An ) r u n , let Ai (ai1 ," , air ) ¬è R r be the sample observations of data for the variables, where i 
1n and n is the sample size. If a given problem can be predefined as s different classes, C1
Cs , then the boundary between the j th and j1th classes can be bj j 1s √≠1. Then we define the
coefficients for an appropriate subset of the variables, denoted by X ( x1 , " xr )T ¬è R r and scalars bj such that the
separation of these classes can be described as follows: Ai X d b1 , Ai ¬è C1 , bk 1 d Ai X d bk , Ai ¬è Ck , k 2, " s  1 ,
and Ai X t bs 1 , Ai ¬è Cs , where Ai X t bs 1 , Ai ¬è Cs , means that data case Ai belongs to class Cj.
For the purpose of simplification, we discuss about the binary classification. Let Ai (ai1 , ", air ) , i=1,‚Ä¶, m, be
the sample observations of data for the variables, boundary b is to separate the two classes C1, and C2, and the
separation can be described as: Ai X d b, Ai ¬è C1 , and Ai X t b, Ai ¬è C2 .
The quality of separation can be measured by minimizing the total overlapping of data and maximizing the
distances of every data to its class boundary simultaneously [6]. Consequently, we have the following conclusions.
Let D i be the overlapping degree with respect of data case Ai within C1 and C2, and we want to minimize the sum
of D i , then the primal linear programming can be written as:
Minimize¬¶ D i

(1)

i

Subject to Ai X d b  D i , Ai ¬è C1
Ai X t b  D i , Ai ¬è C2

Let E i be the distances from Ai within C1 and C2 to its adjusted boundaries, and we want to minimize the sum
of E i , then the primal linear programming can be denoted as:
Maximize¬¶ E i

(2)

i

Subject to Ai X t b  Ei , Ai ¬è C1
Ai X d b  E i , Ai ¬è C2

Incorporating the two targets above, we get the hybrid multiple criteria linear programming model as follows:

2443

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

Minimize¬¶ D i & Maximize¬¶ E i
i

(3)

i

Subject to Ai X

b  D i  Ei Àà Ai ¬è C1

Ai X

b  D i  Ei Àà Ai ¬è C2

Where Ai are given, X and b are unrestricted, and D i , E i t 0 , i 1, ", m .
Rough sets-based rule extraction method: Rough set theory is a useful mathematical analysis tool for dealing
with fuzzy and uncertain information and discovering knowledge and rules hidden behind data [9]. Knowledge or
attribute reduction and rule extraction are the major applications of rough set theory.
Let S=ÀÑU, A, V, fÀÖbe an information system (attribute-value system), where U is a non-empty set of finite
objects, called the universe, and A is a non-empty, finite set of attributes, A C ¬â D , C ¬à D ¬á Ààwhere C is
conditional attributes , and D are decision attributes. With any P ¬é A there is an associated equivalence relation
IND(P): IND( P ) {( x, y ) ¬è U 2 | a ¬è P, a ( x) a( y )} . The relation IND(P) is called a P-indiscernibility relation.
The partition of U is a family of all equivalence classes of IND(P) and is denoted by U/IND(P)(or U/P).
Let X ¬é U be a target set that we wish to represent using attribute subset P. X is called a rough set, if it cannot be
expressed exactly by using the equivalence classes induced by attribute subset P; otherwise X is called a definable
set.
The rough set approach generates the classification rules through attribute reduction of the decision table. For the
information system S=(U, A, V, f), attribute a is reducible, for –°aƒôA , if the following equation is tenable:
ind ( A) ind ( A  {a}) .
Let |U|=n¬∑S be an n√ón discernibility matrix, for any element D ( x, y ) is the discernibility of all the attributes of the
object x and y, where D ( x, y ) {a ¬è A | f ( x, a ) z f ( y, a)} . For attribute a ¬è A , there is a Boolean variable ‚Äú a ‚Äù.
If D ( x, y ) {a1 , a2 ," , ak } z ¬á , then the Boolean function '

¬ñ ¬¶ D ( x, y ) = a

1

¬õ a2 ¬õ " ¬õ ak Àóotherwise, if

( x , y )¬èU uU

D ( x, y ) ¬á , then ∆∏=1 [9]. And for B ¬é A , if B ¬à D ( x, y ) z ¬á , then B is called a reduct of A. The set of attributes
which is common to all reducts is called the core: core( A) {a ¬è A | D ( x, y ) {a}, where x, y ¬è U } the core is the
set of attributes which is possessed by every legitimate reduct, and therefore consists of attributes which cannot be
removed from the information system without causing collapse of the equivalence-class structure.
Let X i be the equivalence class of U/C, Yi be the equivalence class of U/D, the description of X i is denoted
as des ( X i ) , which is the specific value of each conditional attribute for X i . Similarly, the description of Yi is
denoted as des (Yi ) , which is the specific value of each conditional attribute for Yi . The decision rule is defined as
follows: rij : des ( X i ) o des (Y j ) , Y j ¬à X i z ¬á , Where P ( X i , Y j ) | Y j ¬à X i | / | X i | is the certainty factor of rule rij ,

0  P ( X i ,Yj ) d 1 .

3. Rule Extraction based on Clustering for MCLP Classification

For the purpose of simplification, considering the binary classification we discussed above. Ai (ai1 , ", air ) ,
i=1, ƒÇ, m, is the sample observations of data for the variables, two classes C1 and C2 are given, so Ai belongs to
C1, otherwise it belongs to C2. Based on MCLP model, we get the hyper-plane to separate the two classes from each
other. The hyper-plane of r-dimensional space or classification function is as follows:

AX
b
(4)
Where X ( x1 , x2 ," , xn ) is weight vector, b is a scalar. Thus, for Ai ¬è R r , i 1" n . If Ai X  b , then Ai ¬è C1 ; if
Ai X ! b , then Ai ¬è C2 .
In the last few years, there have been many methods proposed to extract rules from black-box models, such as
neural networks and SVM, to generate explainable rules. These approaches can be categorized into two groups:

2444

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Y. Zhang et al./ Procedia Computer Science 00 (2010) 000‚Äì000

decompositional method and pedagogical method [10]. The decompositional method is closely intertwined with the
internal structure of models. For example, Nunez et al. [11] proposed a clustering based rule extraction of SVM
models by creating rule-defining regions based on prototype and support vectors. The extracted rules are represented
by equation rules and interval rules. Fung et al. [12] proposed a non-overlapping rule by constructing hyper cubes
with axis-parallel surfaces. On the other hand, the pedagogical rule extraction method directly extracts rules by
using other machine learning algorithms. For example, after building a black-box model, we can use some other
rule-extraction algorithms such as C4.5 to extract rules out of the model [13, 14].
For MCLP classification approach, we present a clustering-based knowledge mining method for extracting
decision rules. Firstly, a MCLP model is built and all the samples are classified into their own classes. Then in each
class, a clustering method, k-means will be carried out to catch the prototype vectors, which are the centers of the
clusters. After that, for comprehensibility, we generate hyper cubes with edges parallel to the axis and one vertex on
the classification boundary. For the accuracy of the rules, the hyper cubes cannot cross the boundary and the edges
of each other. Moreover, if not all the samples are covered in the hyper cubes, the clustering method will be carried
again with the number of cluster changed until all the instances in the sample are covered by the generated hyper
cubes. At last, each hyper cube can be translated to a decision rule. The procedure can be describes in Algorithm1
Algorithm 1 Rule extraction algorithm based on clustering
Input: The data set Ai (ai1 ," , air ) , MCLP model m, original boundary b
Output: Classification Rules {w}
Processing flow:
Step 1 Classify all the samples in A using model m;
Step 2 Define Covered set C= ) , Uncovered se U=A;
Step 3 While (U != ) ) Do
Step 3.1 for each group Gi,
Calculate the clustering center Pik = Kmeans(Gi), k=+1;
end for
Step 3.2 calculate the Distance d = Distance(m, Gi);
Step 3.3 Draw a new hypercube H=DrawHC(d, Gi);
Step 3.4 for all the instances ai ¬è U ,
if covered by H, then
U = U\ ai , C = Cƒ§ ai ;
end if
end for
end While
Step 4 Translate each hypercube H into Classification Rules W;
Step 5 Return Rule set {w}

4. Rough Set-based rule extraction from MCLP classification approach

The rule extraction approach discussed above based on the assumption that the samples are approximately
linearly separable, while neglects the misclassified samples. However, in many real world classification applications,
there often exists overlapping between classes. That is to say, any linear function can not entirely separate the
classes from each other. MCLP approach gives an optimal classified boundary by a compromise solution approach,
and the accuracy rate of classification. But it did not tell us with what attribute values the samples may be classified
into a wrong class.
For the uncertainty region with misclassified samples, we employed Rough Set theory, which is good at
expressing and dealing with uncertain knowledge.
For the binary MCLP classification above, the r-dimension space is separated by the hyper-plane Ai X b . The
samples Ai (ai1 , ", air ) are classified into class C1 or C2.

2445

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

Fig. 1. A binary MCLP classification

In the overlapping region, there are some misclassified samples, as shown in Figure 1. According to rough set
theory, the r-dimension space R r can be separated into four subspaces, denoted by S j ( j 1, 2,3, 4) , which can be
expressed as follows:
S1 { Ai ¬è R r | Ai X d b  D , Ai ¬é C1} ,
S2

{ Ai ¬è R r | Ai X t b  D , Ai ¬é C2 } ,

S3

{ Ai ¬è R r | b  D d Ai X d b, Ai ¬à C2 z ¬á} ,

S 4 { Ai ¬è R r | b d Ai X d b  D , Ai ¬à C1 z ¬á} .
Consequently, S1 ¬â S 2 is the set of samples which are classified into the right class, and it is a definable set on the
MCLP classification model; S3 ¬â S4 is the overlapping region, and it is the rough set.
Theoretically, D is the max overlapping of two-group (classes) boundary for all cases Ai, that is to say, it is the
farthest distance from the misclassified vector to the classification super-plane. Actually, the outlier samples will
make the D meaningless. Thus, a proper D can be set according to the mode of all the D i , or it can be obtained
according to an appointed threshold of classification accuracy.

For the classification super-plane AX
b , and the classification accuracy acc, let P* be the threshold of


classification accuracy. If acc< P*, we get the adjusted super-planes AX
b  D , and AX
b  D , where the
*
r
classification accuracy accS1 ¬â S2 t P . For S1 { Ai ¬è R | Ai X d b  D } and S 2 { Ai ¬è R r | Ai X t b  D } , the
classification accuracy is calculated as follows:
accS1

| { Ai ¬è R r | Ai X d b  D , Ai ¬é C1} |
| S1 { Ai ¬è R r | Ai X d b  D } |

(5)

accS 2

| { Ai ¬è R r | Ai X t b  D , Ai ¬é C2 } |
| S 2 { Ai ¬è R r | Ai X t b  D } |

(6)

accS1 u | S1 |  accS2 u | S2 |
(7)
| S1 |  | S2 |
Then, the rough set-based rule extraction from MCLP classification approach can be described in algorithm 2 in
detail.
accS1 ¬â S2

Algorithm 2 Rough Set-based Rule Extraction Algorithm
Input: The data set Ai (ai1 ," , air ) , MCLP model m, original boundary b, classification accuracy threshold P*
Output: Classification Rules {w}, certainty factor Pij
Processing flow:

2446

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Y. Zhang et al./ Procedia Computer Science 00 (2010) 000‚Äì000

Step 1 Classify all the samples in A using model m;
Step 2 for S1 { Ai ¬è R r | Ai X d b  D } , and S 2 { Ai ¬è R r | Ai X t b  D } ,
use Algorithm 1 to extract rules,
end for;
Step 3 for S3 ¬â S4 { Ai ¬è R r | b  D d Ai X d b  D } ,
get the decision table S=ÀÑU, A, V, fÀÖ;
get the adjusted decision table S‚Äô=ÀÑU, A, V, fÀÖ;
end for
Step 4 Calculate the discernibility matrix |U|=n¬∑S,
Step 4.1 for any element D ( x, y ) ¬è| U | ,
D ( x, y ) {a ¬è A | f ( x, a ) z f ( y, a)} ;
end for
Step 4.2 if D ( x, y ) {a1 , a2 ," , ak } z ¬á ,
'

¬ñ ¬¶ D ( x, y ) = a

1

¬õ a2 ¬õ " ¬õ ak ;

( x , y )¬èU uU

end if
if D ( x, y )

¬á,

∆∏=1;
end if
Step 5 Get the reduct B, core(A);
Step 6 Get Rules {w};
rij : des( X i ) o des (Y j ) ¬è {w} , Pij | Y j ¬à X i | / | X i | .

To sum up, after we get the MCLP classification model, for the definable set S1 ¬â S 2 , we employ the rule
extraction approach based on clustering; and for the rough set, we use the rough set-based rule extraction approach.
The MCLP knowledge mining model can be shown as figure 2.
Data preprocessing
Training sample set
MCLP classification model
Definable set
Rule extraction
based on clustering

Rough set
Decision table

Rough set-based rule extraction
IfƒÇ, thenƒÇ
IfƒÇ, thenƒÇ

IfƒÇ, thenƒÇ
IfƒÇ, thenƒÇ, 0  P d 1

Fig. 2. MCLP knowledge mining model

5. Experiment Results

We use the VIP Email data set as the benchmark data set for comparison. For the sake of space, we omit the
description of the VIP Email data set here. For more information, please check our former work [15]. As we

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448

2447

Author name / Procedia Computer Science 00 (2010) 000‚Äì000

discussed above, decision tree is a widely used method to extract rules from the training sample. So in the following
experiments, we compare our method with decision tree (which is implemented by the WEKA J48 package). Table
1 lists the comparison results between our method and the decision tree. By using our rule extraction method, we get
more than 20 hyper cubes, due to the limitation of the space, we only list two most representative rules (i.e., Rule 1
for class ‚ÄúLOST‚Äù and Rule 6 for class ‚ÄúCURRENT‚Äù) in the left side of Table 2. Then we find the corresponding
rules from decision tree (i.e., Rule 1‚Äô for class ‚ÄúLOST‚Äù and Rule 6‚Äô for class ‚ÄúCURRENT‚Äù), and list them in the
right side of Table 2. From the results, we can observe that, our rule extraction method can acquire much accurate
rules than the decision tree method. For example, when comparing Rule 1 with Rule 1‚Äô, we can get the conclusion
that Rule 1 is supported by 81.6% examples in the ‚ÄúLOST‚Äù class; on the contrast, rules from decision tree only gets
74.6% supportive examples.
Table 1. Comparisons between our method and Decision Tree‚Äôs Rule
MCLP‚Äôs Rule

Decision Tree‚Äôs Rule

RULE 1:

RULE 1‚Äô:

if 0<= The number of emails<= 3

if The number of emails <= 1

and 0<=the number of POP3 login on Tuesday <= 6

and the number of POP3 login on Tuesday <= 3

and 0<=the number of HTTP login <= 1

and number of HTTP login <= 1

and 0<=Free Email Service <= 1

and Free Email Service = 1

and 0<=The percentage of Charge Type 7<= 0.3

and The percentage of Charge Type 7 <= 0.25

and 0<=The total Charge Fee<= 45 ‚Ä¶
then class LOST [0.816]

and The total Charge Fee <= 50 ‚Ä¶
then class LOST [0.746]

6. Conclusions

The main goal of data mining in business intelligence is to extract a set of hidden patterns describing the
customers‚Äô behaviors. As an effective approach for classification, MCLP generates simple and flexible classification
models. Nevertheless, a major shortcoming of the MCLP is that it generates black-box models which are
unexplainable. To this limitation, in this paper we discussed a ‚Äòsecond‚Äô mining strategy to extract explainable
knowledge from the MCLP model. By using the rule extraction approaches, we can open up the black-box to
acquire usable, readable if-then rules which are easily understood by human beings. Experimental results on the real
world VIP Email dataset demonstrate that our method is effective in extracting accurate rules from MCLP model.

Acknowledgements

This research has been partially supported by a grant from National Natural Science Foundation of China
(#70921061, #70501030, #70621001, #90718042, #60674109, #70871111), Beijing Natural Science Foundation
(#9073020), and President Fund of GUCAS (Grant No.085102HN00).

References
1. N. Ren, M. Zargham, and S. Rahimi, A Decision Tree-Based Classification Approach to Rule Extraction for Security Analysis,
International Journal of Information Technology and Decision Making, (2006) Volume: 5, Issue: 1, pp. 227-240
2. B. Boutsinas, S. Athanasiadis, On Merging Classification Rules, International Journal of Information Technology and Decision Making,
(2008) Volume: 7, Issue: 3, pp. 431-450

2448

Y. Zhang et al. / Procedia Computer Science 1 (2012) 2441‚Äì2448
Y. Zhang et al./ Procedia Computer Science 00 (2010) 000‚Äì000

3. J. He, X.T. Liu, Y. Shi, W.X. Xu,, and N. Yan, Classifications Of Credit Cardholder Behavior By Using Fuzzy Linear Programming,
International Journal of Information Technology and Decision Making, (2004) Volume: 3, Issue: 4, pp. 633-650
4. Y. Shi, Y. Peng, G. Kou, and Z.X. Chen, Classifying Credit Card Accounts for Business Intelligence and Decision Making: A MultipleCriteria Quadratic Programming Approach, International Journal of Information Technology and Decision Making, (2005) Volume: 4, Issue: 4,
pp. 581-599

5. C.H. Lin, and Y.H. Liu, Multiple Trait Selection and Classification of Genotype ‚Äî A Data Mining Approach,
International Journal of Information Technology and Decision Making, Volume: 2, Issue: 3(2003) pp. 445-457
6. N. Freed, and F. Glover, Simple but powerful goal programming models for discriminant problems. European Journal of Operational
Research, (1981)7, 44Ã¢60.
7. Y. Shi and P.L. Yu, Goal setting and compromise solutions. In: B. Karpak and S. Zionts (Eds.), Multiple Criteria Decision Making and
Risk Analysis Using Microcomputers, (1989) pp. 165‚Äì204. Springer-Verlag, Berlin.
8. Y. Shi, Y. Peng, X. Xu and X. Tang, Data mining via multiple criteria linear programming: Applications in credit card portfolio
management. International Journal of Information Technology and Decision Making, (2002), Volume: 1, Issue: 1, 145‚Äì166.
9. Z. Pawlak,.Rough sets. International Journal of Computation and Information Sciences, (1982)11, 341‚Äì356.
10. D. Martens, B. Baesens, T. V. Gestelc, and J. Vanthienena, Comprehensible credit scoring models using rule extraction from support
vector machines, European Journal of Operational Research, Volume 183, Issue 3, 16 December (2007), Pages 1466-1476
11. H. Nunez, C. Angulo, and A. Catala. Rule based learning systems from SVMs. In: European Symposium on Artificial Neural Networks
Proceedings, (2002), pp.107-112.
12. G. Fung, S. Sandilya, R.Bharat Rao, Rule extraction from linear support vector machines, In: Proceeding of KDD‚Äô05. pp. 32-40.
13. J.Y. He, et al, Rule Extraction from SVM for Protein Structure Prediction, Studies in Computational Intelligence (SCI) (2008)80, 227‚Äì
252
14. S.N. Pang, N. Kasabov, Encoding and decoding the knowledge of association rules over SVM classification trees, knowledge Information
System, (2009)19: 79-105.
15. P. Zhang and J.R. Dai, Multiple Criteria Linear Programming for VIP E-Mail Behavior Analysis (2007). Proc. of IEEE ICDM 2007
Workshops, pages: 289-296.

