Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

Collecting Distributed Performance Data with Dataheap:
Generating and Exploiting a Holistic System View
Michael Klugea , Daniel Hackenberga , Wolfgang E. Nagela
a Center

for Information Services and High Performance Computing (ZIH)
Technische Universit¬®at Dresden ‚Äì 01062 Dresden, Germany

Abstract
The task of performance analysis and optimization grows more and more challenging with the increasing scale and
complexity of large computing systems. The need for a holistic system analysis becomes apparent when traditional
approaches do not collect the information that is required to investigate performance penalties caused by shared
system resources. We have developed a distributed approach that is able to collect and process performance data
from shared system resources. We call our software implementation of this approach Dataheap and have integrated
it with a traditional program tracing facility. In this paper we describe the needs that have driven this development
as well as connections to related projects. Dataheap is based on a threaded server, distributed agents that collect
performance data, a storage backend that makes use of diÔ¨Äerent databases, and access libraries that allow external
systems to retrieve current and historic performance data. The server subsequently processes incoming performance
data and allows to create secondary metrics on the Ô¨Çy which helps to transform individual system characteristics
to standard performance metrics. Finally, we brieÔ¨Çy illustrate how this approach has enhanced our performance
debugging capabilities as well as our research on energy eÔ¨Écient computing.
Keywords: Performance Analysis Tools, Distributed Systems

1. Introduction
Performance analysis in the context of High Performance Computing (HPC) is a vibrant topic as it has to adapt
quickly to changing systems designs, new processor generations, and new programming languages. The number of
components that have an impact on the overall system performance and need to be considered to allow a good performance analysis is growing continuously. This is caused by two fundamental trends: First, the number of components
(cores, disks, ...) per system is growing steadily. Second, many of these components gain complexity. Today, performance analysis for parallel programs focuses on the processes running on the CPU cores on the compute nodes.
However, valuable performance data originate not only from processors, but also from compute nodes and systems
as a whole. Experiments in related Ô¨Åelds [1, 2] have shown the value of an integrated solution for arbitrary performance data. We have developed a holistic approach for a system wide data collection and analysis in order to enable
end-to-end performance analysis from the application to arbitrary hardware devices. Initially, the approach had been
aimed to support our research in the areas of energy eÔ¨Éciency and I/O performance analysis. However, it is generally
Email address: michael.kluge@tu-dresden.de (Michael Kluge)

1877-0509 ¬© 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.215

1970

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

applicable to any scenario where performance data scattered over multiple physical locations is used to analyze a
complex performance problem.
This paper is organized as follows: Section 2 describes the main contribution of this paper and surveys related
work. The approach to address the collection of distributed performance data and some aspects of the Dataheap implementation are the topic of Section 3. Section 4 contains additional details about the calculation of secondary
performance metrics. A short performance evaluation is given in Section 5. Section 6 illustrates some example
scenarios in which Dataheap has proven its value. Finally, Section 7 concludes this paper and sketches future work.
2. Contributions and Related Work
The main contribution described in this paper is the system-wide collection and subsequent processing of performance data from diÔ¨Äerent, distributed sources within a production HPC environment, especially from data sources
that are external to the compute nodes. This enables the inclusion of data in the performance analysis process of parallel programs that has usually been treated separately, for example data from RAID controllers, Ô¨Åle servers, power
meters or environmental conditions. The presented system is able to calculate secondary metrics from the collected
data on the Ô¨Çy. This is particularly useful to provide an abstraction layer from the potentially large amount of data
sources. At the end of the paper we give a short performance evaluation of the server infrastructure that shows that
this infrastructure can handle about 500.000 counter updates per core in a multi-threaded environment.
Previous work on performance analysis of parallel programs has typically focused on the runtime analysis of
individual processes on the compute hosts [3, 4, 5]. This includes timing data about function calls, the collection
of performance counter data on the CPUs [6], as well as general information such as the system load [7] or the
memory consumption [8]. This data is particularly helpful for source code tuning and performance debugging of
various parallel programming paradigms such as MPI, OpenMP, and pthreads. In addition, approaches like [9], do
not provide a very detailed analysis of individual program runs but collect generic performance data from all jobs on
a whole system. This allows to identify programs that will most probably beneÔ¨Åt from an in-depth examination. HPC
systems have components that inÔ¨Çuence the behavior at the user level and are either shared among diÔ¨Äerent processes
(e.g. I/O subsystem, network) or are not accessible for a performance analysis from the compute hosts, or both.
Including this kind of data in a performance analysis has already proven its value. One example can be found in the
related Ô¨Åeld of performance analysis of database systems [2]. Using an instrumented version of a research Ô¨Åle system,
it has been shown that a comprehensive approach, that includes performance data from all levels of a distributed
system, can help to track back performance problems to their origin, for example a failed disk. An example for
the second option are power consumption monitors that use a global system view to regulate the maximum power
consumption of individual nodes. In the following, we use I/O subsystems to motivate our work.
Techniques for an eÔ¨Écient system wide performance analysis, for example in the area of I/O on HPC systems,
have only been established for user space I/O requests. The preconditions for a successful analysis in this area are 1) to
identify performance metrics that can potentially contribute to a performance analysis and 2) to collect all required data
and to calculate values for these performance metrics from the original data. As I/O requests in HPC environments are
typically targeted at a resource shared by many processes on diÔ¨Äerent compute nodes, all I/O activities on all nodes as
well as all activities on the Ô¨Åle servers and the storage controllers inÔ¨Çuence the performance observed at the user level
for I/O requests. Thus, performance counters like cache hits on RAID controllers, the utilization of FibreChannel ports
in the storage area network (SAN), or the number and the size of I/O requests that an individual RAID controller has
to serve are very valuable information that can help to identify performance problems. Up to now, the only indication
for these kind of problems was an increase in I/O requests wait time. The data for the new metrics have to be collected
on all RAID controllers as well as on the SAN switches. Both are typically not accessible for users or the performance
analysts, as there is no approach available that enables the inclusion of arbitrary performance data from outside of the
compute nodes into program traces.
Several tools for performance data collection in distributed environments already exist. They diÔ¨Äer with respect
to the supported performance counter types, the storage backend and the integrated analysis possibilities. Moreover,
they can be distinguished by the possibilities to consider historic data, means to track individual requests, or to collect
proÔ¨Åling data only. Several monitoring tools like Cacti [10] or Ganglia are based on RRDTool [11]. RRDTool
provides storage for performance data with a resolution of at maximum one second and assigns values that belong to a
timestamp that is between two interval bounds to the whole interval. Thus, it oÔ¨Äers only a limited temporal resolution.

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

1971

LMT [12] has been developed at the Lawrence Livermore National Laboratory to gather performance data from
Lustre environments. It consists of plug-ins for Cerebro, an enhanced derivative of Ganglia that uses a MySQL storage
backend. A visualization GUI accesses the database directly to show current and historic performance data.
Another tool that collects system wide performance data is SGI‚Äôs Performance Co-Pilot [13]. It uses a distributed
collection system that delivers data upon request from a master process. Thus, to get a value from a source, the master
process sends a message to the collection daemon and receives an updated value. This approach does not work for
distributed data sources with lots of counters with diÔ¨Äerent update frequencies.
Tools like Stardust [2] and DIADS [1] have been designed to track individual I/O requests for a post mortem
analysis. Both systems rely on an instrumented infrastructure that monitors individual requests within a storage system. However, current components used in I/O infrastructures of HPC systems do not allow a direct instrumentation.
Our system is targeted on production systems. Thus, we can not change Ô¨Årmware, Ô¨Åle system code or driver code
to support this kind of analysis. Instead, we focus on the integration of data sources that are available but remain
untapped up to now. Other areas that have similar requirements and therefore have a need for a generic approach
are for example the energy eÔ¨Éciency analysis (using external power meters) and the network analysis (using external
network switches).
3. Dataheap System Architecture
Our distributed data collection system Dataheap consists of multiple components. In this Section we explain the
the main drivers for its development and show how our requirements have inÔ¨Çuenced the system architecture.
3.1. Requirement Analysis
The following requirements had the most inÔ¨Çuence on the overall system design:
1.
2.
3.
4.
5.
6.
7.

allow many data sources to fetch their data in parallel as well as many clients that request data in parallel
support millions of performance counter updates each second
interface to permanent storage
interfaces to other (performance analysis) tools
minimal performance impact on these systems, where the data is collected
use a ‚Äôperformance counter‚Äô as the same basic element
consider secondary performance counters in all stages.

We now discuss these requirements, their implications and the respective software implementation in more detail.
For some, the implications for the software architecture are apparent and are handled in this section. The provision of
secondary performance counters requires a more structured approach and is discussed subsequently in Section 4.
3.2. Software Architecture
For performance reasons we have separated the components for data collection and data processing. Systems that
are typical sources for performance data are often an integral part of an infrastructure that has been designed to meet
a speciÔ¨Åc performance target. In order to minimize the impact of our data collection on these systems, it is in general
viable to only collect the data via an agent process and then to forward this data.
The complete system has been designed around a central, multi-threaded server (Figure 1). Beside for providing
the ‚Äôapplication glue‚Äô to keep all components together, the server also calculates secondary metrics from the data
provided through the agent interface. At startup, Dataheap reads a conÔ¨Åguration Ô¨Åle containing all information about
native and secondary performance counters and sets up the internal structure necessary to calculate secondary values.
The main components beside the server are:
‚Ä¢ Agents: provide updates for performance counters to the Dataheap server,
‚Ä¢ Database connectors: store performance counter data in databases,
‚Ä¢ Access libraries (Request/Monitor interface): to read either live or historic performance counter data.

1972

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978




	


















	

	






!




#




	

"






	








	



	

	



	


Figure 1: The Dataheap software design (green/orange). It depicts the components required to include performance data from RAID controllers (using an agent located in a secured area) into a trace of a parallel program
running on an HPC system. The access to current and historic performance data via an external library used
by an analysis system is sketched as well.

In order to be resilient against failures, the design allows the agents, the database connectors, and the clients to
disconnect and reconnect on demand. All connection from agents and databases are handled by dedicated threads
within the server in order to improve the scalability of the system.
At startup of either an agent or a database process, the program connects to the server and presents a list of
performance counter names that it is able to provide updates for or that it is able to store values for. Upon success,
the server hands back the identiÔ¨Åers for these performance counters which in turn are used instead of the performance
counter names. This way, all processes have the same view on the identiÔ¨Åer list. For performance counter names
unknown to the server process, new entries are written to the conÔ¨Åguration Ô¨Åle so that they can be activated on
demand.
The performance target for this design is to handle all performance counters that are part of an HPC infrastructure.
In the infrastructure that we currently use we have eight DDN S2A couplets that provide updates for about 10.000 different counters each second. The current multi-threaded implementation handles up to 500.000 performance counter
updates per second that are delivered via TCP/IP per processor core. This number scales with the number of cores
within a system and if far beyond the requirements for our research as described in Section 6. A more detailed performance analysis is done in Section 5. In order to keep the system extendible, C, C++, and Python interfaces are
provided.
3.3. Basic Work Unit
All components of Dataheap are consistently based on performance counters as the fundamental functional unit.
Each performance counter incorporates a unique numerical identiÔ¨Åer, a name, a description and a unit. Moreover, it is
either a native performance counter or a secondary performance counter, and can optionally have a storage location.
The name is primarily used to identify performance counters, while the description can be a longer text that describes
the performance counter in more detail.
Counter names should reÔ¨Çect the system hierarchy that they are part of. They should also contain a short comment
about the purpose of the performance counter. For example, typical names for I/O related performance counters from
a RAID device are ‚Äôscratch Ô¨Åles/rack 1/controller 3/port 1/read bandwidth‚Äô or ‚Äôscratch Ô¨Åles/total read bandwidth‚Äô.
Each native performance counter has a data source that is updated by an (external) agent. An update (or a sample)
consists of a triplet that contains the timestamp, the ID of the performance counter and the new value. There are no
restrictions regarding the number of performance counters that a single agent can update in parallel or regarding the
number of agents running on a single host.
A secondary performance counter combines one or more native or other secondary performance counters by
applying a transformation to the input values. Furthermore, each performance counter has a semantic that speciÔ¨Åes its
meaning, e.g. ‚Äômomentary value‚Äô or ‚Äôaverage since the last update‚Äô. However, this is beyond the scope of this paper.
For the remainder of this work it is assumed that all performance counters have the same semantics.

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

1973

3.4. Permanent Storage
Each performance counter can be associated with a storage location. It stores the triplets from performance counter
updates and provides access to historic data. So far we have implemented two database backends (MySQL and SQlite).
It is possible to create new adapters to include other (existing) databases, for example RRD Ô¨Åles. The processes
providing access to the database storage do not necessarily run on the same host as the server process. In order to
make the underlying database technology transparent and to enable a simpliÔ¨Åed retrieval of stored performance data,
we have implemented APIs that allow other tools to query the databases for information such as the average values
for a given time frame or a complete list of values for a time frame.
3.5. Access to Performance Data for External Tools
The Dataheap software is our approach to make external and host-local performance counters accessible for
external tools, for example event monitoring or tracing facilities. The Dataheap server provides an interface that
allows external tools to tell the server to start collecting performance counter values for speciÔ¨Åc performance counters
and to retrieve all collected samples. For this, the server can be queried for the available performance counter names
as well as for the status (presence of the data source and presence of the database connection) of the individual
performance counters.
Several tools have been developed that access this Dataheap interface. For example, stand-alone (Java) applications as well as a web-server (PHP) based tool are available to visualize performance counter data using standard
chart types. Moreover, lightweight command-line wrapper tools can be used to e.g. determine the energy consumption of a given workload based on power consumption measurements that are continuously reported to the server.
Another important client is our performance monitor VampirTrace [3]. We have used the VampirTrace plugin performance counter interface [14] to incorporate Dataheap performance counter data into the VampirTrace context and
subsequently into event traces of (parallel) applications. Details regarding this extension, the design decisions and the
correctness issues involved with merging external data into program traces are beyond the scope of this paper.
4. Calculation of Secondary Values
The pure amount of available performance counters within current supercomputer architectures makes the selection of the right set of performance counters for the performance analyst a diÔ¨Écult task. Summarization can be a
solution to this problem, e.g. providing an artiÔ¨Åcial sum performance counter for corresponding performance counters of multiple instances of the same hardware. In a generic infrastructure, arbitrary arithmetic operations should be
allowed for combining performance counter streams. Within this subsection a structured approach is presented to deal
with performance counter updates, especially with unsynchronized updates from diÔ¨Äerent data sources.
The main question is how to handle operations with more than one input (to combine performance counter values)
if the performance counter updates are issued at diÔ¨Äerent times. This is sometimes inevitable, in particular when
the samples are delivered from external locations. The solution we propose is to use interpolated values of the input
performance counters and compute a concurrent output performance counter sample for every sample of each input
performance counter. Typical examples for such operations are the sum or the average of multiple counters.
In contrast to operations with multiple inputs, unary operations are generally uncritical. This includes for example
the derivative or integral over time, the explicit reduction of the timer resolution, or an average over time. In the
following, the update scheme is formalized and details of the underlying algorithms are given.
4.1. Update Scheme for Two Counters
Each performance counter update consists of a value and a timestamp that has been assigned to the value at the
time of the performance counter update. Thus, an update for a performance counter C can be formally described as
a pair Uc = (t, v) : t, v ‚àà . The timestamp itself is distorted by an error that is inherent to any measurement. At
this point, the error is assumed to be small enough to not have an inÔ¨Çuence on the following considerations. All
performance counter updates for a time interval [t s , . . . , te ] form an update set S c .
The combination of performance counter updates in the general case can be described as an operation that maps
multiple update sets into a new one. Figure 2 gives an example of an update of a secondary performance counter
Cr from two performance counter sources. In this Ô¨Ågure the target performance counter is always updated at any

1974





Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

	














	

	
		





Figure 2: Example of a performance counter update with
two performance counter sources with diÔ¨Äerent but steady
update intervals, for example from a daemon that polls the
values in a Ô¨Åxed frequency

		

	



	

	


	
	


	


		
	


Figure 3: Update scheme for two performance counters, the use of an external
update timer is optional

update of the source performance counters. If both performance counters provide the same semantics, the values
from U21 and U32 can be used directly to calculate the target value U5r . If not, it is often possible to use a unary
function that transforms the semantics of one performance counter in a way that enables the combination of both
performance counters. In this case, the questions arise when and how the secondary performance counter has to be
updated. Obvious answers to the Ô¨Årst question are: either whenever one of the performance counter gets a new value
assigned (source update) or at Ô¨Åxed intervals (target update). In some cases it useful to update the target performance
counter only if one distinct source performance counter is being updated. This for example could be the one with the
lowest update frequency. Answers to the second question depend on the performance counter semantics as well as the
properties of the secondary performance counter.
4.2. Generic Update Scheme
The next paragraphs introduce a generic update scheme that works with an arbitrary number of performance
counters with diÔ¨Äerent properties. First, the correct timestamp for the target performance counter update depends on
either all source performance counters, an external update timer, or on a combination of both. As a consequence it is
reasonable to deÔ¨Åne at Ô¨Årst an interpolation function i that buÔ¨Äers incoming performance counter updates and provides
intermediate values for the timestamps that are needed to calculate the secondary performance counter. Within this
interpolation function is would also be possible to provide integrals and derivatives of the original values. Figure 3
depicts the update scheme as described in this paragraph. DiÔ¨Äerent possibilities to interpolate a value for a timestamp
between two measurements have also to be taken into account. An interpolation should always be done with respect
to the physical properties of the underlying data source. As an example, values from temperature sensors should be
interpolated with functions that do not have discontinuities. If two performance counters are being updated at diÔ¨Äerent
frequencies it may also occur that multiple elements of S 1 have to be used to calculate one element of S r .
One of the overall targets is to provide almost real-time analysis capabilities. Thus, the performance counter
updates of the secondary metrics have to be done with a minimal delay and the calculation has to be done as soon
as possible when one of the native performance counters has been updated. Using the analysis from the previous
paragraph, the maximum update delay for a speciÔ¨Åc secondary performance counter is around the inverse of the update
frequency of this native performance counter that has the smaller update frequency (if the time for the calculation itself
is neglected).
As a basic deÔ¨Ånition, a general function fn that maps n update sets S 1 , . . . , S n into a new update set S r can be
deÔ¨Åned as S r = fn (S 1 , . . . , S n ).
The steps to construct S r (and thus f2 ) for two arbitrary performance counters C1 and C2 are:
1. determine the set of conversion functions ( f1 ) that have to be applied to S 1 and S 2 in order to be able to later
combine S 1 and S 2
2. determine the interpolation functions for both input update sets

1975

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978









		










		














Figure 4: Update tree for the calculation of a GFLOP/s per Watt value from two nodes with individual power meters

request
value

t3

new

value

reader

collect

interpolate

f1
add

notify

new

value

new

value

t2

interpolated
value

Figure 5: UML sequence diagram for adding two values from two power meters

t4

data source power meter rack 2

value

f2 (add power values)

new

interpolate

f1
t1

collect

reader

data source power meter rack 1

Now f3 can be constructed in a recursive fashion by using the output of one f2 as input for the next f2 . A
generalized fn can be constructed by a tree structure of f2 functions. For fn all steps from the list above have to be
applied to the whole tree individually, one step after each other.
Figure 4 provides an example of a calculation tree to generate a GFLOP/s per Watt value of a two node system
where each node is connected to a power meter that measures the actual voltage and current in parallel. The number
of Ô¨Çoating point operations is collected per node and forwarded to the process calculating the secondary performance
counter. The GFLOP values are collected by using traditional PAPI performance counters, thus are monotonically
increasing and provide the number of operations done between two performance counter updates. The values for
arbitrary timestamps are calculated as the number of operations between two performance counter updates, divided
by the time between both updates. The GFLOP/s values from both nodes are then added up.
One interesting question concerns the calculation of a system‚Äôs power consumption for arbitrary timestamps as‚Äì
depending on the measurement device‚Äìthe values for voltage and current may be only valid for the moment of the
measurement. As both are continuous physical terms, a linear or spline-based interpolation is a feasible approach.
One of the less obvious consequences of driving the secondary performance counter update based on the update
frequencies of multiple native performance counters, is that the resulting update frequency is not Ô¨Åxed. Figure 5 shows
how a performance counter update is being handled in such a structure. Four timestamps t1...4 are being considered.
The Ô¨Årst performance counter is updated at the timestamps t1 and t3 and the other performance counter at t2 and t4 . The
Ô¨Ågure shows what happens at t2 to get an updated value from the f2 process. When the second performance counter
is being updated, a notiÔ¨Åcation is sent to f2 which in turn requests a value for this timestamp from the interpolation
function for the Ô¨Årst performance counter. As this interpolation needs both performance counter updates that are
adjacent to an interpolation point, it will block until t3 . Shortly after t3 , the f2 process will release an update for the
secondary performance counter for the timestamp t2 with the update delay t3 ‚àí t2 and with a value that is the sum of
the original value of the second performance counter and the interpolated value of the Ô¨Årst performance counter at t2 .

1976

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

4.3. Consequences from the Users Perspective
After introducing an update scheme for secondary performance counters, the question arises, if and how users are
expected to deal with this complexity. For example, network throughput is collected in Linux environments via the
/proc interface which only reports system wide numbers. The user‚Äôs perspective on a HPC system is typically based
on batch system jobs. If a user job is running on nodes that concurrently execute jobs of other users as well, all node
local performance counters need to be broken down on a per-user or per-process basis in order to be meaningful for
the performance analysis. This is typically impossible for performance counters that can only be used globally instead
of being locked by a speciÔ¨Åc process. The exclusive use of nodes (or any resources) is currently the only way to use
this type of performance counter.
Another, more severe problem for interfaces that provide access to this broader spectrum of performance counters,
is the question how users are supposed to specify the performance counter they want to use and how these are mapped
to physically available measurement points. For example, if a user wants to read energy performance counters and
diÔ¨Äerent parts of a system are connected to diÔ¨Äerent power meters, then the associated power meters have to be
selected after the jobs have been scheduled. The values from diÔ¨Äerent measurement points then need to be combined
to generate the expected results.
A common method to display timelines of performance counter values is to draw them as piecewise constant
functions. This is a correct assumption for performance counters that report the sum or the average of the work done
between two read cycles. This type of presentation has nevertheless a jump at each supporting point of the function
if the function value changes there. If the performance counter represents an inherently continuous function such as
temperature, then this assumption is not true anymore. We therefore need a linear interpolation or other interpolation
methods that reÔ¨Çect the performance counter properties appropriately. A systematic approach to these problems is
beyond the scope of this paper. However, in order to get an ‚Äôeasy-to-use‚Äô system, these challenges still need to be
addressed.
5. Basic Performance Evaluation
One of the key design decisions was to implement a scalable server infrastructure that supports millions of performance counter updates each second. This will enable the inclusion of data sources like power meters with high
sampling rates or RAID controller with lots of diÔ¨Äerent performance counters. For this, we evaluated the performance
of our multi-threaded server implementation in two scenarios, one with a single and one with multiple clients. The
evaluation has been done on HP DL 160G servers equipped with two sockets of 6-core Intel Xeon (Westmere-EP)
servers and connected via 10 Gbit Ethernet.
Figure 6 shows how many performance counter updates can be sent from a single client machine to a single server
with diÔ¨Äerent numbers of threads on this client. The limit of approx. 4 million updates is due to the fact that the
counter updates, one client

counter updates (bulk)
million updates per second

million updates per second

7
6
5
4
3
2
1
0

2

4
no bulk

6
8
number of threads

10

12

bulk

Figure 6: Server performance for a single client machine, with
diÔ¨Äerent numbers of client threads. Each thread attempts to push
500.000 updates per second.

7
6
5
4
3
2
1
0

2

4

6

8

10

12

number of threads per client
2 clients
4 clients
6 clients

8 clients
10 clients

Figure 7: Server performance for multiple client machines and
diÔ¨Äerent numbers of threads (up to 120 threads in total). Each
thread attempts to push 100.000 updates each second.

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

1977

client implementation is not able to send more updates per second from a single machine. We have implemented two
versions of the communication protocol, one that delivers one performance counter update with each packet (no bulk)
and one that delivers multiple updates for diÔ¨Äerent performance counters that belong to the same timestamps (bulk).
Both implementation show the same performance.
The use of many clients allowed us to test the scalability limits of a single server. With 120 diÔ¨Äerent data sources
(10 clients with 12 parallel processes), each pushing 100.000 performance counter updates per second to the servers,
the server is able to handle up to 6 million performance counter updates altogether or 500.000 updates per core.
The current approach with a single, yet scalable server may nevertheless become a bottleneck for an extremely
large number of data sources and/or extremely high update frequencies. For the moment we expect that the number
of agents or daemons that are required to collected data that is not locally available on the compute nodes will not
grow as fast as the number of compute cores. The number of Ô¨Åle servers and RAID controllers for example scales
with the number of compute nodes in a system. Therefore, only a few administrative machines will typically be used
to monitor the I/O infrastructure. These machines run the data collection agents and collect data from all attached
devices. However, for any analysis that requires the number of external agents to scale with the number of cores in
the system, the current single server approach would need to be extended.
6. Dataheap Use Cases and Experiences
In order to be able to perform an end-to-end performance analysis we have implemented a number of node local and system wide data collection agents for various components of today‚Äôs HPC systems. This includes storage
controllers, the Lustre Ô¨Åle system, InÔ¨ÅniBand and several other node based metrics. We have successfully analyzed
the impact of various I/O workloads on the associated Lustre Ô¨Åle servers and the RAID controllers [15, 16]. As I/O
analysis is one of our main research Ô¨Åelds, we perform long term evaluations of I/O requests as observed on DDN
controllers in our high performance storage infrastructure. We use these statistics to search for correlations between
block I/O requests and batch system activity. This allows us to help users that for example issue lots of small I/O
requests and therefore encounter bad Ô¨Åle system performance.
In addition, the Dataheap infrastructure is constantly being used to evaluate the power consumption of various
test platforms. High precision power meters (ZES Zimmer LMG95 and LMG450) measure the AC and/or DC power
consumption of a compute node with up to 20 samples/second. The data source gathers measurement data from the
power analyzers and reports timestamp-value tuples to the Dataheap server. This measurement does not incur any
overhead on the system under test, making it fully performance-neutral. Any analysis of the measurement data is
performed post mortem using timestamps collected on the system under test. This methodology has proven to be
highly useful and versatile. Storing all measurement data in the server allows us to run experiments on our test servers
without knowing the exact analysis method beforehand. A simple average power consumption can later on be extended
with e.g. the standard deviation or even individual samples over time. The work in several energy-eÔ¨Éciency related
projects has strongly beneÔ¨Åted from the Dataheap software infrastructure [17, 18, 19, 20]. Other Dataheap clients on
the producer side query for example the power consumption of a node via IPMI, report the DC power consumption of
individual components measured with sampling rates of up to 1000 Hz, or measure the server inÔ¨Çow air temperature
using a dedicated temperature probe.
7. Conclusion and Future Work
Performance analysis of parallel applications on large scale computing systems is a common task and can be
handled well with appropriate toolkits. These tools typically rely on hardware performance counters to gather performance relevant data from host components, e.g. from CPUs. This approach is signiÔ¨Åcantly limited in terms of shared
system resource analysis. This applies for example to the I/O analysis: Parallel Ô¨Åle systems and the underlying hardware are a shared system resources and provide performance data that is not integrated into common analysis tools.
We have illustrated our approach that extends current toolkits in order to provide a holistic performance view of large
scale HPC systems. Our implementation Dataheap serves as central yet scalable manager for arbitrary performance
data. This software infrastructure enables the collection and processing of performance data from multiple locations
as well as storage and retrieval of this data. An enhanced approach to calculate secondary performance counter values

1978

Michael Kluge et al. / Procedia Computer Science 9 (2012) 1969 ‚Äì 1978

allows users to derive additional performance metrics from the raw performance data with very low overhead. We have
implemented a prototype that supports two of our main research areas, namely performance analysis of parallel Ô¨Åle
systems and energy eÔ¨Éciency analysis. The implementation handles many data sources and millions of performance
counter updates in parallel. Future eÔ¨Äorts may include a further improved design that allows to create single-piece
software building blocks that can e.g. be deployed on a Lustre server to instantly provide multi-level performance
metrics like server load and network traÔ¨Éc.
Acknowledgment We would like to thank Robert Sch¬®one, Thomas Ilsche, Holger Mickler, Daniel Molka and Maik Schmidt for
their contributions during the design process and for continuously testing, improving, and using the software.
[1] S. Babu, N. Borisov, S. Uttamchandani, R. Routray, A. Singh, DIADS: Addressing the ‚ÄùMy-Problem-or-Yours‚Äù Syndrome with Integrated
SAN and Database Diagnosis, in: FAST‚Äô09: Proccedings of the 7th conference on File and stroage technologies, USENIX Association,
Berkeley, CA, USA, 2009, pp. 57‚Äì70.
[2] E. Thereska, B. Salmon, O. Salmon, J. Strunk, M. Wachs, M. Abd-el malek, J. Lopez, G. R. Ganger, Stardust: Tracking Activity in a
Distributed Storage System, in: ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, ACM Press, 2006,
pp. 3‚Äì14.
[3] M. S. M¬®uller, A. Kn¬®upfer, M. Jurenz, M. Lieber, H. Brunst, H. Mix, W. E. Nagel, Developing Scalable Applications with Vampir, VampirServer and VampirTrace, in: C. Bischof, M. B¬®ucker, P. Gibbon, G. R. Joubert, T. Lippert, B. Mohr, F. J. Peters (Eds.), Parallel Computing:
Architectures, Algorithms and Applications, Vol. 15 of Advances in Parallel Computing, IOS Press, 2008, pp. 637‚Äì644.
[4] S. S. Shende, A. D. Malony, The Tau Parallel Performance System, Int. J. High Perform. Comput. Appl. 20 (2) (2006) 287‚Äì311.
doi:10.1177/1094342006064482.
[5] J. Labarta, J. Gimenez, E. Mart¬¥ƒ±nez, P. Gonz¬¥alez, S. Harald, G. Llort, X. Aguilar, Scalability of Tracing and Visualization Tools, in: G. R.
Joubert, W. E. Nagel, F. J. Peters, O. G. Plata, P. Tirado, E. L. Zapata (Eds.), Parallel Computing: Current & Future Issues of HighEnd Computing, Proceedings of the International Conference ParCo 2005, 13-16 September 2005, Department of Computer Architecture,
University of Malaga, Spain, Vol. 33 of John von Neumann Institute for Computing Series, Central Institute for Applied Mathematics, J¬®ulich,
Germany, 2005, pp. 869‚Äì876.
[6] S. Browne, C. Deane, G. Ho, P. Mucci, PAPI: A Portable Interface to Hardware Performance Counters, in: Proceedings of Department of
Defense HPCMP Users Group Conference, 1999.
[7] M. Kluge, W. E. Nagel, Analysis of Linux Scheduling with VAMPIR, in: Proceedings of the 7th International Conference on Computational
Science, Part II, ICCS ‚Äô07, Springer-Verlag, Berlin, Heidelberg, 2007, pp. 823‚Äì830.
[8] M. Jurenz, R. Brendel, A. Kn¬®upfer, M. S. M¬®uller, W. E. Nagel, Memory Allocation Tracing with VampirTrace, in: Y. Shi, G. D. van Albada,
J. Dongarra, P. M. A. Sloot (Eds.), Computational Science - ICCS 2007, 7th International Conference, Beijing, China, May 27 - 30, 2007,
Proceedings, Part II, Vol. 4488 of Lecture Notes in Computer Science, Springer, 2007, pp. 839‚Äì846.
[9] R. Mooney, K. P. Schmidt, R. S. Studham, NWPerf: a system wide performance monitoring tool for large Linux clusters, Cluster Computing,
IEEE International Conference on 0 (2004) 379‚Äì389. doi:10.1109/clustr.2004.1392637.
[10] T. C. Group, Cacti: The Complete RRDTool-based Graphing solution, http://www.cacti.net (2009).
[11] T. Oetiker, RRDTool, http://oss.oetiker.ch/rrdtool/ (2009).
[12] A. C. Uselton, Deploying Server-side File System Monitoring at NERSC, Tech. rep., National Energy Research ScientiÔ¨Åc Computing Center
(2009).
[13] SGI, Performance Co-Pilot Home Page, http://oss.sgi.com/projects/pcp (2009).
[14] R. Sch¬®one, R. Tsch¬®uter, D. Hackenberg, T. Ilsche, The VampirTrace Plugin Counter Interface: Introduction and Examples, in: Euro-Par 2010
Parallel Processing Workshops, Vol. 6586 of Lecture Notes in Computer Science, Springer-Verlag, 2011, pp. 501‚Äì511. doi:10.1007/978-3642-21878-1 62.
[15] H. Mickler, Kombinierte Messung und Analyse von Programmspuren und systemweiten I/O-Ereignissen, Master‚Äôs thesis, Technische Universit¬®at Dresden (2007).
[16] M. Kluge, Comparison and End-to-End Performance Analysis of Parallel Filesystems, Ph.D. thesis, Technische Universit¬®at Dresden, Department of Computer Science (Sep. 2011).
URL http://nbn-resolving.de/urn:nbn:de:bsz:14-qucosa-75432
[17] D. Hackenberg, R. Sch¬®one, D. Molka, M. S. M¬®uller, A. Kn¬®upfer, Quantifying power consumption variations of HPC systems using SPEC
MPI benchmarks, Computer Science - Research and Development 25 (2010) 155‚Äì163. doi:10.1007/s00450-010-0118-0.
[18] D. Molka, D. Hackenberg, R. Sch¬®one, M. S. M¬®uller, Characterizing the Energy Consumption of Data Transfers and Arithmetic Operations on x86-64 Processors, in: Proceedings of the 1st International Green Computing Conference, IEEE, 2010, pp. 123‚Äì133.
doi:10.1109/GREENCOMP.2010.5598316.
[19] R. Sch¬®one, D. Hackenberg, On-line analysis of hardware performance events for workload characterization and processor frequency scaling
decisions, in: Proceeding of the second joint WOSP/SIPEW international conference on Performance engineering, ICPE ‚Äô11, ACM, New
York, NY, USA, 2011, pp. 481‚Äì486. doi:10.1145/1958746.1958819.
[20] D. Molka, D. Hackenberg, R. Sch¬®one, T. Minartz, W. E. Nagel, Flexible workload generation for HPC cluster eÔ¨Éciency benchmarking,
Computer Science - Research and Developmentdoi:10.1007/s00450-011-0194-9.

