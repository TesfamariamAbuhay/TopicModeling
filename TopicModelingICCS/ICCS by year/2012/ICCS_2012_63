Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 216 â€“ 225

International Conference on Computational Science, ICCS 2012

High Performance Dense Linear System Solver with Resilience to
Multiple Soft Errorsâœ©
Peng Du, Piotr Luszczek1 , Jack Dongarra
Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville

Abstract
In the multi-peta-ï¬‚op era for supercomputers, the number of computing cores is growing exponentially. However,
as integrated circuit technology scales below 65 nm, the critical charge required to ï¬‚ip a gate or a memory cell
has been reduced and thus causing higher soft error rate from cosmic-radiations. Soft errors aï¬€ect computers by
producing silently data corruption which is hard to detect and correct. Current research of soft errors resilience for
dense linear solver oï¬€ers limited capability when facing large scale computing systems, and suï¬€ers from both soft
error and round-oï¬€ error due to ï¬‚oating point arithmetic. This work proposes a fault tolerant algorithm that recovers
the solution of a dense linear system Ax = b from multiple spatial and temporal soft errors. Experimental results
on Cray XT5 supercomputer conï¬rm scalable performance of the proposed resilience functionality and negligible
overhead in solution recovery.
Keywords: soft error, fault tolerance, multiple errors, dense linear system solver

1. Introduction
Soft errors, normally in the form of bit ï¬‚ips, are events in microelectronic circuit that result in transient modiï¬cation without permanently damaging the device. They corrupt computed data and produce erroneous results without
leaving a trace. High-end computer systems are especially susceptible to such errors due to the ever increasing chip
density and shrinking assembly scale. Between 2003 and 2004, the 2048-node ASC Q supercomputer for scientiï¬c
computing in Los Alamos National Laboratorys experienced failures from extensive soft errors [1]. By comparing the
error logs with a radiation experiment conducted in a lab, the cause was soon identiï¬ed to be the cosmic ray striking
the parity-protected cache tag array. A similar incident has also appeared in a commercial computing system from
Sun Microsystems that caused outages for many of its customers due to cosmic ray soft errors [2]. These incidents
signify that soft error is a real issue that both hardware and software developers must face. Soft error rate (SER) in
6
memory is usually quantiï¬ed using FIT (failure in time) per MB, 1 FIT is 1 failure per 109 operation hours per 10
âœ© This research used resources of the Oak Ridge Leadership Facility at the Oak Ridge National Laboratory, which is supported by the Oï¬ƒce of
Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.
Email addresses: du@eecs.utk.edu (Peng Du), luszczek@eecs.utk.edu (Piotr Luszczek), dongarra@eecs.utk.edu (Jack Dongarra)
1 Corresponding author

1877-0509 Â© 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.023

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

217

bits. Google has reported between 778 and 25,000 FIT from errors in the DRAMs of their server ï¬‚eet, an order of
magnitude higher than previously expected [3].
Among HPC applications that could beneï¬t from resilience capability to soft errors, dense linear algebra applications such as the HPL benchmark for the TOP500 competition [4] and the AORSA fusion energy simulation
program [5], are representative examples. These applications normally involve solving a dense system of equations of
the form Ax = b on large scale HPC systems with matrix sizes of A being as large as 500,000. Soft errors that occur
during such long running applications produce incorrect solution with no apparent trace. This lowers productivity by
wasting valuable time and power in error tracing with little chance of ever locating the error.
To mitigate the impact of soft errors, hardware and software methods have been developed. For example, ECC
(error correcting code) [6] is a commonly used hardware technique which is although limited to a small number of soft
errors due to the overhead of encoding/decoding. In software, until now most of the soft error resilience techniques
for dense linear solvers are limited to small scale computing installations, such as on systolic arrays, assuming that
encoding/decoding can be carried out with exact arithmetic [7, 8]. Unfortunately, this assumption does not hold for
todayâ€™s Pï¬‚op/s supercomputer systems. In previous work [9], we have demonstrated the ï¬rst attempt to take on the
challenge of recovering the solution from a dense linear system solver of Ax = b with a single error occurrence in both
L and U of the LU factorization. This work further extends that eï¬€ort into multiple soft errors resilience as a more
performance friendly alternative to the complex hardware ECC. The proposed algorithms consider both the temporal
and spatial distribution of multiple errors. Temporal soft errors occur at diï¬€erent time, whereas spatial soft errors
manifest as simultaneous multiple bit ï¬‚ips in disparate locations. The proposed method may also be extended to other
one-sided factorizations for the recovery of linear system solution and factorization matrices.
The rest of the paper is organized as follows. Section 2 introduces an LU based dense linear solver. The impact of
soft error on the linear solver is then analyzed and the general workï¬‚ow of the proposed soft error resilience algorithm
is shown in Section 3. Sections 4 and 5 develop the protection method for both the left factor L and right factor U.
Section 6 proposes a segmented encoding method to reduce the computational complexity of the error detection and
locating. Finally, the recovery algorithm is discussed in Section 7 and the experimental results are shown in Section 8.
Related work is described in Section 9 while Section 10 concludes the paper.
2. High Performance Linear System Solver
For dense matrix A, the LU factorization produces PA = LU (or P = ALU), where P is a pivoting matrix, and
L and U are unit lower and upper triangular matrix respectively. LU factorization is popular for solving system of
linear equations. With L and U, the linear system Ax = b is solved by Ly = b and then U x = y. ScaLAPACK[10]
implements the right-looking version of LU with partial pivoting based on a block algorithm and 2D block cyclic data
distribution. Without loss of generality, this block algorithm is described with an N Ã— N matrix (or submatrix) A.
Split A into 2 Ã— 2 blocks with block size NB. A11 has size NB Ã— NB, A12 is NB Ã— (N âˆ’ NB), A21 is (N âˆ’ NB) Ã— NB,
and A22 is (N âˆ’ NB) Ã— (N âˆ’ NB), which is also known as the â€œtrailing matrixâ€. Decompose A as
A11
A21

A12
L
= 11
L21
A22

0 U11
L22 0

U12
L22

and therefore
â§
âª
A11
L
âª
âª
âª
= 11 U11
âª
âª
A21
L21
â¨
âª
âª
âª
A12 = L11 U12
âª
âª
âª
â©L22 U22 = A22 âˆ’ L21 U12

â†’ PDGET F2
â†’ PDT RS M
â†’ PDGEMM

(1)

This poses as one iteration (step) of the factorization, and pivoting is applied to the left and right side of the current
panel. The routines names in the ScaLAPACK LU are listed after â€œâ†’â€. For description, we use UÂ¯ to represent the
area of U12 modiï¬ed by PDTRSM, and UËœ for A22 after PDGEMM. Note that the size of UÂ¯ and UËœ changes as the LU
algorithm proceeds. Block algorithms oï¬€er good granularity to beneï¬t from high performance BLAS routines, while
2D block-cyclic distribution ensures scalability with load balancing.

218

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

3. Soft Error Resilience Framework
Since soft errors occur at times and locations unknown to the host algorithm, diï¬€erent methodologies are required
to provide resilience to diï¬€erent part of the matrix. In this section, the error propagation in LU factorization is
discussed and a general workï¬‚ow of error detection and recovery is given.
3.1. Error Pattern in the Block LU Algorithm
During LU factorization, the left factor L and right factor U have
diï¬€erent â€œdynamicsâ€ with regard to the frequency of data change. For
L, once a panel is factorized, the resulted data stored under the diagonal comes to the ï¬nal form without undergoing any further changes
except pivoting. Therefore Soft errors occurred in this area do not
propagate. This oï¬€ers an opportunity to use traditional diskless checkpointing method to protect these data. Algorithm based fault tolerance (ABFT) cannot be applied to the panel factorization since otherwise checksum rows for the panel could be moved into data by pivoting
which causes erroneous result. In LU, partial pivoting that swaps rows
of both L and U is adopted for better stability, but this pivoting operation could break the static feature of the L data as explained in [9],
and therefore in this work the pivoting to the factorized L is delayed to
the end of factorization. Since soft errors could strike at any moment,
checkpointing frequency as high as once per panel factorization is necessary, but this also potentially leads to high performance overhead and
Figure 1: Example of error propagation in the U
therefore checkpointing should be used to the minimum. For example,
result of a 30 Ã— 30 matrix
even though the factorized UÂ¯ (result of PDTRSM) also stays static
once produced, it can be protected by ABFT checksum with much less
overhead.
UËœ diï¬€ers from L and UÂ¯ in that it undergoes changes constantly from trailing matrix update. If soft errors alter
Ëœ and the erroneous data are carried along with computation to update the U,
Ëœ even a single-bit soft error
data within U,
Ëœ let alone multiple errors at diï¬€erent time of the factorization.
could propagate into large area of U,
Figure 1 shows an example of error propagation in U in a small matrix. Gaussian elimination is applied to a 30Ã—30
matrix. To simplify the illustration, no pivoting nor block algorithm is used. Each step of the Gaussian elimination
zeros out elements below the diagonal in one column. The color in the ï¬gure is related to the diï¬€erence between the
correct and incorrect upper triangular results. Higher brightness means larger absolute diï¬€erence in value and black
means no diï¬€erence. During the factorization, Two soft errors were injected at step 1 and 3 at location (6,13) and (12,
18) by adding random values. Since both errors occurred below the row 3, these errors fell in the UËœ area of steps 1 to
3. The two white dots at (6,13) and (12, 18) are the initial injection locations. Starting from step 4, the trailing matrix
update, which is a GEMM (matrix-matrix multiplication), picked up the erroneous data for computation. As the
iteration continued, the errors grew downward into the trailing matrix (in yellow). When they reached the diagonal,
the erroneous data started to participate in the vertical scaling of zeroing out values below diagonals, and as a result
the entire trailing matrix was contaminated immediately, shown in red dots. Both of the two errors followed the same
propagation pattern. This example shows that soft error propagation could result in large area of erroneous data.
3.2. General Workï¬‚ow
We proposed an ABFT based method to protect the LU factorization based linear solver. This method can tolerate
multiple occurrences of soft error in the whole area of factorization result and recover the correct solution x to the
linear system of equations Ax = b. The general workï¬‚ow of error detection and recovery is in Algorithm 1. Checksum
that is generated before solving the system is used to check and locate the soft errors and eventually recover the
solution. The veriï¬cation process is performed with no â€œonline checkingâ€ interruption.

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

219

Algorithm 1 Fault Tolerant System Workï¬‚ow
Require: Ax = b; Generator matrix G; Check matrix H
Step 1: Checkpoint A by Ac = A A Ã— G
Step 2: Perform LU factorization Lc Uc = P Ã— Ac in block algorithm of block size NB with partial pivoting; Pivoting
to L is delayed; Panel factorization result in each step is checkpointed immediately once produced; Errors in L are
correct after factorization and then pivoting in L is applied
Step 3: Detect error occurrence by checking Î´ = Uc Ã— H
if Found error(s) by Î´ >> 0 then
Step 3.1: Locate initial error(s) in U using Î´
Ë†
Step 3.2: Calculate xË† by xË† = U(\L\(P
Ã— b)), and
Step 3.3: Adjust xË† to the correct solution x = xË† + Î”
else
Step 4: Reach the correct solution x = U\(L\(P Ã— b))
end if
4. Encoding for Multiple Errors in L
The ï¬rst step of the workï¬‚ow in Algorithm 1 is to checkpoint the input matrix A with a generator matrix G. For
1 Â·Â·Â· 1
the single error case, it has been demonstrated in [9] that generator matrix G1 =
and check matrix
w 1 Â· Â· Â· wn
1 Â· Â· Â· 1 âˆ’1
work for the entire area of factorization result. In this section, we apply this idea to multiple
H1 =
w1 Â· Â· Â· wn âˆ’1
errors in L, and the next section further extends it to protecting U. Only the encoding issue is discussed. For a scalable
implementation, we continues to use the local checkpointing method in [9] where each process checkpoints its local
participating blocks in the current panel area.
For any column of the factorized panel in L, [l1 , l2 , Â· Â· Â· , lk ]T , the vertical checkpointing produces the following
three checksums c1 to c3 :
â§
âª
l1 + l2 + Â· Â· Â· + lk = c1
âª
âª
â¨
w
l
+ w2 l2 + Â· Â· Â· + wk lk = c2
(2)
âª
1
1
âª
âª
â© u l + u l + Â·Â·Â· + u l = c
1 1

2 2

k k

3

Since all computation is carried out in ï¬‚oating point number that has a ï¬xed number of digits for exponent and
fraction, the selection of wi and ui should avoid causing large contrast between operands that encourages the accumulation of round-oï¬€ errors. As an opposite example, in [8], the use of Vandermonde matrix where wi = j and ui = j2
incurs fast increase of value magnitude in checksum and causes notable precision loss from round-oï¬€ errors.
To work with round-oï¬€ errors, we propose to choose wi and ui from random numbers between 0 and 1. Suppose
soft errors change li and l j to lË†i and lË†j respectively, i < j. During the error locating step (step 3.1) in Algorithm 1,
re-generating the checksum gives:
â§
âª
âª
l1 + Â· Â· Â· + lË†i + Â· Â· Â· + lË†j + Â· Â· Â· + lk = cË†1
âª
âª
â¨
(3)
w1 l1 + Â· Â· Â· + wi lË†i + Â· Â· Â· + w j lË†j + Â· Â· Â· + wk lk = cË†2
âª
âª
âª
âª
â© u1 l1 + Â· Â· Â· + ui lË†i + Â· Â· Â· + u j lË†j + Â· Â· Â· + uk lk = cË†3
Subtract (3) from (2), we have
â§
âª
âª
cË†1 âˆ’ c1 = lË†i âˆ’ li + lË†j âˆ’ l j
âª
âª
â¨
c
Ë†
âˆ’
c2 = wi (lË†i âˆ’ li ) + w j (lË†j âˆ’ l j )
âª
2
âª
âª
âª
â© cË†3 âˆ’ c3 = ui (lË†i âˆ’ li ) + u j (lË†j âˆ’ l j )

(4)

Deï¬ne the system of equations in (4) as the â€œsymptom equationsâ€. The symptom equations establish the relationship
between soft errors and checksum, however it cannot be solved â€œas isâ€ since the six unknowns lË†i , lË†j , wi , w j and ui , u j
outnumber the available three equations.

220

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

To reduce the number of unknowns, let ui = w2i , i = 1, Â· Â· Â· , k. Combine the ï¬rst and second equation in (4):
lË†j âˆ’ l j =

1
((cË†2 âˆ’ c2 ) âˆ’ wi (cË†1 âˆ’ c1 ))
w j âˆ’ wi

(5)

And similarly combine the ï¬rst and third equation:
lË†j âˆ’ l j =

(cË†3 âˆ’ c3 ) âˆ’ w2i (cË†1 âˆ’ c1 )

(6)

w2j âˆ’ w2i

Eliminate lË†j âˆ’ l j from (5) and (6) by connecting the right hand sides, (4) can be eventually reduced to
(cË†3 âˆ’ c3 ) âˆ’ (wi + w j )(cË†2 âˆ’ c2 ) + wi w j (cË†1 âˆ’ c1 ) = 0

(7)

Deï¬ne this equation as the â€œcheck equationâ€. wi , w j can be determined by iterating through all possibilities in w with
O(N 2 ) complexity because i < j, and for each i, N âˆ’ i pairs of wi w j are tested in (7).
The error detection and recovery algorithm can be extended to t errors with complexity O(N t ) to determine the
locations of up to t errors. For example, when t = 3, symptom equation 4 is
â§
âª
cË†1 âˆ’ c1 = lË†i âˆ’ li + lË†j âˆ’ l j + lË†k âˆ’ lk
âª
âª
âª
âª
âª
â¨cË†2 âˆ’ c2 = wi (lË†i âˆ’ li ) + w j (lË†j âˆ’ l j ) + wk (lË†k âˆ’ lk )
âª
âª
âª
cË†3 âˆ’ c3 = ui (lË†i âˆ’ li ) + u j (lË†j âˆ’ l j ) + uk (lË†k âˆ’ lk )
âª
âª
âª
â© cË† âˆ’ c = h (lË† âˆ’ l ) + h (lË† âˆ’ l ) + h (lË† âˆ’ l )
4

4

i i

i

j

j

j

k k

(8)

k

Here i, j and k correspond to the three errorsâ€™ locations. Similar to the double-error case, the check equation can be
derived as
C4 (wi âˆ’ w j ) + w3i (w jC1 âˆ’ C2 ) âˆ’ w3j (wiC1 âˆ’ C2 )
(wi âˆ’ w j )w3k âˆ’ (wi âˆ’ wk )w3j + (w j âˆ’ wk )w3i

=

C3 (wi âˆ’ w j ) + w2i (w jC1 âˆ’ C2 ) âˆ’ w2j (wiC1 âˆ’ C2 )
(wi âˆ’ w j )w2k âˆ’ (wi âˆ’ wk )w2j + (w j âˆ’ wk )w2i

Similarly, by iterating through all possible pairs of wi , w j and wk using the check equation, the three error locations
can be determined and the error value can be found accordingly.
5. Encoding for Multiple Errors in UÂ¯ and UËœ
Soft errors in UÂ¯ and UËœ diï¬€er from those in L because they participate in the computation and cause error propagation. The t = 2 case is discussed in detail and is then extended to t > 2 errors.
5.1. Soft Errors Modeling
For temporal multiple soft errors, Luk et al. has proposed to cast soft error to a diï¬€ernt initial matrix to avoid
the diï¬ƒculty of knowing when soft error occurs [7]. The eï¬€ect of soft error during factorization is treated as rankone perturbation to the original matrix. Fitzpatrick et al. applied this method to double error modeling for Gaussian
elimination [8]. This section extends the encoding method in section 4 to provide protection to U.
LU factorization can be viewed as multiplying a set of triangularization matrices from the left to the input matrix
A to get the ï¬nal triangular form. Let A0 = A, and At = Ltâˆ’1 Ptâˆ’1 Atâˆ’1 . Ptâˆ’1 is the partial pivoting matrix at step t âˆ’ 1.
At the end of the factorization, PA0 = LU, where U is an upper triangular matrix.
Suppose two soft errors occur in the UÂ¯ or UËœ area at locations (i1 , j1 ) and (i2 , j2 ) in step s1 and s2 separately. In the
most general case, s1 s2 , i1 i2 and j1 j2 . Without loss of generality, let s1 < s2 .
At step s2 , express the soft error as a perturbation to the matrix at location (i2 , j2 ): AË† s2 = A s2 âˆ’ Î´ei2 eTj2 . A s2 is the
state of the matrix at step s2 before soft error occurs, and AË† s2 is outcome of A s2 modiï¬ed by a soft error of magnitude
Î´ at location (i2 , j2 ). ei2 and e j2 are column vectors with all 0s except a 1 at rows i2 and j2 respectively.

221

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

The error at step s2 is cast back as a perturbation to the matrix at step s1 ,

âˆ´

AË† s2 = A s2 âˆ’ Î´ei2 eTj2 = L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L s1 P s1 AË† s1 âˆ’ Î´ei2 eTj2
(L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L s1 P s1 )âˆ’1 AË† s2 = AË† s1 âˆ’ (L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L s1 P s1 )âˆ’1 Î´ei2 eTi

2

Let f = (L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L s1 P s1 )âˆ’1 Î´ei2 , and (L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L s1 P s1 )âˆ’1 AË† s2 = AË† sÂ¯2 , we have:
AË† sÂ¯2 = AË† s1 âˆ’ f eTj2

(9)

Continue casting (9) to the soft error at step s1 , eventually we have
AË† sÂ¯1 = A0 âˆ’ deTj1 âˆ’ geTj2

(10)

Here g = (L s1 âˆ’1 P s1 âˆ’1 L s1 âˆ’2 P s1 âˆ’2 Â· Â· Â· L0 P0 )âˆ’1 Ã— f = (L s2 âˆ’1 P s2 âˆ’1 L s2 âˆ’2 P s2 âˆ’2 Â· Â· Â· L0 P0 )âˆ’1 Î´ei2
Through this modeling process, the two soft errors are cast back to the input matrix A0 as perturbation to column
j1 and j2 . For more than 2 errors, the same process can be repeated and the general model for t errors is
t

AË† 0 = A0 âˆ’

d ji eTji

(11)

j=1

5.2. Errors Detection and Locating
With the model for soft errors, errorsâ€™ locations can be determined. This model is for the case where soft errors
occur only in matrix A. In fact checksum and the right hand sides b of Ax = b are equally susceptible to soft errors.
These cases can be protected by duplication and cross check, and the protection method for L in section 4 can be
directly applied to protect right hand sides.
In [8], four columns of checksum are used to locate two soft errors. Instead, we show that for N errors, N + 1
columns are suï¬ƒcient for error detection and data recovery.
For the input matrix A âˆˆ R NÃ—N , checksum is generated before the factorization using generator matrix
â¤
â¡ T â¤ â¡
â¢â¢â¢ e â¥â¥â¥ â¢â¢â¢ 1 Â· Â· Â·
1 â¥â¥â¥
â¥ â¢
â¥
â¢
(12)
G = â¢â¢â¢â¢ wT â¥â¥â¥â¥ = â¢â¢â¢â¢w1 Â· Â· Â· wN â¥â¥â¥â¥
â£ 2 Tâ¦ â£ 2
2â¦
(w )
w1 Â· Â· Â· wN
and A is encoded as [A, A Ã— GT ] = [A, Ae, Aw, Aw2 ]. The squaring operation is elementwise.
LU factorization is applied with the three additional checksum columns on the right as
P[A, Ae, Aw, Aw2 ] = L[U, c, v, s]
c, v, s âˆˆ R NÃ—1 are checksum after factorization.
Due to soft errors, A becomes erroneous. As shown in the error model, the LU factorization infected with soft
errors is equal to an soft-error-free LU factorization of a diï¬€erent initial (erroneous) matrix AË† 0 . Using A to represent
the original correct initial matrix and AË† for the erroneous initial matrix, (5.2) becomes:
Ë† U,
Ë† cË† , vË† , sË†]
Ë† A,
Ë† Ae, Aw, Aw2 ] = L[
P[
And using relationship between cË† and Ae:
cË† =

Ë† Tj + Pge
Ë† Tj )e = Ue
Ë† + LË† âˆ’1 Pd
Ë† = LË† âˆ’1 P(
Ë† AË† + deTj + geTj )e = LË† âˆ’1 (LË† UË† + Pde
Ë† + LË† âˆ’1 Pg
Ë†
LË† âˆ’1 PAe
1
2
1
2

Ë† + LË† âˆ’1 Pg.
Ë†
Ë† = LË† âˆ’1 Pd
Therefore, cË† âˆ’ Ue
Ë† + w j2 LË† âˆ’1 Pg,
Ë† and sË† âˆ’ Uw
Ë† 2 = w2 LË† âˆ’1 Pd
Ë† + w2 LË† âˆ’1 Pg.
Ë†
Ë† = w j1 LË† âˆ’1 Pd
By the same token, vË† âˆ’ Uw
j1
j2
â§
âª
Ë†
cË† âˆ’ Ue = x + y
âª
âª
âª
Ë† = w j1 x + w j2 y
Ë† âˆˆ R NÃ—1 , and y = LË† âˆ’1 Pg
Ë† âˆˆ R NÃ—1 . We have â¨
v
Ë†
âˆ’
Uw
Let x = LË† âˆ’1 Pd
âª
âª
âª
âª
Ë† 2 = w2 x + w2 y
â© sË† âˆ’ Uw
j1
j2

222

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

This system of equations is the vector form of (4), and similarly can be reduced to the check equation:
Ë† + w j1 w j2 (Ë†c âˆ’ Ue)
Ë† =0
Ë† 2 ) âˆ’ (w j1 + w j2 )(Ë†v âˆ’ Uw)
( sË† âˆ’ Uw

(13)

w j1 and w j2 can be determined by iterating through all possible N Ã— (N âˆ’ 1) combinations in w for a pair that makes
(13) hold. As a result, the error columns j1 and j2 are determined. Later, using the error columns, solution of Ax = b
can be recovered.
For t soft errors, with the error model in (11), the check equation is:
â§
Ë† 0
âª
= w0j1 x1 + Â· Â· Â· + w0jt xt
c0 âˆ’ Uw
âª
âª
âª
âª
âª
Ë† 1
âª
= w1j1 x1 + Â· Â· Â· + w1jt xt
âª
â¨ c1 âˆ’ Uw
(14)
âª
..
âª
âª
âª
.
âª
âª
âª
âª
â©ctâˆ’1 âˆ’ Uw
Ë† tâˆ’1 = wtâˆ’1 x1 + Â· Â· Â· + wtâˆ’1 xt
j1
jt
All the powers in (14) are elementwise. This general case of check equation in vector form for t soft errors exhibits
the same structure as in the scalar form. For t = 3 it has been shown that check equation (9) can be used to determine
Ë† i.
error locations except the scalar residues Ci is replaced with vector residues ci âˆ’ Uw
3
For two errors, the complexity of locating w j1 and w j2 is O(N ) because for each pair of w j1 and w j2 a vector norm
is calculated to test for zero vector in (13) which takes O(N) operations. For t > 2, the complexity to determine the
error columns exceeds the complexity of LU factorization, rendering this method computationally impractical for real
use. The same problem exists for L protection too when t > 3. The next section provides solution to this issue.
Since errors in UÂ¯ and UËœ propagate, the solution to (14) alone is insuï¬ƒcient for recovering the right factor U
as only the columns of the initial errors can be determined. However for a system of linear equations, by using
Sherman-Morrison-Woodbury formula, the solution can be recovered.
6. Complexity Reduction
As the number of tolerable errors t increases, the complexity of locating initial error columns grows exponentially.
To allow practical use, a complexity reduction method is proposed in this section.
6.1. Reduction for L
In the complexity O(N t ) for locating t errors, N is the factor that determines the range of search. By breaking
the search range into smaller segments, the complexity can be decreased to an aï¬€ordable level. There exist many
ways of segmenting N but since each segment requires separate storage for checksum, the segmenting method should
minimize the overall storage requirement. Use Nk to represent the segment size, the kth root of the vector length is
chosen in this work as the segment size where k is integer and , k â‰¥ 1.
1
1
Split N into equally sized segments of length Nk = N k . Apply the encoding method in (2) to each of the N 1âˆ’ k
segments. For each vector to tolerate t errors, t + 1 checksum items are required. Therefore for a vector of length N,
the total amount of space required to store checksums is
1

N 1âˆ’ k Ã— (t + 1)

(15)

And the storage overhead over that for the data vector has the trend
1

lim (N 1âˆ’ k Ã— (t + 1) Ã—

Nâ†’âˆ

t+1
1
) = lim âˆšk
=0
Nâ†’âˆ
N
N

(16)

Since the expensive error locating procedure is now carried out within a smaller range, the complexity of error
âˆšk
1
detection is largely reduced. The total overhead of locating t soft errors includes N 1âˆ’ K vector norms
â§ of length N,
âª
âˆš
âª
if t â‰¤ k
1
1
1
t
â¨O(N)
and iterating in k N for the correct pair of wi and w j O(N k Ã— N 1âˆ’ k ) + O((N k )t ) = O(N) + O(N k ) = âª
âª
â©O(N kt ) if t > k
Note that the number of tolerable soft error t is for each segment. Therefore to tolerate the same amount of soft
errors, each segment could opt for a smaller t than in the k = 1 case. For a ï¬xed t, increasing k has the same eï¬€ect by
reducing the range of search, but comes at the cost of more extra storage according to (15).

223

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

6.2. Reduction for U
Ëœ without any complexity management, locating t soft errors requires O(N t+1 ) operations, one order
For UÂ¯ and U,
higher than the original complexity for L protection. To reduce the complexity to an aï¬€ordable level, the segmenting
method in section 6.1 is extended to the block LU algorithm for UÂ¯ and UËœ protection.
6.2.1. Block Encoding of Matrix
In block LU algorithm, the panel factorization itself is an LU factorization of a tall and skinny block, therefore the
encoding technique in (5) can be used to protect a panel or several panels too if the encoding is performed accordingly.
For the whole matrix, the segmented encoding can be applied based on the following theorem (proof is omitted due
to space constrain):
Theorem 6.1. Block Encoding protects the trailing matrix at the end of each iteration of LU factorization
For illustration, consider the following example: take a matrix A of 2 Ã— 2 blocks encoded using the generator in (12)
Ac =

A11
A21

A12
A22

A11G
A21G

A12G
A22G

0 U11 U12 C11 C12
L11
L21 L22 0 U22 C21 C22
C11 = U11G, C12 = U12G
. This shows that after LU factorization, the added
And C1 to C4 are calculated as:
= âˆ…, C22
= U22G
C21
four checksum blocks oï¬€er protection to the three data blocks U11 , U12 and U22 independently. Since G in (12) oï¬€ers
t errors protection capability, the three data blocks in U each can tolerate up to t soft errors.
In ScaLAPACK,
matrix A is split into blocks of size NB Ã— NB, therefore whenâˆšk = 2,
âˆš
âˆš the encoding block size
Nk is N Ã— N rounded to multiple of NB. Error detection is performed on each N Ã— N blocks. For U11 , ï¬rst
U11 Ã— G(:, 1) âˆ’ C11âˆš(:, 1) âˆš
is checked and if the norm is suï¬ƒciently large, the error detection procedure in 5.2 is then
performed for this N Ã— N block.
The complexity of performing blocked error detection and locating includes the error check that is either a full
1
or upper triangular matrix-vector multiplication and the error locating operation within the block. Suppose Nk = N k
rounded to a multiple of NB, and the generator matrix G has size Nk Ã— t for t error resilience capability. Since error
1
checking is only carried out in the upper triangular blocks of A, there are in total 1 + 2 + Â· Â· Â· + N 1âˆ’ k number of blocks.
Carry out LU factorization to Ac and we have: Ac =

1âˆ’ 1
k

1âˆ’ 1
k

Therefore the error checking complexity is (1 + 2 + Â· Â· Â· + N 1âˆ’ k ) Ã— O((N k )2 ) = N (N2 +1) Ã— O(N k ).
t
1
And the error locatingâˆšcomplexity
is O(N k Ã— N k ). For instance, when k = 2 and t = 2, the total overhead of error
âˆš
3
detection and locating is N( 2 N+1) Ã— O(N) + O(N 2 ) = O(N 2 ) < O(N 3 ). Therefore the overhead is aï¬€ordable for the
solver based on LU factorization.
1
The total amount of extra storage for storing checksum columns is N Ã— N 1âˆ’ k Ã— (t + 1). And the storage overhead
1
âˆšk = 0.
over that for the data vector has the trend limNâ†’âˆ (N Ã— N 1âˆ’ k Ã— (t + 1) Ã— N12 ) = limNâ†’âˆ t+1
1

1

2

N

7. Recovery Algorithm
After soft errors are detected and located by their columns, the correct solution to the system of equations Ax = b
can be recovered using the Sherman-Morrison-Woodbury formula as suggested by [8]. The computing complexity of
recovery the solution x is O(N 2 ).
8. Performance Evaluation
Soft errors in the left factor are static, and the detection and recovery for this area has been validated in [11, 9]
showing little performance impact to the host algorithm. The algorithms for multiple soft errors in the right factor,
Â¯ on the other hand, have higher complexity and are most eï¬€ectively aï¬€ected by the proposed encoding
namely UËœ and U,
and complexity reduction method. Therefore only the validation for this part is shown in this section.

224

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225


"



!





	







	






















         

		



	

"!$$
 

!""

!

	



 $#

!%!

!!%!%

##!




 !!

!$!

!!#

#!%"%

!#$




 $"

! %

!

#!"

!!%"%

$

!" " 

$$
!%!

Figure 2: Experimental results on Cray XT5 (Kraken): a run on 16,384 (128 Ã— 128) cores (left) and weak scaling (right).

The experiments are performed on a large scale distributed memory system: the Kraken supercomputer by Cray
Inc. at the Oak Ridge National Lab. Kraken has 9,408 compute nodes. Each node has two Istanbul 2.6 GHz six-core
AMD Opteron processors and 16 GB of memory. All the nodes are Connected by the Cray SeaStar2+ interconnect.
In the experiments, two soft errors are injected into location (336, 361) and (347, 359) at the beginning of the 2nd and
3rd panel factorization, respectively. Data values are incremented with random magnitudes
to simulate the results of
âˆš
bit ï¬‚ips in the memory slots that hold these data. The block size for encoding is N.
Figure 2 shows the eï¬€ectiveness of the complexity reduction method for U with a 16 Ã— 16 process grid on Kraken,
nonâˆ’FT âˆ’FLOPS FT
%. When Nk = N, block encoding for soft
t = 2. The overhead is in Gï¬‚op/s and calculated as FLOPSFLOPS
nonâˆ’FT
errors in U is not in eï¬€ect. The whole matrix is encoded with a generator matrix of size N Ã— 3. In this case the
overhead is close to 100% (the blue line), which means the error detection and recovery combined take as much time
as solving the linear system of equations. This is consistent
with the theoretical complexity of O(N t+1 ) = O(N 3 ). The
âˆš
red line, on the other hand, is the result when Nk = N. The overhead drops quickly from a little less than 40% to
2%, which veriï¬es that block encoding largely reduces the error detection overhead. The cost of this improvement is
the extra space for storing checksum which is roughly 1% of the input matrix of size 50,000.
Figure 2 also shows the weak scalability experiment result where matrix size and grid dimension are doubled in
proportion. Throughout all the testing sizes from 64 to 16,384 cores, FT-PDGESV declares around 1% overhead with
and without soft error recovery. The solution to the linear system was successfully recovered.
The experiment result conï¬rms that the complexity of recovering the solution to Ax = b from two soft errors in
the right factor has been eï¬€ectively managed by the complexity reduction method, and soft errors can be precisely
detected and located with the presence of round-oï¬€ error. The fault tolerance functionalities can recover the solution
of the dense linear system with trivial performance impact.
9. Related Work
In the ï¬eld of fault tolerance for HPC systems, checkpoint-restart (C/R) is the most commonly used method [12].
The running state of the application is written to reliable storage at certain intervals automatically by the message
passing middleware or at the request of the user application. C/R requires the least user intervention but suï¬€ers from
high overhead from checkpointing through disk I/O. To reduce the overhead, diskless checkpointing [13] turns to
system memory for checksum storage rather than disks. Applications have seen better fault tolerance performance
than C/R [14] for dense linear algebra problem. Both C/R and diskless checkpointing need the error information
for recovery, which is not available with soft error. Algorithm based fault tolerance (ABFT) eliminates the need for
periodical checkpointing. This signiï¬cantly reduced checkpointing overhead during computing, and the checksum by
ABFT reï¬‚ects the most current status of the data and therefore oï¬€ers clues for soft error detection and recovery. ABFT
was originally introduced to deal with silent error in systolic arrays [15, 16]. Data is encoded before the computation
begins. Matrix algorithms are designed to work on the encoded checksum along with matrix data, and the correctness
is checked after the matrix operation completes.
Using ABFT to mitigate single soft errors in dense matrix factorization has been explored in [7, 17]. Later, this
was extended to multiple errors [18, 8, 19] by adopting methodology from ï¬nite-ï¬eld based error correcting code (e.g.

Peng Du et al. / Procedia Computer Science 9 (2012) 216 â€“ 225

225

Reed-Solomon [20]) where only the right factor of factorization result is protected and computation is assumed to take
place with exact arithmetics. These make the ECC based error location determination method loose eï¬€ectiveness.
Recently, iterative solvers were evaluated for soft error vulnerability [21, 22, 23], signifying the continued awareness of soft error for solving large scale problems. For dense matrices, the eï¬€ect of soft errors on linear algebra
packages like BLAS and LAPACK has also been studied [24], which showed that their reliability can be improved
by checking the output of the routine, and the error patterns do not depend on the problem size. Also, the possibility of predicting the fault propagation is explored. For dense matrix factorization based solver, method to mitigate
single soft error has been discussed in [9] and the recovery of matrix factorization was shown in [11] using QR for
demonstration.
10. Conclusion
Soft error resilient algorithm for LU factorization based dense linear system solver is proposed in this work. Both
spatial and temporal multiple soft errors in the whole matrix can be addressed with the existence of round-oï¬€ errors
from ï¬‚oating point operation. Once errors are detected, the solution of Ax = b can be recovered with low overhead
using the complexity reduction technique. Experimental results on the Kraken supercomputer conï¬rm both the soft
error mitigation capability and the negligible performance overhead. In future work, the proposed method will be
extended to the protection of dense matrix factorizations like LU and QR.
References
1. S. Michalak, K. Harris, N. Hengartner, B. Takala, S. Wender, Predicting the number of fatal soft errors in los alamos national laboratoryâ€™s asc
q supercomputer, Device and Materials Reliability, IEEE Transactions on 5 (3) (2005) 329â€“335.
2. D. Lyons, Sun screen, available at http://www.forbes.com/forbes/2000/1113/6613068a.html (November 13 2000).
3. B. Schroeder, E. Pinheiro, W. Weber, DRAM errors in the wild: a large-scale ï¬eld study, in: Proceedings of the eleventh international joint
conference on Measurement and modeling of computer systems, ACM, 2009, pp. 193â€“204.
4. H. W. Meuer, E. Strohmaier, J. J. Dongarra, H. D. Simon, TOP500 Supercomputer Sites, 36th Edition, (The report can be downloaded from
http://www.netlib.org/benchmark/top500.html) (November 2010).
5. R. Barrett, T. Chan, E. Dâ€™Azevedo, E. Jaeger, K. Wong, R. Wong, Complex version of high performance computing linpack benchmark (hpl),
Concurrency and Computation: Practice and Experience 22 (5) (2010) 573â€“587.
6. D. Abts, J. Thompson, G. Schwoerer, Architectural support for mitigating dram soft errors in large-scale supercomputers.
7. F. Luk, H. Park, An analysis of algorithm-based fault tolerance techniques* 1, JPDC 5 (2) (1988) 172â€“184.
8. P. Fitzpatrick, C. Murphy, Fault tolerant matrix triangularization and solution of linear systems of equations, in: Application Speciï¬c Array
Processors, 1992. Proceedings of the International Conference on, IEEE, 1992, pp. 469â€“480.
9. P. Du, P. Luszczek, J. Dongarra, High performance dense linear system solver with soft error resilience, in: Proceedings of the IEEE Cluster
2011, IEEE Computer Society Press, 2011.
10. J. Dongarra, L. Blackford, J. Choi, A. Cleary, E. Dâ€™Azevedo, J. Demmel, I. Dhillon, S. Hammarling, G. Henry, A. Petitet, et al., ScaLAPACK
userâ€™s guide, Society for Industrial and Applied Mathematics, Philadelphia, PA.
11. P. Du, P. Luszczek, S. Tomov, J. Dongarra, Soft error resilient QR factorization for hybrid system., Tech. Rep. 252, LAPACK Working Note,
http://www.netlib.org/lapack/lawnspdf/lawn252.pdf (Jul. 2011).
12. Fault tolerance for extreme-scale computing workshop report (2009).
URL http://www.teragridforum.org/mediawiki/images/8/8c/FT_workshop_report.pdf
13. J. Plank, K. Li, M. Puening, Diskless checkpointing, Parallel and Distributed Systems, IEEE Transactions on 9 (10) (1998) 972â€“986.
14. J. Plank, Y. Kim, J. Dongarra, Algorithm-based diskless checkpointing for fault-tolerant matrix operations, in: ftcs, Published by the IEEE
Computer Society, 1995, p. 0351.
15. K. Huang, J. Abraham, Algorithm-based fault tolerance for matrix operations, Computers, IEEE Transactions on 100 (6) (1984) 518â€“528.
16. J. Abraham, Fault tolerance techniques for highly parallel signal processing architectures, Highly parallel signal processing architectures
(1986) 49â€“65.
17. F. Luk, H. Park, Fault-tolerant matrix triangularizations on systolic arrays, Computers, IEEE Transactions on 37 (11) (1988) 1434â€“1438.
18. H. Park, On multiple error detection in matrix triangularizations using checksum methods, JPDC 14 (1) (1992) 90â€“97.
19. C. Anï¬nson, F. Luk, A linear algebraic model of algorithm-based fault tolerance, Computers, IEEE Transactions on 37 (12) (1988) 1599â€“1604.
20. I. Reed, G. Solomon, Polynomial codes over certain ï¬nite ï¬elds, Journal of the SIAM 8 (2) (1960) 300â€“304.
21. G. Bronevetsky, B. de Supinski, Soft error vulnerability of iterative linear algebra methods, in: Proceedings of the 22nd annual international
conference on Supercomputing, ACM, 2008, pp. 155â€“164.
22. K. Malkowski, P. Raghavan, M. Kandemir, Analyzing the soft error resilience of linear solvers on multicore multiprocessors, in: Parallel &
Distributed Processing (IPDPS), 2010 IEEE International Symposium on, IEEE, pp. 1â€“12.
23. V. Heuveline, D. Lukarski, F. Oboril, M. Tahoori, J. Weiss, Numerical defect correction as an algorithm-based fault tolerance technique for
iterative solvers.
24. G. Bronevetsky, B. de Supinski, M. Schulz, A Foundation for the Accurate Prediction of the Soft Error Vulnerability of Scientiï¬c Applications,
Tech. rep., Lawrence Livermore National Laboratory (LLNL), Livermore, CA (2009).

