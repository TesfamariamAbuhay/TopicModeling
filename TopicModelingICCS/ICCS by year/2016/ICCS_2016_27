Procedia Computer Science
Volume 80, 2016, Pages 1702â€“1711
ICCS 2016. The International Conference on Computational
Science

Efficient Computation of the Tensor Chordal Kernels
BogusÃ¡aw Cyganek1,2, MichaÃ¡ WoÄ¨niak2
1
2

AGH University of Science and Technology, Al. Mickiewicza 30, 30-059 KrakÃ³w, Poland
Wroclaw University of Technology,WybrzeÄªe WyspiaÄ”skiego 27, 50-370 WrocÃ¡aw, Poland
cyganek@agh.edu.pl, Michal.Wozniak@pwr.wroc.pl

Abstract
In this paper new methods for fast computation of the chordal kernels are proposed. Two versions of the
chordal kernels for tensor data are discussed. These are based on different projectors of the flattened
matrices obtained from the input tensors. A direct transformation of multidimensional objects into the
kernel feature space leads to better data separation which can result with a higher classification accuracy.
Our approach to more efficient computation of the chordal distances between tensors is based on an
analysis of the tensor projectors which exhibit different properties. Thanks to this an efficient eigendecomposition becomes possible which is done with a version of the fixed-point algorithm.
Experimental results show that our method allows significant speed-up factors, depending mostly on
tensor dimensions.
Keywords: Kernel methods, chordal kernel, tensor, subspace classification

1 Introduction
Computation science is about development of new methods or modifications of the existing ones for
scientific computations. This paper addresses the problem of development of efficient kernel methods
for multi-dimensional pattern recognition.
Development of kernel methods opened new possibilities in pattern recognition and data analysis
disciplines. The main idea is to transform data into a usually higher dimensional space, so called feature
space, in which data classification is easier to achieve due to better separation of data belonging to
different classes [3]. Also, due to the so called kernel trick, computations need not to be directly
performed in this high dimensional space [12][4]. All these lead to flourish of kernel based methods in
various classification tasks [19][20][11]. Nevertheless, majority of kernel methods were restricted to
one-dimensional vector spaces, although processed data, such as video, hyperspectral, etc. usually are
of higher dimensions. To cope with this problem, tensor methods were introduced to the pattern
recognition community [13][14][10][4]. However, until recently there was no direct link of the tensor
multi-dimensional data to the kernel domain. Only recently, such kernels were proposed by Signoretto
et al. [18] based on subspace distances studied by Hamm and Lee [9]. They introduced a chordal kernel
1702

Selection and peer-review under responsibility of the Scientiï¬c Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.511

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

to deal with tensors on Grassmann manifolds. When operating with two tensors of dimension L, the
main idea is to accumulate L distances among pairs of corresponding subspaces spanned by different
flattenings of the two tensors. The chordal tensor showed many useful properties in pattern
classification. These were verified in the paper by Signoretto et al., as well as in the work by Cyganek
et al. [5].
In this paper two different chordal kernels are analyzed. An efficient algorithm for computation of
the chordal kernel is also developed and presented. It allows much faster computations especially for
the big data objects, as will be presented. The original input of this paper is an analysis of chordal
distances for tensor data which are based on the complementary projectors, as well as introduction of
the fixed-point eigen-decomposition method to computation of the tensor subspaces. The proposed
methodology allows shortening computation times by few times as compared to a complete subspace
computation with the full singular value decomposition of the flattened representations of the input
tensors. Properties of the proposed method were verified in computation of the kernel matrix between
3D objects of color images.

2 Chordal Kernels for Tensor Data
The chordal kernel allows direct computation of the kernel function on tensor objects [18]. Application
of both, high dimensional tensor methods and kernel computations, in many domains leads to superior
results [18][5][4]. In this section we present only a brief outline of the chordal kernel while details can
be found in publications [18][5]. Details of tensor processing can be found in literature [4][10].
For a brief presentation, let us define a tensor
 ÂÂƒ

N 1 uN 2 uN L

(1)

,

which can be perceived as a L-dimensional cube of data of real valued data, with each dimension
corresponding to a different factor of the contained data. From a tensor  the following matrix can be
created, as follows
A

j 

ÂÂƒ



N j u N 1N 2 N j  1N j  1 N L

,

(2)

j
j
where columns of A   are called j-mode vectors of . The matrix A   is a j-mode tensor flattening
j
[14][4]. In the above, the j denotes a row index of A , whereas its column index is a product of all the
rest L-1 indices of the tensor .
j
With the above definitions, for two tensors  and  and their j-th flattened mode matrices A and
j
B , their chordal distance is defined as a distance between projections of the subspaces of the
corresponding tensors, as follows [9][18]



 
D1 A , B

where P

Â§ T j Â·
Â¸
RÂ¨A
Â¨
Â¸
Â©
Â¹

j

j



Â§
Â·
DF Â¨ P Â§ T  j  Â· , P Â§ T  j  Â· Â¸
Â¨Â¨ R Â¨ A Â¸ RÂ¨ B Â¸ Â¸Â¸
Â© Â¨Â© Â¸Â¹ Â¨Â© Â¸Â¹ Â¹

2

P

Â§ T j Â·
Â¸
RÂ¨ A
Â¨
Â¸
Â©
Â¹

P

Â§ T j  Â·
Â¸
RÂ¨ B
Â¨
Â¸
Â©
Â¹ F

,

(3)

    of a transpose of a matrix A  . It can be
T j

denotes a projector onto the range R A

j

shown that [17][6]
1703

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

j  T j 
DA ,1 DA ,1 ,

P

Â§ T j  Â·
Â¸
RÂ¨A
Â¨
Â¸
Â©
Â¹

(4)

j 
j
where DA,1 is a matrix from the SVD decomposition of the j-th flattening matrix A of the tensor ,

that is
A

j 

j  j  T j 
S V D

ÂªS j 
Â«Â¬ A,1

Âª j 
j  V
S A,2 Âº Â« A,1
Â»Â¼ Â« 0
Â¬

T j 
0 Âº ÂªÂ« DA ,1 ÂºÂ»
Â» Tj .
0 Â» Â« DA,2  Â»
Â¼Â¬
Â¼

(5)

j 
j 
It is well known that columns of DA,1 and columns of SA,1 constitute orthogonal bases for the ranges

    and R A   , respectively. Therefore, we can defined another projector directly associated

with R  A  , which can be computed from (5) as follows [17]
T j

j

R A

j

P

Â§ j  Â·
RÂ¨ A Â¸
Â¨
Â¸
Â©
Â¹

j  T j 
S A,1S A,1 .

(6)

In consequence we can define second chordal distance for two tensors  and , as follows [15]

D2



j

j
A ,B

2

Â§
Â·
Â¨
DF P Â§  j  Â· , P Â§  j  Â· Â¸
Â¨Â¨ RÂ¨ A Â¸ R Â¨ B Â¸ Â¸Â¸
Â© Â©Â¨ Â¹Â¸ Â©Â¨ Â¹Â¸ Â¹



P

Â§ j Â·
RÂ¨ A Â¸
Â¨
Â¸
Â©
Â¹

P

Â§ j Â·
RÂ¨ B Â¸
Â¨
Â¸
Â©
Â¹ F

.

(7)

Both chordal distances between tensors, given in equations (3) and (7), respectively, convey
information on distances between different subspaces, however associated with the same tensors. The
former was proposed by Signoretto et al. [18], whereas the latter was used in the paper by Liu et al.
[15]. Both chordal distances exhibit different properties which are under scrutiny. However, both also
showed very good properties in the pattern recognition algorithms designed to directly operate in
multidimensional data spaces. Finally, both distances and in the same way were used to construct proper
kernels for tensor data. That is, a kernel for the j-th mode is defined as follows [18][15]



K cj , 



Â§ 1
Â·
exp Â¨  2 Dc ,  Â¸ ,
Â© 2V
Â¹







(8)



where Dc  ,  are given by (3) and (7), respectively, and V is a parameter. Finally, the full kernel is
obtained as a product of all j-th mode kernels, as follows



K c , 

L

 Â– K ,   .
j 1

c
j

(9)

Inserting (8) into (9) and considering (3) and (7), the following two kernels are obtained



K 1 , 

1704

L

 Â– exp Â¨  2V1
j 1

Â§
Â©

2

j Tj
j Tj
DA,1DA,1  DB,1DB,1

2

Â·
Â¸,
F Â¹

(10)

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

and



K 2 , 

L

 Â– exp Â¨  2V1
Â§
Â©

j 1

j Tj
j Tj
SA,1SA,1  SB,1SB,1

2

2

Â·
Â¸.
F Â¹

(11)

Kernel (10) was also used to assess its properties with the SVM [5]. An equivalent formula to (10)
which is slightly more convenient for computations is also presented in [5]. Nevertheless, computation
of either kernel requires computations of 2L times of the SVD decompositions, which in a case of large
tensors might pose problems. To remedy this problem we propose a faster computation of the limited
number of leading components.

2.1 Efficient Computation of the Chordal Kernel
Let us observe that the chordal kernel, outlined in the previous section, is based on distance between
subspaces. This is expressed by a distance between respective projectors as in (3) or (6). However, for
each projector there is a perpendicular projector PA . These are related as follows [17]

PA

I  P.

(12)

Analogously, the following hold
P

IP

Â§ j  Â·
N Â¨A Â¸
Â¨
Â¸
Â©
Â¹

Â§ T j  Â·
Â¸
RÂ¨ A
Â¨
Â¸
Â©
Â¹

P

IP

Â§ T j  Â·
Â¸
N Â¨A
Â¨
Â¸
Â©
Â¹

Â§ j Â·
RÂ¨ A Â¸
Â¨
Â¸
Â©
Â¹

,

(13)

,

(14)

where N(M) denotes a null space of a matrix M. On the other hand, the chordal distances between related
projectors and their perpendiculars are the same. This can be easily shown, as follows
2

DF2 Â§Â¨ P  j  , P  j  Â·Â¸
AB Â¹
Â© AA
2

P
B

j

P
A

j

F

P
AA

j 

P
AB

j

Â§I  P Â·  Â§I  P Â·
Â¨
Â¨
j Â¸
j  Â¸
A Â¹
B Â¹
Â©
Â©

F

2

F

(15)

D Â§Â¨ P  j  , P  j  Â·Â¸ .
B Â¹
Â© A
2
F

This means also that, alternatively, the chordal distances can be computed from the respective null
j 
j 
spaces, that is from the matrices DA,2 and SA,2 in (5), assuming that the null space is not empty. This
property can be used to speed up computations, depending on the sizes of the range and null spaces,
respectively.
As alluded to previously, computation of the chordal kernel requires a number of SVD
decompositions from which successive subspaces are obtained. These, in turn, can be computed by
means of the fast eigen-decomposition based on the fixed-point methods, as proposed by Bingham and
HyvÃ¤rinen [1]. However, this algorithm requires a symmetrical matrix. Thus, starting from (5) the
following can be easily obtained
  j 
A

T j

A



T j

D

V

  j  .
D

2 j

(16)

1705

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

T j  j 
j
Thus, eigen-decomposition of A A directly leads to computation of the D matrices.
j
However, since A is a j-th flattened version of a tensor, then usually it is a rectangular matrix of
dimensions NuM, where NÂ«M. Let us recall that computational complexity of the eigen-decomposition
of a matrix of dimensions NxN is O(N3) [8]. Therefore, in many practical computations, eigenT j  j 
decomposition of the product A A might be computationally prohibitive due to a huge size of this
matrix. In this case, we arrive to the following computation

j  T j 
A A

 j  2 j  T  j 
S V S .

(17)

However, in majority of real cases, size of the matrix AAT in (17) is much lower than ATA in (16).
This is due to the construction of the flattening matrix in which the row dimension is equal to the j-th
dimension of the tensor, whereas the column dimension equals product of all other dimensions of that
tensor. Thus, we have to choose right algorithm for decomposition depending on dimensions of the input
tensor, as will be discussed.

2.2 Fast Eigen-decomposition Algorithm
In this work, a full SVD decomposition of the flattened matrix of the input tensors will be substituted
with computation of only a limited number of leading eigenvectors. Following the works by Bingham
and HyvÃ¤rinen [1], for this purpose we propose to use the fixed-point algorithm, as shown in Algorithm
1.
Algorithm 1: Fixed-point eigen-decomposition method. Input â€“ a symmetric matrix
C, a number of eigenvectors kmax. Output â€“ kmax leading eigenvectors of C.
1:
2:
3:
4:
5:

 0
Initialize vector e0 with random values; set threshold Uth ; k m 0

for k < kmax
im1
do
i 
i 1
ek m Cek
i 
Gram-Schmidt orthogonalization of ek to eigenvectors e0d j k :

6:

7:

^



i 
i  i 
Normalization ek : ek m ek
  i 
ek ek  1
T i 1

8:

U

9:

imi+1

while U ! Uth
10:
11: end for

1706



k 1
T i 
i 
i 
ek m ek  Â¦ j 0 ek e j e j

i 
ek

2

`

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

The method allows computation of kmax leading eigenvectors of a matrix C which are orthogonal
[15]. However, the algorithm is iterative, although in practice it converges fast. For its input, the
orthogonality threshold Uth needs to be provided.
Algorithm 1 relies on a number of known methods of eigenvalue computation. Step 5: draws from
the power method [6]. It allows computation of only the first eigenvector associated with the largest
eigenvalue, however. Thus, to compute the rest of eigenvectors, a deflation scheme based on the GramSchmidt orthogonalization is employed (step 6: of Algorithm 1). Consecutively, each k-th eigenvector
is obtained from the already estimated k-1 orthonormal eigenvectors, subtracting all their projections
onto the k-th one, and so on. As will be shown, this algorithm allows much faster computation of the
tensor chordal kernel based on representation (7).
After computation of the kmax leading eigenvectors, the corresponding eigenvalues can be easily
obtained by means of the following equation



eTk Aek

Ok eTk ek



Ok .

(18)

These are used to guide the process of computation of the consecutive eigenvectors. In other words,
if eigenvalues drop below a preset threshold, the process of computation is immediately stopped which
reduces dimensions of the obtained subspace. Alternatively, a fixed number of eigenvectors can be
assumed. This approach was also used in our experiments, as will be discussed.

3 Experimental Results
The methods presented in this paper were implemented in C++ using the Microsoft Visual 2013.
Presented experiments were run on a laptop computer equipped with the IntelÂ® Coreâ„¢ i7-4800MQ
CPU @2.7GHz, 32GB RAM, as well as OS 64-bit Windows 7. For experiments the kernel matrix was
computed ten times and average run-time results are presented. Elements of the kernel matrix K are
values computed in accordance with (11), that is

k pq





K 0p , 0q ,

(19)

where 0p denote L-dimensional tensors. In our experiments 0 p are 3D tensors of five exemplary
images taken from the Georgia Tech Face Database [7], shown in Figure 1.

Figure 1: Examples 3D tensor objects used in experiments. Exemplary images come from the Georgia Tech Face
Database [7]. The database contains 50 subjects in different poses.

From the test exemplars different configurations were arranged and used to measure performance of
the new chordal distance algorithm. Their specific settings are shown in Table 1, whereas timing plots
are depicted in Figure 2, Figure 3, and Figure 4, respectively.

1707

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

Figure 2: Timing diagrams for configuration no. 1 in Table 1.

Figure 3: Timing diagrams for configuration no. 2 in Table 1.

In all experiments the number of important components used for computation of the chordal tensor
was set to 25. This value was used in our experiments presented in [5].

1708

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

Table 1: Experimental configurations.
Plot no.

Figure 2

Figure 3

Figure 4

Test no.

Num. of tensors

Tensor dimensions

Max. components

1

3

141x196x3

25

2

3

181x241x3

25

3

5

181x241x3

25

1

3

95x145x3

25

2

3

186x311x3

25

3

3

382x582x3

25

1

5

95x145x3

25

2

5

201x311x3

25

3

5

382x582x3

25

In all experiments the number of important components used for computation of the chordal tensor
was set to 25. This value was used in our experiments presented in [5].

Figure 4: Timing diagrams for configuration no. 3 in Table 1.

Also interesting is to notice that the higher speed-up is obtained for larger number of tensor objects,
as well as for larger dimensions of the tensors. For instance, for a configuration with 5 tensors of size
382x582x3, the speed-up exceeds 25.

1709

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

However, we need to remember that for the fast fixed-point eigen-decomposition method, presented
in Algorithm 1, we need to compute a product of the flattened matrices. It is either (16) or (17),
depending on a type of the chordal kernel, as it was discussed in Section 2.1.
Table 2: Speed-up results for different test configurations.
Plot no.

Figure 2

Test no.

1

2

3

4

5

6

7

8

9

Speed up

5.15

6.64

6.67

3.96

7.56

25.06

3.94

7.46

25.36

Figure 3

Figure 4

Table 2 presents speed-up results for different test configurations. We can observe that the speed-up
factor is from almost 4 up to over 25.
Thus, if a number of columns of the matrix A in (16), or rows in (17), is very large, then a direct SVD
decomposition should be employed to avoid construction and then decomposition of a huge product
matrix.

4 Conclusions
In this paper methods for efficient computation of the chordal kernel for tensor data were presented
and analyzed. Most importantly, we showed that two different chordal distances can be computed from
the perpendicular projectors which opens the way for application of the fast fixed-point eigendecomposition method. Thanks to this, speed-up factor from 3 to 25 was achieved. Summarizing, based
on the presented analysis and experiments, the following conclusions can be formulated:
(1) There are two tensor chordal kernels, given in equations (10) and (11), which correspond to two
T j 
j
and R A
of the tensor flattening matrices, respectively.
ranges R A

 

 

(2) Both, the range and null space projectors, defined in (13) and (14) respectively, lead to the same
chordal distance, as shown in (15). This property can be used if computation of the perpendicular
projectors is faster.
(3) The fast fixed-point eigen-decomposition algorithm leads to significant computation speed-up if
the product matrix, (16) or (17), respectively, has smaller dimensions than the maximal
dimension of the original matrix A, otherwise
(4) a direct SVD decomposition should be employed to avoid construction and then decomposition
of a large product matrix.
Furthermore, since eigen-decompositions of tensor subspaces for each dimension of a tensor are
computed independently, there is an open way for parallel implementation. This can further shorten
computation time, efficiently exploiting multi-core processor platforms. Further research will be
devoted to analysis of the properties of the chordal kernel in different machine learning tasks, as well as
to development of new and more efficient implementations.

Acknowledgement
This work was also supported by the Polish National Science Center under the grant no. DEC2014/15/B/ST6/00609. This work was supported by EC under FP7, Coordination and Support Action,
Grant Agreement Number 316097, ENGINE â€“ European Research Centre of Network Intelligence for
1710

Eï¬ƒcient Computation of the Tensor Chordal Kernel

B. Cyganek and M. Wozniak

Innovation Enhancement (http://engine.pwr.wroc.pl/). All computer experiments were carried out using
computer equipment sponsored by ENGINE project.

References
1.

2.
3.
4.
5.

6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

16.
17.
18.
19.
20.

Bingham E., HyvÃ¤rinen A. (2000). A Fast Fixed-Point Algorithm For Independent Component Analysis of
Complex Valued Signals. International Journal of Neural Systems, Vol. 10, No. 1, World Scientic Publishing
Company
Cichocki, A., Zdunek, R., Amari, S. (2008). Nonnegative Matrix and Tensor Factorization. IEEE Signal
Processing Magazine, Vol. 25, No. 1, pp. 142-145
Cortes C.,Vapnik V. (1995). Support-vector networks.Mach.Learn.20, pp. 273â€“297
Cyganek, B. (2013). Object Detection and Recognition in Digital Images: Theory and Practice, Wiley
Cyganek B., Krawczyk B., WoÄ¨niak M. (2015). Multidimensional Data Classification with Chordal Distance
Based Kernel and Support Vector Machines. Engineering Applications of Artificial Intelligence, Elsevier, Vol.
46, Part A, pp. 10â€“22
Demmel J.W. (1997). Applied Numerical Linear Algebra. Siam
Georgia Tech Face Database, 2013. http://www.anefian.com/research/face_reco.htm
Golub, G.H., van Loan, C.F. (2013). Matrix Computations. Johns Hopkins Studies in the Mathematical
Sciences. Johns Hopkins University Press
Hamm, J., Lee, D. (1997) Grassmann discriminant analysis: a unifying view on subspace-based learning. In
ACM Proceedings of the 25th international conference on machine learning, pp. 376â€“383
Kolda, T.G., Bader, B.W. (2008). Tensor Decompositions and Applications. SIAM Review, pp. 455-500
Krawczyk B. (2015). One-class classifier ensemble pruning and weighting with firefly algorithm.
Neurocomputing 150, pp. 490-500
Kung S.Y. (2014). Kernel Methods and Machine Learning. Cambridge Universisty Press
Lathauwer, de L. (1997). Signal Processing Based on Multilinear Algebra. PhD dissertation, Katholieke
Universiteit Leuven
Lathauwer, de L., Moor de, B., Vandewalle, J. (2000). A Multilinear Singular Value Decomposition. SIAM
Journal of Matrix Analysis and Applications, Vol. 21, No. 4, pp. 1253-1278
Liu C., Wei-sheng X., Qi-di W. (2013). Tensorial Kernel Principal Component Analysis for Action
Recognition. Mathematical Problems in Engineering, Vol. 2013, Article ID 816836,
doi:10.1155/2013/816836.
Marot J., Fossati C., Bourennane S. (2008). About Advances in Tensor Data Denoising Methods. EURASIP
Journal on Advances in Signal Processing
Meyer C.D. (2001). Matrix Analysis and Applied Linear Algebra Book and Solutions Manual. SIAM
Signoretto M., De Lathauwer L., Suykens J.A.K. (2011). A kernel-based framework to tensorial data analysis.
Neural Networks 24, pp. 861â€“874
WoÄ¨niak M. (2011). A hybrid decision tree training method using data streams. Knowledge Information
Systems, 29(2), pp. 335â€“347
WoÄ¨niak, M., Grana, M., Corchado, E. (2014). A survey of multiple classiÂ¿er systems as hybrid systems.
Information Fusion 16(1), pp. 3â€“17

1711

