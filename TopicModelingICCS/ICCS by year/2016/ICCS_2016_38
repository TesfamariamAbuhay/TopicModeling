Procedia Computer Science
Volume 80, 2016, Pages 190‚Äì200
ICCS 2016. The International Conference on Computational
Science

A Multi-GPU Fast Iterative Method for Eikonal Equations
using On-the-Ô¨Çy Adaptive Domain Decomposition
Sumin Hong and Won-Ki Jeong
Ulsan National Institute of Science and Technology
Ulsan, Republic of Korea
{sumin246,wkjeong}@unist.ac.kr

Abstract
The recent research trend of Eikonal solver focuses on employing state-of-the-art parallel
computing technology, such as GPUs. Even though there exists previous work on GPU-based
parallel Eikonal solvers, only little research literature exists on the multi-GPU Eikonal solver
due to its complication in data and work management. In this paper, we propose a novel onthe- y, adaptive domain decomposition method for e cient implementation of the Block-based
Fast Iterative Method on a multi-GPU system. The proposed method is based on dynamic
domain decomposition so that the region to be processed by each GPU is determined on-the- y
when the solver is running. In addition, we propose an e cient domain assignment algorithm
that minimizes communication overhead while maximizing load balancing between GPUs. The
proposed method scales well, up to 6.17√ó for eight GPUs, and can handle large computing
problems that do not t to limited GPU memory. We assess the parallel e ciency and runtime
performance of the proposed method on various distance computation examples using up to
eight GPUs.
Keywords: Eikonal Equation, GPU, parallel computing, domain decomposition

1

Introduction

The eikonal equation has a wide range of applications including geoscience [1], computer vision [8], image processing [7], path planning [5] and computer graphics [10], and is de ned as
follows:
H(x ‚àáŒ≤) = |‚àáŒ≤(x)|2 ‚àí
Œ≤(x) = 0 x ‚àà

1
f 2 (x)

= 0 ‚àÄx ‚àà

‚äÇ Rn

(1)

‚äÇ

is a computational domain de ned on an n-dimensional rectilinear grid,
is the
where
collection of seed points (i.e., boundary condition), Œ≤(x) is the rst arrival time from the seed
190

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.309

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

region to the location x, and f (x) is a scalar speed function de ned on x. We refer node as
a grid point on
de ned by an n-tuple of numbers (i j k). On a discrete grid, we can use
the rst order Godunov upwind discretization g(x) of the Hamiltonian H(x ‚àáŒ≤) to numerically
solve Equation (1) as shown in [8, 4]. In this paper, we focus on the three dimensional case
only (n = 3). The solution of the eikonal equation represents wave propagation from the seed
region where the motion is governed by the speed function.
The most popular Eikonal solvers include Fast Marching Method(FMM) [9] and Fast Sweeping method(FSM) [11]. FMM is based on the label-setting algorithm similar to Dijkstra s
shortest path algorithm. Even though FMM is worst-case optimal, its parallel implementation
is challenging due to the serial nature of the algorithm. FSM belongs to the label-correcting
algorithm, i.e., the solution is iteratively updated until the entire domain converges to a steady
state. FSM uses a special computing sequence, i.e., Gauss-Seidel update, in order to increase
the convergence rate. Zhao et al. [12] parallelized FSM by executing each Gauss-Seidel update
concurrently using parallel processors, but the algorithm restricts the maximum concurrency
to eight for 3D grids. Later, Weber et al. [10] introduced an alternative discretization and
update sequence to overcome the limitation of Zhao s parallel FSM, but this method only deals
with Eikonal equations de ned on parametric surfaces. More recently, a similar idea has been
implemented on a single [2] and multiple GPUs [6], respectively, to solve Eikonal equations on
rectilinear grids. Even though Krishnasamy et al. [6] attempted to address the same problem
as ours, i.e., multi-GPU Eikonal solver on a shared memory system, their approach is based on
a static domain decomposition and is not e ciently applicable to adaptive algorithms like our
method. Fast Iterative Method(FIM) [4] is a variant of the label correcting algorithm speci cally designed to exploit ne-grain parallelism of recent many-core processors, such as GPUs.
The algorithm manages active list, a collection of grid nodes being updated by the solver. Unlike
FMM, FIM does not manage expensive ordered data structures, and the entire list is updated
concurrently. Activation of node is determined by convergence, i.e., the solution on the node
does not change over consecutive iterations, and nodes are allowed to be re-activated even after
removed from the active list. Jeong et al. showed that FIM can be e ciently implemented on
a single GPU by employing a block-based update scheme, called BlockFIM [4].
Even though BlockFIM showed good performance on a single GPU, it is not straightforward
to leverage multiple GPUs due to the adaptive nature of the algorithm meaning that a naive
static domain decomposition, which works well on other iterative solvers like FSM, may result
in load imbalance and excessive communication overhead. To address this issue, we propose
an on-the- y adaptive domain decomposition method for multi-GPU BlockFIM. The core idea
is that each GPU owns its disjoint local sub-domain that progressively grows as the active
list expands. Our proposed algorithm estimates the regions to be computed in the following
iteration and assigns under-utilized GPUs with a higher priority while promoting the same
GPUs to be assigned adjacent to each other. By doing so, we can maximize load balancing
and minimize communication between GPUs. The proposed method showed up to around 2√ó
speed up compared to naive static domain decomposition methods.

2

Multi-GPU Extension of BlockFIM

BlockFIM is a variant of FIM speci cally designed for Single Instruction Multiple Data (SIMD)
architecture, such as GPUs. BlockFIM splits the computational domain into disjoint blocks
where each block consists of multiple nodes (in our experiments, we use an 8 √ó 8 √ó 8 block), and
treats the block as a basic compute primitive meaning that the nodes in the same block are
updated concurrently by the parallel computing cores in the GPU. Therefore, BlockFIM man191

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

ages the active list that contains blocks instead of nodes, and the algorithm iteratively updates
solutions of the active blocks. However, the previous BlockFIM targets only a single GPU,
so we propose a multi-GPU extension of BlockFIM using a domain decomposition method,
which splits the input computational domain into sub-domains and assigns them to GPUs.
Each sub-domain is a collection of blocks, and partially overlaps with its adjacent sub-domains
around its boundary, called halo (Figure 1), which allows each sub-domain to be processed
independently on diÔ¨Äerent GPUs. When a boundary block is updated, then its adjacent halo
region must be synchronized accordingly via communication between GPUs. Since the amount
of communication depends on the halo size, domain decomposition strategies signiÔ¨Åcantly aÔ¨Äect
the performance of the solver.
Halo

Sub-domain 1

Sub-domain 2
Halo

Figure 1: Halo at the boundary of adjacent sub-domains

(a) 1-axis, multi-split

(b) 3-axis, single-split

(c) 3-axis, multi-split

Figure 2: Examples of regular domain decomposition on a 3D rectilinear grid. (a) is multiple
splits along one axis, (b) is single split along each axis, and (c) is multiple splits along each
axis.

2.1

Regular Domain Decomposition

A commonly used domain decomposition strategy is a simple axis-aligned grid splitting as
shown in Figure 2. Most of existing parallel Eikonal solvers also employ a regular domain
decomposition method ‚Äì Herrmann et al. [3] parallelized FMM [9] on regular sub-domains
using multiple computing processes. The proposed algorithm maintains an independent local
heap list to select the computing node on each sub-domain, which signiÔ¨Åcantly impairs the
192

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

scalability on massively parallel architecture. Krishnasamy et al. [6] extended a 3D parallel
sweeping method [2] on multiple GPUs. The solver uses plane sweeping, i.e., a 2D plane sweeps
the 3D volume along axis-aligned sweep directions while the entire 2D plane is updated in
parallel. In order to parallelize sweeps over multiple GPUs, domain decomposition is restricted
to 1D axis-aligned splitting along the sweep direction (as in Figure 2 (a)).
Any domain decomposition strategy can be applied to multi-GPU BlockFIM because blocks
in active list can be independently updated. However, the shape of active list can be arbitrary
depending on the input speed function, so evenly distributing blocks across regular sub-domains
is infeasible in most cases. For example, Figure 3 shows regular domain decomposition of a 2D
grid where circles represent blocks and rectangles represent sub-domains (in this example, the
input grid is decomposed into four sub-domains, and each sub-domain consists of nine blocks).
Assume each sub-domain is assigned to a GPU. In Figure 3 (a), the initial active list is evenly
distributed across four sub-domains so that two blocks are located in each sub-domain. After
one BlockFIM update, only a half of the active list converged (Figure 3 (b), marked in green).
In the next iteration, the green blocks will be removed from the active list and some of its
adjacent neighbor blocks are activated (Figure 3 (c)). Because the active list expands towards
the bottom-left direction, more active blocks are located in the bottom-left region (four blue
circles) than other sub-domains (three blue circles in top-left and bottom right regions, and two
blue circles in top-right region), which introduces load imbalance. This was not the case for
other parallel Eikonal solvers, especially sweeping algorithms, because they are not relying on
active list and update the entire grid per each sweeping iteration. Therefore, we need a more
exible domain decomposition method to address this issue.

(a)

(b)

(c)

Figure 3: Example of load imbalance in regular domain decomposition. Blue circles are active
blocks, green circles are converged blocks, and stripe circles are candidate blocks to be active
in the following iteration. Due to partial convergence of active list, blocks are not evenly
distributed in (c).

2.2

On-the-Ô¨Çy Adaptive Domain Decomposition

The problem of regular domain decomposition discussed above is mainly due to the fact that
dynamics of the active list cannot be determined in advance. In FIM, the active list dynamically
grows or shrinks depending on the speed value de ned on each grid node in a nonlinear fashion.
Therefore, the strategy we propose is to dynamically decompose the domain as the active list
193

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

propagates meaning that as the active list expands, we incrementally expand sub-domains so
that roughly the same number of active blocks are assigned to each sub-domain to maximize
load balance. Another important performance metric to consider is communication cost for
halo synchronization meaning that we try to minimize the halo size between di erent subdomains. For this, blocks assigned to the same sub-domain should be spatially clustered as
much as possible.
For e cient implementation of the proposed dynamic domain decomposition, we use a
simple block-to-subdomain mapping table, i.e., Domain Mapping Table (DMT). When the
solver starts, the DMT entries for initial active blocks are lled with their corresponding subdomain index (can be assigned randomly at the beginning). The DMT entries for other nonactive blocks are marked as un-assigned. As the active list expands, some of active blocks
are converged and removed from the active list, and some non-active blocks are activated and
inserted into the active list. For newly activated blocks, we check their DMT entries and
only un-assigned block s sub-domain index is assigned using the domain assignment algorithms
discussed below.
Domain Assignment Algorithm In order to consider both load balancing and communication cost, we propose Algorithm 1 based on the following strategies. First, we assign adjacent
blocks to the same sub-domain as much as possible, i.e., if two blocks are adjacent each other
then they belong to the same sub-domain, to increase clustering and reduce halo communication. This can be done by assigning a new block to a sub-domain that one of its adjacent
neighbor blocks belong to. To further reduce the communication cost between sub-domains,
we choose the largest adjacent sub-domain when assigning a new block. For example, Figure 4
(a), the orange circle (center) is the new block to assign a sub-domain. It has three adjacent
neighbor blocks whose sub-domains are already assigned (left, right, and bottom). Among
them, the left and bottom blocks are assigned to the blue sub-domain, while the right block is
assigned to the green sub-domain. Since the block sub-domain is the largest (i.e., assigned to
two blocks) among the adjacent blocks, the orange block is assigned to the blue sub-domain.
This is mainly for reducing the size of boundary between adjacent sub-domains as shown in the
Figure. However, this strategy does not consider fair assignment of sub-domains because it does
not take into account the global information (i.e., the size of active lists). Therefore, the second
strategy we propose is to enforce load balance between newly assigned blocks. We repeatedly
move blocks from the largest to the smallest active list until its di erence becomes less than
the user given threshold ( 1 ). Algorithm 1 shows the entire procedure as the combination of
these two strategies.
Load balancing in Algorithm 1 is mainly focusing on matching task loads among sub-domains
for the next iteration. However, frequent load balancing will impair clustering of sub-domains
and increase communication overhead. Thus, we need to perform load balancing as sparsely
as possible. One strategy is performing load balancing once per every pre-determined number
of iterations, or using a metric to detect load imbalance. In this paper, we use the standard
deviation of the size of active lists as the load imbalance metric.
Improved Clustering Algorithm Algorithm 1 shown above tries to address two problems,
load balance and clustering, which are intrinsically orthogonal. The solution we provide is
performing load balancing periodically but not too frequently, but there exists a room for
improvement especially in clustering. In order to increase the performance further, we propose
an improved clustering algorithm using a local load balancing strategy. The idea is that when
we insert an unassigned block v to a temporally list Q, we examine not only the count of
adjacent sub-domains but also the number of expected task loads for each sub-domain. Simply
194

Multi-GPU FIM for Eikonal Equation

(a)

Sumin Hong and Won-Ki Jeong

(b)

(c)

Figure 4: Domain assignment for clustering. (a) Orange circle (center) is an un-assigned block
that has two blue and one green adjacent blocks, (b) blue is assigned to the center, and (c)
green is assigned to the center. Arrows represent communication across sub-domains.
speaking, this approach tries to maximize clustering as well as load balancing between adjacent
sub-domains using estimated active list size ci where Œ± and Œ≤ aÔ¨Äect how fast the list grows
relatively. If cj and ck are similar, i.e., the size of largest and smallest active lists are similar,
then there is no problem in load balancing so the current block should be assigned to Qj to
promote clustering. Otherwise, the block is assigned to Qk to improve load balance because
k is the sub-domain that has the smallest expected active list at this moment. Algorithm 2
describes the proposed improved clustering algorithm, and this code block can simply replace
the clustering code in Algorithm 1 from line 1 to 7 (assume P is collected from L in advance).

3

Result

We evaluate the performance of proposed method on a computing server equipped with eight
NVIDIA Tesla M2080 GPUs (each has 512 CUDA cores and 6GB device memory). We implement the experiment code in C++ and CUDA 6.0 with OpenMP with the -O3 level optimization. The grid dimension is 6403 , and a single seed is located at [1/8 , 1/8 , 1/8] on the
normalized domain Œ© = [0, 1] [0, 1] [0, 1]. The seed location is intentionally placed near
the corner to show the worst-case performance of the algorithm for unbalanced task loads. All
computations are conducted in double precision, and the block width is 8. The speed maps
used in our experiments are deÔ¨Åned as follows (and their distance maps are shown in Figure 5):
We tested three regular decomposition methods and our on-the-Ô¨Çy adaptive decomposition
methods. Each decomposition generates same number of sub-domains as the number of GPUs
(except 3-axis multi-split) so that each sub-domain is assigned to a single GPU. 1-axis multisplit method splits the data uniformly only along z-direction. 3-axis single-split method splits
the data along the diÔ¨Äerent axis whenever the number of GPUs is doubled. 3-axis multi-split
method splits the data multiple times along each axis to generate more sub-domains than the
number of GPUs (we use 163 sub-domain size) and assign GPUs to sub-domains multiple times
in a checkerboard fashion. For our methods, we used the empirically-obtained best parameters
values Œ±, Œ≤, Œµ1 , Œµ2 , and Œ¥ as 1, 6, 50, 1.05, and 0.1, respectively. Table 1 lists the running time
of each domain decomposition method on diÔ¨Äerent speed maps using various number of GPUs.
Among regular decomposition methods, 3-axis multi-split shows the best performance due to
its ability to distribute tasks evenly over many small sub-domains. Our on-the-Ô¨Çy adaptive
195

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

Algorithm 1: Domain Assignment Algorithm
Input: Active List L, Domain Mapping Table T
/* N : total number of sub-domains */
/* Li : sub active list for sub-domain i (L = L1 ‚à™ L2 ‚à™ ‚à™ LN ) */
/* 1. Block clustering for reducing halo communication */
/* Pi is the list of already assigned blocks in Li */
/* Qi is the list of unassigned blocks in Li */
1 forall the i = 1 to N do
2
forall the v ‚àà Li do
3
if T (v) = unassigned then
4
j ‚Üê index of sub-domain adjacent to v that appears most
5
Insert v to Qj
else
Insert v to Pi

6
7

8
9
10
11
12
13

/* 2. Load balancing between Q1
QN */
if stddev(L1
LN ) > then
repeat
forall the i = 1 to N do
ni ‚Üê size of Pi + size of Qi
imax ‚Üê i if ni ‚â• nj for all j = 1 i ‚àí 1
imin ‚Üê i if ni ‚â§ nj for all j = 1 i ‚àí 1
= nimax ‚àí nimin
Load balancing by moving blocks from Qimax to Qimin
until > 1

14
15
16

17
18
19

/* Update Domain Mapping Table and Collect Active List */
forall the i = 1 to N do
forall the block v ‚àà Qi do
T (v) ‚Üê i

20

Li ‚Üê Pi ‚à™ Qi

decomposition (Algorithm 2) outperformed regular decomposition in most cases, up to 3.3√ó
and 6.1√ó speed up on four and eight GPUs, respectively.
Figure 6 shows the amount of task (upper curves) and data communication (lower curves)
for Map 1 and 3 running on four GPUs. We select these two maps as representative results
because they are the fastest and slowest results, as shown in Table 1. As shown in Figure 6 s
left column, the amount of task per GPU diverges as iteration goes but periodically adjusted
by load balancing algorithm. The middle column is the case that load balancing is performed
more frequently, which makes task distribution more evenly but communication cost increases
(because global load balancing in Algorithm 1 does not take into account clustering). However,
due to the improved clustering method in Algorithm 2, the right column shows better load
balancing with much low communication overhead.
Figure 7 shows weak scaling e ciency on Map 1 and Map 3 to elaborate how each decomposition method can handle large data that does not t to a single GPU. We assign 2 GB of
data per GPU, so up to 16 GB of data is processed by eight GPUs. Similar to strong scalability
196

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

Algorithm 2: Improved Clustering Algorithm using Local Load Balancing

1
2
3
4
5
6
7
8
9
10

/* N : total number of sub-domains */
LN , Assigned Block List P1 PN
Input: Active List L1
Output: Unassigned block list Q1
QN
/* ci : expected size of active list Li after clustering */
forall the i = 1 to N do
ci ‚Üê √ó size of Pi
forall the i = 1 to N do
forall the v ‚àà Li do
if T (v) = unassigned then
j ‚Üê index of sub-domain adjacent to v that appears most
k ‚Üê index of sub-domain adjacent to v whose c is the smallest
if cj ck < 2 then
Insert v to Qj
// Default policy, increase clustering
c j = cj +
else
Insert v to Qk
c k = ck +

11
12
13

Map
Map
Map
Map

1:
2:
3:
4:

// Expected task loads are unbalanced

f = 1. Constant speed map.
f = 1 4, 1 2, 1. Speed map with three layers of di erent speed values.
f = 1 + 0 5sin(20 x) ‚àó sin(20 y) ‚àó sin(20 z). (x y z ‚àà = [0 1])
Circular maze map with permeable barriers. (f = 0.01 on barriers, otherwise 1)

result, Algorithm 2 shows the best scaling performance in all cases.
Lastly, we wrap up our discussion by brie y comparing our result with the state-of-the-art
method. To the best of our knowledge, the multi-GPU 3D parallel sweeping by Krishnasamy
et al. [6] is the only similar work to ours as of today. In this work, they reported up to 2.8√ó
speed up on four GPU, which is lower than the scaling performance of our method (up to 3.3√ó).
We believe this is mainly due to the fact that their sweeping approach requires data shu ing
between each sweep iteration (for example, after sweeping along x-axis, the entire data should
be shu ed in order to proceed sweeping along y-axis in the next iteration). In addition, our
load balancing and clustering methods can e ectively handle the adaptive nature of BlockFIM
algorithm and improve the performance on multiple GPUs.

4

Conclusion

In this paper, we present a novel on-the- y adaptive domain decomposition algorithm for parallel BlockFIM on multi-GPU systems. In order to handle the adaptive nature of FIM, the
proposed method progressively expands the computation domain as the active list of FIM
grows. We proposed a simple table-based mapping scheme and domain assignment algorithm
that takes into account spatial clustering and load balancing in order to minimize communication between GPUs while maximizing computing resource utilization. We showed that the
proposed method outperforms other regular and naive dynamic domain decomposition strate197

Multi-GPU FIM for Eikonal Equation

(a) Map 1

(b) Map 2

Sumin Hong and Won-Ki Jeong

(c) Map 3

(d) Map 4

Figure 5: Color-coded distance map with iso-contours of our test datasets visualized in 2D.
Blue to red color : from nearest to farthest distances to the seed point.

1-axis, multi-split

3-axis, single-split

3-axis, multi-split

Algorithm 1

Algorithm 2

1
2
4
8
2
4
8
2
4
8
2
4
8
2
4
8

GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU
GPU

Map 1
4.88
3.62 (1.35x)
2.18 (2.23x)
1.25 (3.90x)
3.62 (1.35x)
2.82 (1.73x)
2.04 (2.39x)
3.25 (1.50x)
2.13 (2.29x)
1.64 (2.97x)
2.99 (1.63x)
2.04 (2.39x)
1.16 (4.20x)
2.96 (1.64x)
1.70 (2.86x)
1.05 (4.64x)

Map 2
20.29
16.05 (1.26x)
11.04 (1.84x)
6.54 (3.10x)
16.05 (1.26x)
10.37 (1.96x)
7.54 (2.69x)
11.03 (1.84x)
6.71 (3.02x)
5.68 (3.57x)
11.89 (1.71x)
6.71 (3.02x)
3.72 (5.45x)
11.29 (1.80x)
6.69 (3.03x)
3.62 (5.60x)

Map 3
29.17
19.43 (1.50x)
13.02 (2.24x)
6.99 (4.17x)
19.43 (1.50x)
12.54 (2.33x)
9.57 (3.05x)
16.24 (1.80x)
10.88 (2.68x)
8.13 (3.59x)
17.95 (1.62x)
9.26 (3.15x)
4.92 (5.93x)
16.01 (1.82x)
8.72 (3.34x)
4.73 (6.17x)

Map 4
21.80
14.97 (1.46x)
9.66 (2.26x)
5.48 (3.98x)
14.97 (1.46x)
9.99 (2.18x)
6.65 (3.28x)
13.09 (1.67x)
7.07 (3.08x)
5.34 (4.08x)
12.81 (1.70x)
7.42 (2.94x)
4.42 (4.93x)
12.63 (1.73x)
7.11 (3.07x)
4.41 (4.94x)

Table 1: Running time using diÔ¨Äerent number of GPU device (1 to 8) measured in second. The
fastest time for each dataset is marked in boldface.
gies by a large margin, and observed that the proposed method allows BlockFIM to scale up
to 6.1 on eight GPUs. We also showed that our method scales better than state-of-the-art
multi-GPU Eikonal solvers.
In the future, we plan to extend our dynamic domain decomposition algorithm to distributed
systems using message passing interface (MPI) and streaming computing strategy to solve the
Eikonal equation on extremely large computational domains. Exploring real-world application
of the Eikonal equation on large computing problems, such as seismic image analysis and
simulations, will be another interesting future research direction.

198

Multi-GPU FIM for Eikonal Equation















































































(a) Map 1, Algorithm 1, Œ¥=0.3

(b) Map 1, Algorithm 1, Œ¥=0.1

(c) Map 1, Algorithm 2, Œ¥=0.1












































	







































	










	



	




Sumin Hong and Won-Ki Jeong



















	























(d) Map 3, Algorithm 1, Œ¥=0.3














	



(e) Map 3, Algorithm 1, Œ¥=0.1

(f) Map 3, Algorithm 2, Œ¥=0.1













Figure 6: Amount of task loads and data communication on Map 1 and 3 using four GPUs. The
left column ((a) and (d)) is Algorithm 1 performed with infrequent load balancing ( = 0.3).
The middle column ((b) and (e))is Algorithm 1 performed with more frequent load balancing
( = 0.1). The right column ((c) and (f)) is Algorithms 2. In each graph, upper four curves are
task loads and lower four curves are communication loads per GPU. Algorithm 2 performed
well in both of task loads and communication loads.



 !"!
  #"!
 !"!
$#!%






	










 !"!
  #"!
 !"!
$#!%






	





(a) Map 1

(b) Map 3








Figure 7: Weak scaling e ciency test on Map 1 and 3. The data size is set to around 2 GB per
GPU (6403 , double precision)

199

Multi-GPU FIM for Eikonal Equation

Sumin Hong and Won-Ki Jeong

Acknowledgments
This work was partly supported by Institute for Information & communications Technology
Promotion(IITP) grant funded by the Korea government(MSIP) (No.R0190-15-2012, High
Performance Big Data Analytics Platform Performance Acceleration Technologies Development), Technology Innovation Program (No.10054548, Development of Suspended Heterogeneous Nanostructure-based Hazardous Gas Microsensor System) funded by the Ministry of
Trade, industry & Energy(MI, Korea), and Basic Science Research Program through the
National Research Foundation of Korea(NRF) funded by the Ministry of Education (NRF2014R1A1A2058773).

References
[1] Tor Gillberg. A semi-ordered fast iterative method (SOFI) for monotone front propagation in
simulations of geological folding. The 19th International Congress on Modelling and Simulation
(MODSIM2011), December 2011.
[2] Tor Gillberg, Mohammed Sourouri, and Xing Cai. A new parallel 3d front propagation algorithm
for fast simulation of geological folds. In ICCS, volume 9 of Procedia Computer Science, pages
947‚Äì955. Elsevier, 2012.
[3] M Herrmann. A domain decomposition parallelization of the fast marching method. Technical
report, DTIC Document, 2003.
[4] Won-Ki Jeong and Ross T. Whitaker. A fast iterative method for eikonal equations. SIAM J. Sci.
Comput., 30(5):2512‚Äì2534, July 2008.
[5] Ron Kimmel and James A Sethian. Optimal algorithm for shape from shading and path planning.
Journal of Mathematical Imaging and Vision, 14(3):237‚Äì244, 2001.
[6] Ezhilmathi Krishnasamy, Mohammed Sourouri, and Xing Cai. Multi-gpu implementations of
parallel 3d sweeping algorithms with application to geological folding. Procedia Computer Science,
51:1494‚Äì1503, 2015.
[7] R. Malladi and J. Sethian. A uniÔ¨Åed approach to noise removal, image enhancement, and shape
recovery. IEEE Trans. on Image Processing, 5(11):1554‚Äì1568, 1996.
[8] E. Rouy and A. Tourin. A viscosity solutions approach to shape-from-shading. SIAM Journal on
Numerical Analysis, 29:867‚Äì884, 1992.
[9] James A. Sethian. A fast marching level set method for monotonically advancing fronts. In Proc.
Natl. Acad. Sci., volume 93, pages 1591‚Äì1595, February 1996.
[10] OÔ¨År Weber, Yohai S. Devir, Alexander M. Bronstein, Michael M. Bronstein, and Ron Kimmel.
Parallel algorithms for approximation of distance maps on parametric surfaces. ACM Trans.
Graph., 27(4):104:1‚Äì104:16, November 2008.
[11] H. Zhao. A fast sweeping method for eikonal equations. Mathematics of Computation, 74:603‚Äì627,
2004.
[12] H. Zhao. Parallel implementations of the fast sweeping method. Journal of Computational
Mathematics, 25(4):421‚Äì429, 2007.

200

