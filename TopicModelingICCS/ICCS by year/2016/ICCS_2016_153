Procedia Computer Science
Volume 80, 2016, Pages 439‚Äì449
ICCS 2016. The International Conference on Computational
Science

An Evaluation of Data Stream
Processing Systems for Data Driven Applications
Jonathan Samosir, Maria Indrawan-Santiago and Pari Delir Haghighi
Faculty of IT, Monash University, Australia.

Abstract
Real-time data stream processing technologies play an important role in enabling time-critical decision
making in many applications. This paper aims at evaluating the performance of platforms that are
capable of processing streaming data. Candidate technologies include Storm, Samza, and Spark
Streaming. To form the recommendation, a prototype pipeline is designed and implemented in each of
the platforms using data collected from sensors used in monitoring heavy-haul railway systems.
Through the testing and evaluation of each candidate platform, using both quantitative and qualitative
metrics, the paper describes the findings, where Storm is found to be the most appropriate candidate.
Keywords: Big data, real-time data stream processing, Storm, Spark, Samza, Hadoop ecosystems

1 Introduction
Data intensive science, as the fourth paradigm of science [1], involves the process of capturing,
curating, analysing and communicating data for scientific discovery. With the proliferation of sensor
technologies as a tool to capture physical measurements, there is a need to support real-time decisions
based on intermittent data availability, in addition to the ability to process large volumes of data.
Current distributed technologies, such as Hadoop, have been used to address these use-cases, however
they are often not appropriate for use-cases involving non-deterministically available data, or dynamic
real-time data. There are currently numerous real-time data processing platforms that are in
development and in production use, both in industry and academia. In this paper, we present our
experience in evaluating and ultimately selecting a data stream processing platform for processing
sensor data collected from the monitoring of heavy-haul railway systems.
To evaluate the candidate real-time data streaming platforms, and forming a recommendation for
the most suitable solution, appropriate data stream processing system (DSPS) technologies need to be
looked at, tested, and evaluated. We selected three data stream platforms for evaluation and
implemented each of the platforms to test their suitability for our project‚Äôs data processing needs. The
platform was designed as a data processing ‚Äúpipeline‚Äù that allows data to be streamed from the railway
and processed, in real-time, according to the given processing requirements.
From describing our experience, we provide an insight on the capabilities of the chosen platforms.
Each of the platforms is evaluated based on a set of quantitative and qualitative criteria. Both the

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.322

439

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

quantitative and qualitative performance report can be used to guide others when selecting DSPS
technologies for scientific projects that match the use-case.

2 Real-time Data Processing of Big Data
‚ÄúTraditional‚Äù methods of processing big data, involving MapReduce jobs to batch process data, are
not completely suitable for real-time use-cases involving processing data with non-deterministic
availability. Real-time data processing, enabled by DSPS technologies, allows data to be processed as
soon as it is made available, without the need of a storage system, such as HDFS. A 2013 industry
survey on European company use of big data technology shows that over 70% of responders show a
need for real-time processing [2]. Since that time, there has certainly been a response from the opensource software community, through the rapid development of modern DSPS technologies.
One very notable DSPS technology developed independently of Hadoop, and that is gaining
immense popularity and growth in its user base, is the Storm project [3]. Storm was originally
developed by a team of engineers lead by Nathan Marz at BackType. Toshniwal et al. [4] describe
Storm, in the context of its use at Twitter, as ‚Äúa real-time distributed stream data processing engine‚Äù
that ‚Äúpowers the real-time stream data management tasks that are crucial to provide Twitter services‚Äù
[3, pg. 147]. Since the project‚Äôs inception, Storm has seen mass adoption in industry, including
amongst some of the biggest names, such as Twitter, Yahoo!, Alibaba, and Baidu [5].
Spark [6] is another popular big data distributed processing framework, offering of both real-time
data processing and more traditional batch mode processing, running on top of Hadoop YARN [6].
Spark is notable for its novel approach to in-memory computation, through Spark‚Äôs main data
abstraction, the resilient distributed dataset (RDD). Spark Streaming [8], built on the Spark engine,
affords the processing of streamed data.
Samza is a relatively new real-time big data processing framework originally developed in-house
at LinkedIn, which has since been open-sourced at the Apache Software Foundation [9]. Samza offers
much similar functionality to that of Storm. While Samza is lacking in maturity and adoption rates, as
compared to projects such as Storm, it is built on mature components, such as YARN and Kafka,
which many core features are offloaded to. Samza processes streams of data through pre-defined jobs,
which perform any specified operation on the data streams. The source of streams in Samza comes
from Kafka, which can be used as an output destination for jobs for further pipeline-style processing.

3 Data Filtering Pipeline Design
The proposed data filtering pipeline is designed in such a way that readings from sensors can be
fed into the pipeline, processed in some specified manner, then output for further use, including
storage for batch processing, or discarded in the case of noisy data. Figure 1 shows an overview of the
pipeline.

Figure 1: An overview of the data filtering pipeline components

440

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

The input to the Input Component is designed to be an abstract interface, allowing data to be fed
into the pipeline from any type of sensor that is able to adhere to the interface.
The Processing Component is made up of an arbitrary number of tasks, each of which are tasked to
process the data in some manner before forwarding it onto whatever the next specified task is. This, in
itself, acts as a mini-pipeline where the core of processing will be done.
The output from the Output Component includes whatever extensions are needed to be made to the
pipeline. Hence, rather than acting as a sink, this component is another abstract interface which any
extension needed will be able to adhere to extend the pipeline. Designing the pipeline in this manner
allows the output data to be dealt with according to the requirements of the application users.
For implementation of the pipeline that was used in the evaluation parts of the project, we needed
concrete components for testing. For evaluation we had to simulate the sensor component of the
overall pipeline design. Note that this does not affect the pipeline in any way, since the actual sensor
hardware part of the design is abstracted away from the software pipeline itself. The Input Component
is implemented as a network socket connection, and the Processing Component as a filtering task.
Speed sensor readings are passed in along the socket connection using a tool allowing interface to
network ports, such as netcat1, and filtered to make sure they are valid speed values, between the limits
of 0 and 90. Any speeds outside of these limits can safely be deemed to be either bad, or noisy, sensor
readings. This resulted in a concrete design for implementation, as shown in Figure 2.
In Samza, a project is defined by the abstractions of systems and tasks. Systems exist to produce
streams of data, which tasks can consume and process in some specified way before producing new
streams to forward onto other tasks or systems. In our Samza implementation, a system has been
created to be the Input Component, listening on a socket for data and producing a stream. That stream
is then consumed by a task that acts as our Processing Component to flag invalid speed values,
producing two streams; a stream of valid speed readings and one for invalids. The Output Component
does not need to be implemented, as no after-the-fact processing needs to be performed on this data.
Like Samza, Storm is built around the key concept of streams. However, Storm approaches stream
computations as transformations of streams through use of Storm bolts. Spouts are the key Storm
abstraction for data stream sources, similar to Samza systems. Hence, for the Storm implementation of
our pipeline, a spout has been created to listen on a particular socket, being the Input Component,
emitting a stream of speed data which is then processed by the Processing Component, which is
implemented as a Storm bolt. Like in the Samza implementation, the Processing Component emits two
streams; one for speed values deemed valid, and the other for invalids. Again, the Output Component
does not need to be implemented in the Storm implementation.
Unlike Storm and Samza, which many comparisons can be drawn between, Spark Streaming does
not have concrete abstractions for stream sources and processing nodes on which it is built around.
Instead a DStream object exists, which represents a data stream, then specified functions are applied to
it, producing a slightly different DStream at each step.

Figure 2: A concrete implementation of our pipeline design.
1

http://netcat.sourceforge.net/

441

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

For our Input Component, a socket is opened which produces a new DStream object from the
incoming speed data. This DStream is then operated on by a defined function, which makes up our
Processing Component. This function simply performs the flagging of invalid speed values. Again, the
Output Component is not needed to be implemented, as no after-the-fact processing needs to be
performed on the DStream, however further processing can be specified by defining further functions
to operate on the DStream after the Processing Component‚Äôs function. Note that we built two Spark
Streaming implementations; one in Scala, on the Spark JVM engine, and one in Python, on the
PySpark engine.

4 Evaluation
Evaluation of each of the chosen DSPS technologies was performed using both qualitative and
quantitative testing methods of evaluation. The quantitative evaluation methods used focused on the
benchmarking of various features that are common to each of the technologies, using the pipelines
implemented. The qualitative evaluation methods will focus on mostly programmer-centric metrics.

4.1 Test Data
In this evaluation, data streams will have to be simulated from a batch dataset acquired from the
Monash Institute of Railway Technology (IRT) team. The dataset includes 99,999 rows of data
recorded from sensors placed on rail cars. To simulate streaming data from the data samples, netcat2 is
used to send data to the pipeline. Each data value represents a speed-reading formatted in kilometres
per hour as a floating point number. For the test data throughput time test, it requires multiple
differently sized large batches of test data which have been generated by extracting the speed readings
from the provided sample dataset and replicated them a number of times to generate the large batches.
The result is five datasets containing speed readings of sizes 100,000, 500,000, 1,000,000, 5,000,000,
and 10,000,000 data values, respectively. For the response time to streaming data test, it only requires
a single data value from a sensor reading; the arbitrary value of 42.002 was used.

4.2 Testing Environment
The testing environment on which the prototype pipelines were implemented using each of the
candidate DSPS technologies is the National eResearch Collaboration Tools and Resources
(NeCTAR) cloud computing environment, as supplied by the Australian Government.
Our environment consists of a single NeCTAR cloud instance with the following details: Ubuntu
GNU/Linux 14.04.2 LTS x86_64, Linux 3.13.0-36-generic kernel, 16 virtual CPUs, and 64 GB RAM.
The NeCTAR cloud instance utilises a single node upon which all processing is performed.
The DSPS technologies that were chosen to be focused on in this study include: Samza Version
0.9.0, Storm Version 0.9.4, and Spark Streaming Version 1.3.0. Prototype pipelines were implemented
in both Scala 2.9.2, targeting the JDK 1.8.0, and Python 2.7. The specific JDK implementation for all
tests was the Intel 64-bit OpenJDK 1.8.0 release 45 built and packaged for the GNU/Linux distribution
of Ubuntu 14.04.2.

4.3 Quantitative Test Metrics
The quantitative methods to be used to evaluate the candidate DSPS technologies include mostly
performance-based metrics: response time to streaming data, test data throughput time, peak system
resource usage, and system start-up time.
2

442

http://netcat.sourceforge.net/

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

Response time to streaming data refers to the amount of time it takes for each DSPS pipeline to
fully process a single value of sensor reading data. This time is defined from the time that value is sent
along the network socket connection to the time the given data value is pushed onto the pipeline‚Äôs
output stream. This can be measured using the runtime logs provided for each of the DSPS
technologies. This test aims to look at how fast a given pipeline can respond to a sensor‚Äôs reading.
Test data throughput time refers to the amount of time it takes for each DSPS pipeline to fully
process large batches of sensor readings. Five different tests will take place, each with data batches of
readings of size 100,000, 500,000, 1,000,000, 5,000,000, and 10,000,000, respectively. The
throughput time is defined as the time from when the batches are piped along the network socket
connection, to the time the pipelines have finished processing. These times are measured using the
internal DSPS technology runtime logs.
Peak system resource usage refers to the peak amount of resident system memory in-use while
each of the pipelines are running. Measurements will be taken using the UNIX tool top to collect
readings for the amount of resident memory being used by the DSPS technology.
System start-up time relates to the amount of time taken for each candidate DSPS pipeline to startup. This time is defined from the time each DSPS technology is started, via the system shell, to the
time the DSPS technology is ready to accept incoming data streams. This will be measured using the
UNIX time command, and allowing the DSPS technology to terminate upon readiness. This is
considered to be a less important test, as these systems are expected to be deployed and run for long,
continuous periods of time.

4.4 Quantitative Test Results
Figure 3 shows the results for response time. Both Spark Streaming APIs exhibit relatively good
performance compared to both Storm and Samza. In particular, Spark Streaming running on the Spark
JVM API exhibited over 3√ó the performance of the next best system, Storm, and nearly 5√ó the
performance of the slowest contender, Samza. Further of note is that at this size of data (a single
sensor reading) both Spark Streaming APIs exhibit very similar performance in relation to the other
DSPS technologies. As Spark Streaming is essentially an extension built on top of a batch processing
system, the fact that it outperformed both Samza and Storm, two technologies built from the ground
up with the aim of processing data in real-time, is quite a surprising find.
The results of the test data throughput time tests are shown in Figure 4. The first fact to note with
these results relates to the performance of Spark Streaming, running on the PySpark API, as the input
data increases. Compared to the other technologies, Spark Streaming (PySpark) exhibits very poor
scaling performance when dealing with much larger amounts of input data, particularly with 5,000,000
data points and greater. This drop in performance is so much of an issue that at the scale at which the
results are shown, it is hard to make out differences in performance for the other systems.

443

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

-(



,(

,+

+(
*1

*(
)(

)+

1

(




&'



& 
'


!




Figure 3: Results of response time to streaming data test.

),(((
)*(((
)((((



0(((
.(((
,(((
*(((
(

)(("(((



-(("(((



)"((("((( -"((("((( )("((("((


(







& 
'

).)$,,+

0+.$/0.

),+1$1-.

-.0,$,// )*./,$,)1


!

*)+$*./

+)*$*+,

-,+$00.

)((+$*+,

*+..$,+)




)0*$+1*

*+.$-+*

+)/$*+/

1+*$.-1

).-/$,-+



&'

)+0$0,+

)+*$+*,

*.+$//,

0.-$/()

)(0-$--*

Figure 4: Results of test data throughput time tests.

444

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

*-((
*(((



)-((
)(((
-((
(


!

)(("(((



-(("(((



)"((("((( -"((("((( )("((("((


(




*)+$*./

+)*$*+,

-,+$00.

)((+$*+,

*+..$,+)




)0*$+1*

*+.$-+*

+)/$*+/

1+*$.-1

).-/$,-+



&'

)+0$0,+

)+*$+*,

*.+$//,

0.-$/()

)(0-$--*

Figure 5: Results of our test data throughput time tests (PySpark omitted)

Hence, to better analyse the results of the other systems, a chart from the same data was created
with the Spark Streaming (PySpark) results omitted. This can be viewed in Figure 5.
The results of Spark Streaming (Spark JVM), Storm, and Samza mostly show a similar
performance at scale, relative to what was seen with Spark Streaming (PySpark). Spark Streaming
shows the greatest performance, with greater performance than both Storm and Samza in all tests,
while Storm generally is not too far off Spark Streaming‚Äôs performance, showing a similar growth for
all tests apart from the 10,000,000 data points test. Again, Samza shows the worst overall performance
out of the three candidates (not counting PySpark), with less than half the performance of Spark
Streaming on the 10,000,000 data points test.
The results for the peak system resource usage test can be seen in Figure 6. Interestingly, Spark
Streaming excels again here, with JVM Spark having over double the performance from the next best
contender, Storm. Samza, at peak usage, takes up over 5√ó the amount of resident memory that Spark
Streaming does, and nearly 3√ó the amount that Storm uses. Much of the resident memory usage may
be caused by a lot of the software dependencies that Samza and Storm rely on, which Spark Streaming
generally does not. Spark, the base project, not the real-time processing extension, was designed to be
an entire software ecosystem in itself, hence a lot of external dependencies that Storm and Samza may
have are not needed, possibly leading to this extreme reduction in resident memory usage shown here.
A further point of interest here is that Spark Streaming, and thus Spark, is renowned for its inmemory computation model, where data is placed solely in resident memory during computation. This
design decision would lead to the expectation of Spark computations using far more peak resident
system memory than other technologies that do not make such use of resident memory. However, as
seen here, Spark Streaming is shown to use the least amount of resident memory. This may be to do
with the amounts of data used. For example, a more data-intensive workload would likely provide
much higher resident system memory usage in Spark than that of Storm or Samza.

445

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

)"-((
)+-)



)"(((
-((

-,.
*,*

*/.

(
	 


&'



& 
'


!




Figure 6: Results of peak system resource usage test.

),"(((
)*"(((

)**+.



)("(((
0"(((

1.1)
/..0

."(((
,"(((

,*..

*"(((
(


%


&'



& 
'


!




Figure 7: Results of system start-up time test.

The results for the system start-up time test can be seen in Figure 7. It is interesting to note here
that Spark Streaming, the overall best performing DSPS technology in all other performance-based
tests, shows the worst performance in-terms of start-up time, particularly the PySpark version of Spark
Streaming. Samza, the overall worst performing technology, here shows the best performance.

4.5 Qualitative Tests
The qualitative methods used to evaluate the candidate DSPS systems focus on the ease of
extensibility, available DSPS features, ease of programmability, and ease of deployment.
Ease of extensibility - Parameters we looked at for ease of extensibility include whether or not an
entire project needs to be recompiled and deployed to be extended, and the ease of modifying overall
system configurations. Spark Streaming is noted as the worst performing technology in terms of
extensibility, overall. This is mostly due to the fact that, by design, a Spark Streaming program is
generally very tightly coupled in terms of processing modules. All processing logic revolves around
the StreamingContext object in a Spark Streaming program, hence everything is tightly integrated to
the placement of this object. Of course, this object can be passed around different modules, however it
is a rather na√Øve solution considering the other projects exhibit loose coupling by design. This tight

446

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

coupling of logic in Spark Streaming leads to any extensions written to an existing Spark Streaming
project to be written as modifications of the original source code. Likewise, in Spark Streaming,
configuration is tightly coupled with the programming logic, through use of the SparkConf object, on
which the StreamingContext object is instantiated. This leads to any future changed in configuration to
be written to the original source code, thus leading to project recompilation.
Samza is noted to be far more loosely coupled than Spark Streaming, with any extensions of an
existing project being able to be written separately of the original code. Essentially, a Samza project‚Äôs
processing tasks will output their results to specific Samza streams. On these streams, a new extension
can be written to listen on, without the existing processing tasks needing to be aware of this new
extension task. This loose coupling of processing tasks is by design in Samza, with tasks existing in a
dataflow-like sequence, where tasks can be added and removed with ease. For configuration in Samza,
processing tasks and systems are configured separately from the underlying logic, using a noncompilation necessary medium. Effectively, this means that Samza projects can be reconfigured onthe-fly without recompilation of the entire project. This is an important factor that Spark Streaming
lacks native support for.
Storm is very similar to Samza in the way that extensions can be written to extend an existing
Storm topology without modification of the existing source code. However, extensions written to an
existing Storm topology do not require knowledge of output streams from existing Storm processing
tasks (bolts), which is a necessity when extending a Samza project. Configuration in Storm is also kept
separate of the logic written in the source code, hence also does not require recompilation to
reconfigure. However, to reconfigure an existing Storm topology layout, this is generally written in
source code using the TopologyBuilder class, hence does require a recompilation step.
To summarise, Spark Streaming has shown to lack in extensibility, compared to the other
candidate DSPS technologies, mainly due to being tightly coupled by design, while Storm and Samza
exhibit similar ease. Samza and Storm extensions can be written separate of existing logic, without the
existing logic being aware of the extensions. This is not the case with Spark Streaming.
Available DSPS features - Stonebraker, √áentintemel, and Zdonik proposed what they claim to be
the eight requirements for real-time data processing systems [9]. These requirements are used here for
comparison of real-time data processing technologies. Table 2 shows the results. Both Spark
Streaming and Samza lack many of the real-time processing features that are present in Storm. This
may be put down to the fact that both Spark Streaming, as an extension of the greater project Spark,
and Samza are still relatively immature projects. Furthermore, of note is that many features that are
supported in Samza actually depend on the integration of Kafka with Samza.
Ease of programmability - Spark Streaming is regarded as the most accessible of the candidate
DSPS technologies in terms of programmability. Spark Streaming, as of version 1.2.0, has official
support for Java, Scala, and Python. Storm has native support for Java and Clojure, and essentially any
programming language through use of a Thrift definition file3, however official documentation is only
provided for Java and Clojure. Samza only has native support for Java. Writing Spark Streaming
projects, and also Storm and Samza projects, in JVM languages require compilation to JVM bytecode
before they are to be run, and also having the requirement that the compiled bytecode be packaged as a
distributable JAR archive. Python use with PySpark allows for Spark Streaming projects to be written
and run directly from the Python source files deployed to a Spark installation.

3

https://thrift.apache.org/

447

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

Requirements
1: Data Mobility
2: SQL on streams
+$

,#
-#

.#
/#
0#

Storm
Fetch
Extensions (Trident)
User responsibility

Spark Streaming
Micro-batch
Yes (Spark SQL)
N/A

Samza
Fetch (Kafka)
No
Yes (Kafka)

N/A
Yes (Trident)
Yes (rollback
recovery)
User responsibility
Yes

N/A
Yes(Spark)
Yes (Checkpoint
recovery)
No
Yes

N/A
No
Yes (rollback
recovery)
Yes(Kafka)
Yes

Table 1: Available features in each candidate DSPS technology

With Storm and Samza, there are a number of interfaces that are required to be implemented to
make even the most simple of projects. Spark Streaming does away with these interfaces, and instead
all processing logic surrounds the StreamingContext class, meaning that as long as this class is
instantiated, all the possible processing functions are available to be used. Thus, a simple processing
pipeline based on simple processing logic implemented in each system would require a number of
different files, each implementing required interfaces, in Storm and Samza, while the same logic in
Spark Streaming could be done in a number of lines in any supported programming language. Samza,
in particular, has been noted as having the steepest of the learning curves, in terms of
programmability, due to a required knowledge of many of the underlying software dependencies, such
as Kafka, ZooKeeper, and various serialisers and deserialisers for Samza messages.
In terms of required configuration, Spark Streaming allows for the most ease in configuring, due to
a low-level understanding of Spark configuration not being needed for most tasks. Storm also has
somewhat of an easy required configuration knowledge, mostly due to having the more complicated
configuration options being abstracted away from the programmer unless necessary, and the majority
of configuration being defined in the topology file. Samza, however, requires separate, and
particularly verbose, configuration files for every task and system within a Samza project, as well as
many configuration options not having defaults. Again, this leads to a good understanding of Samza
being needed to configure even a simple project, particularly if a Samza system expands to be greater
than a small number of different processing tasks.
Spark Streaming has been noted as the most simple of the candidate DSPS technologies in terms of
programmability and overall learning curve. Storm presents the next most simple technology with a lot
of the configuration and software dependencies being abstracted away from the programmer. Samza
comes in last, with the steepest learning curve, due to a good understanding of the overall Samza
project, along with Samza software dependencies needed to implement even a simple Samza project.
Ease of deployment - Projects written for each of the DSPS technologies are expected to be
deployed and distributed in similar ways. As all of the technologies are built on JVM technologies,
they require compiled JVM bytecode to be packaged as a JAR file, generally using a build automation
tool, such as Maven4 or Gradle5. However, for deployment and distribution of Spark Streaming
projects written in Python, these compilation and build assembly steps are not necessary, with Python
and Scala Spark Streaming projects affording deployment simply as the Python or Scala source files
themselves. This greatly eases overall deployment, and distribution of Spark Streaming projects
between different Spark Streaming installations residing on different clusters. Furthermore, Storm and
Samza require the set-up of external technology dependencies, such as Kafka or ZooKeeper, on the
4
5

448

https://maven.apache.org/
https://gradle.org/

Evaluation of Data Stream Processing Systems J. Samosir, M. Indrawan-Santiago, P. Delir Haghighi

cluster on which they will be run, if the implemented project requires such dependencies. Spark
Streaming can be deployed without external technology dependencies as the Spark project provides
Spark distributions with all such dependencies built-in and pre-configured [11].

5 Conclusion
In this paper, a comparison of streaming platforms was presented. A data processing pipeline for
the monitoring of heavy-haul railway systems was used for testing, and consequently evaluation, of
the different candidate DSPS technologies, allowing us to arrive at the recommendation of the Storm
DSPS. Storm shows great overall performance in processing data in real-time, and also allows for
extensible projects to be created, suitable for large projects, intended to be in production for long
periods of time, and subject to change. Furthermore, Storm affords a great number of possible realtime and batch processing features, while exhibiting a relatively low footprint on system resources.

References
1.

Hey, A., Tansley, S. and Tolle, K. (2009) The Fourth Paradigm: Data-Intensive Scientific
Discovery. Microsoft Research.
2. Bange, D., Grosser, T. and Janoschek, N. (2013). ‚ÄúBIG DATA SURVEY EUROPE - Usage,
technology and budgets in European best-practice companies‚Äù. [online] Wuerzburg, Germany:
BARC Institute. Available at: http://www.pmone.com/fileadmin/user_upload/doc/study/
BARC_BIG_DATA_SURVEY_EN_final.pdf [Accessed 27 Jan. 2016]
3. Apache Storm.: https://storm.apache.org/
4. Toshniwal, A., Taneja, S., Shukla, A., Ramasamy, K., Patel, J. M., Kulkarni, S. & Bhagat, N.
(2014). ‚ÄúStorm@ twitter‚Äù. In Proceedings of the 2014 ACM SIGMOD international conference
on Management of data, pp. 147-156. ACM.
5. Companies Using Apache Storm: https://storm.apache.org/documentation/Powered-By.html
6. Zaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., & Stoica, I. (2010). ‚ÄúSpark: cluster
computing with working sets‚Äù. In Proceedings of the 2nd USENIX conference on Hot topics in
cloud computing, vol. 10, pp. 10.
7. YARN: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html
8. Zaharia, M., Das, T., Li, H., Shenker, S., & Stoica, I. (2012). Discretized streams: an efficient and
fault-tolerant model for stream processing on large clusters. In Proceedings of the 4th USENIX
conference on Hot Topics in Cloud Computing, pp. 10-10. USENIX Association.
9. Samza: https://samza.apache.org/
10. Stonebraker, M., √áetintemel, U., & Zdonik, S. (2005). The 8 requirements of real-time stream
processing. ACM SIGMOD Record, 34(4), pp. 42-47.
11. Apache Spark: https://spark.apache.org/downloads.html

449

