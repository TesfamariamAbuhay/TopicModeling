Procedia Computer Science
Volume 80, 2016, Pages 2181â€“2189
ICCS 2016. The International Conference on Computational
Science

Algorithmic Approach for Learning a Comprehensive
View of Online Users
Kourosh Modarresi
Adobe Inc., San Jose, U.S.
kouroshm@alumni.stanford.edu

Abstract
Online users may use many different channels, devices and venues for any online user experience. To
make all services such as web design, ads, web content, shopping, personalized for every user; we need
to be able to recognize them regardless of device, channels and venues they are using. This, in turn,
requires building up a comprehensive view of the user which includes all of their behavioral
characteristics - that are spread all over these different venues. This would not be possible without
having all behavioral related data of the user which requires the capacity of connecting the user all over
the devices, and channels, so to have all of their behavior under a single view. This work is a major
attempt in doing this using only behavioral data of users while protecting the userâ€™s privacy.
Keywords: Matrix Completion, Regularization, Matrix Reconstruction, Singular Value Decomposition, User
Behavior Data

1 Introduction
Creating an ideal digital usersâ€™ experience has to be based on understanding the users and so creating
an echo environment that is favorable to them. This way, the users would have a desirable online
experience. One could expect a great digital experience may lead to a maximum return on the emarketing different metrics such as increase in usage, purchase and so on. Often, the basis of the study
of the usersâ€™ preferences is on tracking cookies, login parameters - such as email address, userid and so
on. Tracking users through cookies is not very reliable method in gaining a comprehensive view of the
users. One major issue about tracking by cookies is that users could delete them besides the fact that
cookies cannot recognize the same users as they may use different devices or digital venues and
channels. Deploying login credentials has only limited effects since users may not user any credentials
in their online journeys. Also, tracking IP address has the major limitation of being restricted to only
Selection and peer-review under responsibility of the Scientiï¬c Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.378

2181

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

one device as the IP address changes from one device to another. Those are some of the reasons
supporting the use of behavior data for user identification for this work. Users behavior will be recorded
regardless of whether they use any login credentials, or what device they are using and is also
independent of whether any cookies are used to track them. One of the major challenges in user
recognition based on behavioral data is that user behavior is distributed among all devices (such as
phone, desktop and so on), digital venues (email, browser, â€¦) and channels (search engine, commercial
websites, .). Users may not be fully understood if we look at only a fraction of their interactional data
with the digital space and thus to get a comprehensive view of a user, we need to collect all behavioral
data the users have left on all these dispersed locations. To do this, i.e., to gather all these characteristics
of the users - from all these channels, devices and venues â€“ we need to find a way to recognize the user
cross all these devices, channels and venues. This would suggest that this process is an iterative problem
and as we could recognize the same users â€“ no matter where they are engaging the digital sphere â€“
enabling us to bring together more behavioral data from all points, we get a better understanding of the
user through all this collected data and thus we could identify the same user across all the channels more
effectively.

2 The Model in Recognizing the Users
This work uses only behavioral data and assumes no access to private information of users, such as
name, last name, email address, SSN, DLN, login information. Also, no information from cookies is
used. The model is based on the following steps;
(A) Centering the Data Matrix
(B) Transformation of the Data Matrix X to a Higher Dimensional Space
(C)
(D)
(E)
(F)
(G)
(H)

Computing SVD (Singular Value Decomposition)
Completing the Matrix
Computing the Best Rank-k Matrix for the Matrix X
Updating the Original Matrix
Projecting the New Row onto the k-dimensional Space (SVD Updating)
Computing the New Distance and Detecting a Possible Match

2.1 The Format and Representation of Userâ€™s Behavioral Data
The user data entails all possible information of the interactions users have in the digital space.
Matrix representation of data is used in this work and so, the data is represented in the matrix form,
called matrix X. Rows of the matrix are users (i) and its columns (j) are different features that
represent usersâ€™ online interactions. The columns could include interaction variables such as â€œbrowser
typeâ€, â€œoperating system typeâ€, â€œurlâ€, â€œthe time of interactionâ€ and â€œlength of interactionâ€. Each
matrix entry - â€«İ”â€¬à¯œà¯ â€“ displays the specific feature of user i on interaction behavior j. Thus each entry
2182

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

(â€«İ”â€¬à¯œà¯ ) is the relation between a specific interactive variable j and a specific user i. The data matrix, X
has m rows and n columns (m Ã— n).
In general, the entries of the matrix X may be explicit such as the browser, rating and total purchases,
or they may be implicit data such as like/not like of specific content derived from some explicit data.
Explicit data is not always available and often not enough of that could be found [11,12,15]. Historic
(logged) data or live data (streaming) data and quite often a combination of both are used to design
and test the user recognition model.

2.2 Centering the Data Matrix
The data matrix contains a variety of data types that have different range and scales. The matrix also
includes non-numerical data that may be transformed to numeric data using dummy variables. All types
of data need to be centered to prevent domination of some variables (columns) as a results of their larger
range. The process of centering [ 85] is done by reducing the mean of each column from all the column
entries,
à¬µ

Üº = Üº( â€«Ü«â€¬à¯¡ àµ† 11à¯§ )
à¯¡

Where I am the identity matrix and 1 is a column vector with all entries to be one.

2.3 Transformation of the Data Matrix X to a Higher Dimensional Space
The data matrix X has n variables (columns). In general, for a unique definition and identification of
each user, this number of dimensions may not be sufficient. Thus, to make the users separable and
distinct from each other, the data matrix is transformed to a higher dimensional space. The number of
dimensions of the new space will be computed so that every user is defined and identified uniquely.
Modern data possesses many characteristics such as high correlation. To make the transformation of the
data matrix X, we use new coordinates of n+1 to n+p, where p is the required number of added features
so we could identify all users uniquely.
à¯¡à¬¾à¯£

â€«İ”â€¬à¯ª =

à· Ü½à¯Ÿ â€«İ”â€¬à¯Ÿ + â€«×â€¬à¯Ÿ
à¯Ÿà­€à¯¡

Where â€«×â€¬à¯Ÿ is random white noise and w = n+1:n+p .

2183

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

2.4 Computing SVD
For this work the singular value decomposition (SVD) of the matrix X is defined the usual way [33],
as;

Üº = Ü·â€« Ü¸Ü¦â€¬à¯§
Where: U, the left singular vectors, is mÃ—n orthogonal matrix,
Ü·Ü· à¯§ = Ü· à¯§ Ü· = â€«Ü«â€¬
V, the right singular vectors, is nÃ—n orthogonal matrix
Ü¸Ü¸ à¯§ = Ü¸ à¯§ Ü¸ = â€«Ü«â€¬
and D = diag (İ€à¬µ , İ€à¬¶ , â€¦ , İ€à¯¡ ) with the singular vectors;
İ€à¬µ àµ’ İ€à¬¶ àµ’ â€« Ú®â€¬. àµ’ İ€à¯¡ àµ’ 0
Another view of the computation of SVD is by using minimum reconstruction error,
min à¸®Üº àµ† Ü·à¯¤ â€«Ü¦â€¬à¯¤ Ü¸à¯¤ à¸®
Which could be rewritten as,
argminÔ¡Üº àµ† Üºâ€«İ‘İ’â€¬à¯§ Ô¡à¬¶à¬¶
(à¯¨,à¯©,à¯—)

Equivalently,
min à·[â€«İ”â€¬à¯œà¯ àµ† â€«İ‘â€¬à¯œ İ€à¯œ â€«İ’â€¬à¯ à¯ ]à¬¶

à¯, à®½,à¯

2.5 Completing the Matrix
Modern data is often very sparse [ 58] and the data under study in this work is even more sparse.
That is due to the fact that for every user entry (row) of the data matrix X, even when coming from the
same channel or device, there are many missing (unknown) entries. This is due to the fact the number
of variables is large and it is unlikely that userâ€™s interaction involves all different variables. Since the
columns of data matrix X is a combination of all variables from all possible devices and entries and
although these channels and devices have many similar and overlapping features (columns), still each
of these venues have many unique variables that are not shared by many other venues. As a result of
these two factors, i.e., the missing values for each channel and the uniqueness of many variables of these
channels, each row contains many unknown (missing) entries.
At this step, the unknown entries of the data matrix are computed using an iterative SVD algorithm
[ 75,76]. In short, at the first step, the missing entries in each column of X are replaced with the median
of the column. Then, SVD is computed for the matrix X. The third step involves the computation of the
best k-rank matrix (k<< min (m, n)). The forth step requires the reconstruction of X using the low rank
(k-rank) singular vectors and singular values matrix. Then, the newly computed values of the missing
entries are compared with the values from the previous iteration. All of the above steps are repeated
until convergence. The summary of the algorithm in this part is;
2184

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

For a centered X:
Step (1) compute the
İ‰İ…İŠ
à¸– à¸®Üº àµ† Ü·à¯¤ â€«Ü¦â€¬à¯¤ Ü¸à¯¤ à¸®
à¯à³œ ,à¯à³œ ,à®½à³œ

Where q is computed using a threshold of variation of the original data in X.
Step (2) computes the new X â€“ of q-rank
Üºà¯¤ = Ü·à¯¤ â€«Ü¦â€¬à¯¤ Ü¸à¯¤
Using newly computed Üºâ€«İâ€¬, we have new values for the missing entries.
Step (3) Iterate steps (1) and (2) till convergence,
â€«İ…( İÜºÛ…â€¬Ã­Üºâ€«ßœ Â”Û…)İ…( İÜºÛ…Û…İ…( İâ€¬
For small ßœ.

2.6 Computing the Best Rank-k Matrix for the Matrix X
At this step, using the results in the previous sections, we need to compute the best low dimensional
space that we should project matrix X onto. Matching of any new user will take place in this new space.
This new space has k- dimensions (section 2.5). The computation of k is based on the preservation of a
minimum threshold of the original information (variation) of the data. The determination of a specific
threshold depends on many considerations including the specific applications, the required accuracy and
the computational cost.

2.7 Updating the Original Matrix
For any incoming user for whom the identity is unknown, update the original matrix by adding the
new row and do all steps of 2.2-6.

2.8 Projecting the New Row onto the k-dimensional Space (SVD
Updating)
Project the new row in SVD space to get the new coordinates of the new data point.
For any new entryâ€«İ”â€¬à¯ à¬¾à¬µ :
(1) Compute the missing entry (section 2.5) to get,â€«İ”â€¬à¯¡à¯˜à¯ª .
(2) Compute its projection in the new (low dimensional) space;
2185

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

â€«İ‘â€¬à¯¡à¯˜à¯ª = â€«İ”â€¬à¯¡à¯˜à¯ª * V

2.9 Computing the New Distance and Detecting a Possible Match
For the new row in U, â€«İ‘â€¬à¯¡à¯˜à¯ª , compute the distance between the new row (vector) with all of the
previous rows in the matrix U (left singular vector of X). In other words, compute the closest user in the
space to the new one, in the sense of Euclidean (2nd) norm.
â€«İŠİ…Ü¯â€¬
à¸– â€«İ‘Û…â€¬à¯¡à¯˜à¯ª Ã­â€«İ‘â€¬à¯œ â€«İ‘Û…Û…â€¬à¯œ â€«Â”Û…â€¬Ä°
à¯œ

For a small Ä° If the equation condition is not satisfied i.e., if there is no user satisfying the inequality
then there is no match with the known users and the â€«İ”â€¬à¯¡à¯˜à¯ª is flagged as a new row (new user) in the data
matrix X.

2.10 SAMPLE OF RESULTS
Different data set is used to test this model. For all different examples of data matrices, k-fold cross
validation is used to validate the model. The model was applied on X=200,000 by 234 data matrix.
The obtained accuracy was 92%.

References
[1]

[2]
[3]
[4]
[5]
[6]
[7]

[8]
[9]
[10]
[11]

G. Adomavicius and A. Tuzhilin, â€œTowards the next generation of recommender systems: a survey
of the state-of-the-art and possible extensions,â€ IEEE Trans. on Data and Knowledge Engineering
17:6, pp. 734â€“749, 2005.
L. Backstrom, J. Leskovec, â€œSupervised Random Walks: Predicting and Recommending Links in
J. Baumeister, â€œStable Solution of Inverse Problemsâ€, Vieweg, Braunschweig, Germany, 1987.
S. Becker, J. Bobin, and E. J. CandÃ¨s. NESTA,â€ a fast and accurate first-order method for sparse
recovery,â€ SIAM J. on Imaging Sciences 4(1), 1-39, 2009.
A. Bjorck, â€œNumerical Methods for Least Squares Problemsâ€ ,SIAM, Philadelphia,1996.
S. Boyd and L. Vandenberghe, â€œConvex Optimizationâ€, Cambridge University Press, 2004.
J.S. Breese, D. Heckerman, and C. Kadie, â€œEmperical analysis of predictive algorithms for
collaborative filtering,â€ Proceedings of Fourteenth Conference on Uncertainty in Artificial
Intelligence. Morgan Kaufmann, 1998.
P. A. Businger, G. H. Golub, â€œSingular value decomposition of a complex Matrixâ€, Algorithm 358,
Comm. Acm, No. 12, pp. 564-565, 1969.
J. Cadima and I. T. Jolliffe, â€œ Loadings and correlations in the interpretation of principal
componentsâ€, Journal of Applied Statistics, 22:203â€“214, 1995.
J-F Cai, E. J. CandÃ¨s and Z. Shen, â€œA singular value thresholding algorithm for matrix completion,â€
SIAM J. on Optimization 20(4), 1956-1982, 2008.
E. J. CandÃ¨s and Y. Plan, â€œMatrix completion with noise,â€ Proceedings of the IEEE 98(6), 925936, 2009.

2186

Algorithmic Approach for Learning a Comprehensive View of Online Users
[12]

[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]

[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]

Kourosh Modarresi

E. J. CandÃ¨s and Y. Plan, â€œTight oracle bounds for low-rank matrix recovery from a minimal
number of random measurements,â€ IEEE Transactions on Information Theory 57(4), 2342-2359,
2009.
E. J. Cand`es and T. Tao, â€œDecoding by linear programmingâ€, IEEE Transactions on Information
Theory, 51(12):4203â€“4215, 2005.
E. J. CandÃ¨s and B. Recht, â€œExact matrix completion via convex optimization,â€ Found. of Comput.
Math., 9 717-772, 2008.
E. J. CandÃ¨s, â€œCompressive sampling,â€ Proceedings of the International Congress of
Mathematicians, Madrid, Spain, 2006.
E. J. CandÃ¨s and T. Tao, â€œNear-optimal signal recovery from random projections: universal
encoding strategies,â€ IEEE Trans. Inform. Theory, 52 5406-5425, 2004.
Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., Netes, D., and Sartin M., "Combining
content-based and collaborative filters in an online newspaper," Proceedings of the ACM SIGIRâ€™99
Workshop on Recommender Systems, 1999.
R. Courant and D. Hilbert, â€œMethods of Mathematical Physicsâ€, Vol. II, Interscience, New York,
1953.
A. dâ€™Aspremont, L. El Ghaoui, M.I. Jordan, and G. R. G. Lanckriet, â€œA direct formulation for sparse
PCA using semidefinite programmingâ€, SIAM Review, 49(3):434â€“448, 2007.
A. R. Davies and M. F. Hassan, â€œOptimality in the regularization of ill-posed inverse problemsâ€, in
P. C. Sabatier (Ed.), Inverse Problems: An interdisciplinary study, Academic Press, London, UK,
1987.
B. DeMoor, G. H. Golub, â€œThe restricted singular value decomposition: properties and
applicationsâ€, SIAM J. Matrix Anal. Appl., 12, No. 3, pp. 401-425, 1991.
D. L. Donoho and J. Tanner, â€œ Sparse nonnegative solutions of underdetermined linear equations
by linear programmingâ€, Proc. of the National Academy of Sciences, 102(27):9446â€“9451, 2005.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R., â€œLeast Angle Regression,â€ The Annals of
Statistics, 32, 407â€“499, 2004.
Lars Elden, â€œAlgorithms for the Regularization of Ill-Conditioned Least Squares Problemsâ€, BIT
17, pp. 134-145, 1977.
Lars Elden, â€œA Note on the Computation of the Generalized Cross-Validation Function for IllConditioned Least Squares Problemsâ€, BIT 24, pp. 467-472, 1984.
Heinz. W. Engl, M. Hanke, and A. Neubauer, â€œRegularization methods for the stable solution of
inverse problemsâ€ , Surv. Math. Ind., No. 3, pp. 71-143, 1993.
H. W. Engl, M. Hanke, and A. Neubauer, â€œRegularization of Inverse Problemsâ€, Kluwer,
Dordrecht, 1996.
H. W. Engl, K. Kunisch, and A. Neubauer, â€œConvergence rates for Tikhonov regulari- sation of
non-linear ill-posed problemsâ€ , Inverse Problems, (5), pp. 523-540, 1998.
H. W. Engl , C. W. Groetsch (Eds), â€œInverse and Ill-Posed Problemsâ€, Academic Press, London,
1987.
M. Fazel, H. Hindi, and S. Boyd. â€œA rank minimization heuristic with application to minimum order
system approximationâ€, Proceedings American Control Conference, 6:4734â€“4739, 2001.
W. Gander, â€œOn the linear least squares problem with a quadratic Constraintâ€, Technical report
STAN-CS-78-697, Stanford University, 1978.
G. H. Golub, C. F. Van Loan, â€œMatrix Computationsâ€, 4th Ed., Computer Assisted Mechanics and
Engineering Sciences, Johns Hopkins University Press, US, 2013.
Gene H. Golub, Charles F. Van Loan, â€œAn Analysis of the Total Least Squares Problemâ€, Siam J.
Numer. Anal., No. 17, pp. 883-893, 1980.
Gene H. Golub, W. Kahan, â€œCalculating the Singular Values and Pseudo-Inverse of a Matrixâ€,
SIAM J. Numer. Anal. Ser. B 2, pp. 205-224, 1965.
Gene H. Golub, Michael Heath, Grace Wahba, â€œGeneralized Cross-Validation as a Method for
Choosing a Good Ridge Parameterâ€, Technometrics 21, pp. 215-223, 1979.
Hastie, T., Tibshirani, R., and Friedman, J. ,â€ The Elements of Statistical Learning; Data mining,
Inference and Predictionâ€, New York: Springer Verlag, 2001.
Hastie, T.J and Tibshirani, R. "Handwritten Digit Recognition via Deformable Prototypes", AT&T
Bell Laboratories Technical Report, 1994.

2187

Algorithmic Approach for Learning a Comprehensive View of Online Users
[38]

[39]

[40]
[41]
[42]
[43]

[44]
[45]
[46]
[47]
[48]
[49]
[50]
[51]
[52]
[53]
[54]
[55]
[56]

[57]
[58]
[59]

[60]
[61]
[62]
[63]
[64]

Kourosh Modarresi

Hastie, T., Tibshirani, R., Eisen, M., Brown, P., Ross, D., Scherf, U., Weinstein, J., Alizadeh, A.,
Staudt, L., and Botstein, D., â€œ â€˜Gene Shavingâ€™ as a Method for Identifying Distinct Sets of Genes
With Similar Expression Patterns,â€ Genome Biology, 1, 1â€“21, 2000.
David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, and Carl
Kadie, â€œDependency networks for inference, collaborative filtering, and data visualization,â€ Journal
of Machine Learning Research, 1:49â€“75, 2000.
T. Hein, â€œSome analysis of Tikhonov regularization for the inverse problem of option pricing in the
price-dependent case,â€ ,SIAM Review, (21)No. 1, pp. 100-111, 1979.
T. Hein and B. Hofmann, â€œOn the nature of ill-posedness of an inverse problem in option pricing,â€
,Inverse Problems,(19), pp. 1319-11338, 2003.
B. Hofmann, â€œRegularization for Applied Inverse and Ill-Posed problems ,â€ Teubner, Stuttgart,
Germany, 1986.
B. Hofmann, â€œRegularization of nonlinear problems and the degree of illposedness,â€ in G. Anger,
R. Goreno, H. Jochmann, H. Moritz, and W.
Webers (Eds.), inverse Problems: principles and Applications in Geophysics,Technology, and
Medicine, Akademic Verlag, Berlin, 1993.
T. A. Hua and R. F. Gunst, â€œGeneralized ridge regression: A note on negative ridge parameters,â€
Comm. Statist. Theory Methods, 12, pp. 37-45, 1983.
V. K. Ivankov, â€œOn linear problems which are not well-posed ,â€ Dokl. Akad. Nauk SSSR, 145, pp.
270-272, 1962.
Jeffers, J., â€œTwo Case Studies in the Application of Principal Component,â€ Applied Statistics, 16,
225â€“236, 1967.
Jolliffe, I. , Principal Component Analysis, New York: Springer Verlag, 1986.
I. T. Jolliffe, â€œRotation of principal components: choice of normalization Constraints,â€ Journal of
Applied Statistics, 22:29â€“35, 1995.
I. T. Jolliffe, N.T. Trendafilov, and M. Uddin, â€œA modified principal component technique based
on the LASSO,â€ Journal of Computational and Graphical Statistics, 12:531â€“547, 2003.
Misha E. Kilmer and Dianne P. OLeary, â€œChoosing regularization parameters in iterative methods
for ill-posed problems,â€ SIAM J. MATRIX ANAL. APPL., Vol. 22, No. 4, pp. 1204-1221. 2001.
Andreas kirsch, â€œAn Introduction to the Mathematical theory of Inverse problems ,â€ Springer
Verlag, New York, 1996.
Mardia, K., Kent, J., and Bibby, J., â€œMultivariate Analysis,â€ New York: Academic Press, 1979.
G. Linden, B. Smith, and J. York, â€œAmazon.com recommendations: item-to-item collaborative
filtering,â€ Internet Computing 7:1, pp. 76â€“80, 2003.
Rahul Mazumder, Trevor Hastie and Rob Tibshirani, â€œSpectral Regularization Algorithms for
Learning Large Incomplete Matrices,â€ JMLR 2010 11 2287-2322, 2010.
McCabe, G., â€œPrincipal Variables,â€ Technometrics, 26, 137â€“144, 1984.
Kourosh Modarresi and Gene H Golub, â€œAn Adaptive Solution of Linear Inverse Problemsâ€,
Proceedings of Inverse Problems Design and Optimization Symposium (IPDO2007), April 16-18,
Miami Beach, Florida, pp. 333-340, 2007.
Kourosh Modarresi, â€œA Local Regularization Method Using Multiple Regularization Levelsâ€,
Stanford, CA, April 2007.
Kourosh Modarresi, â€œComputation of Recommender System Using Localized Regularizationâ€,
Procedia Computer Science, Volume 51, 2015, Pages 2407â€“2416.
Kourosh Modarresi and Gene H Golub, â€œAn Efficient Algorithm for the Determination of Multiple
Regularization Parameters,â€ Proceedings of Inverse Problems Design and Optimization
Symposium (IPDO), April 16-18, 2007, Miami Beach, Florida, pp. 395-402, 2007.
D. W. Marquardt, â€œGeneralized inverses, ridge regression, biased linear estimation,â€ and nonlinear
estimation, Technometrics, 12, pp. 591-612, 1970.
K. Miller, â€œLeast Squares Methods for Ill-Posed Problems with a prescribed bond,â€ SIAM J. Math.
Anal., No. 1, pp. 52-74, 1970.
B. Moghaddam, Y. Weiss, and S. Avidan, â€œSpectral bounds for sparse PCA: exact and greedy
algorithms,â€ Advances in Neural Information Processing Systems, 18, 2006.
V. A. Morozov, â€œOn the solution of functional equations by the method of regularizationâ€,Sov.
Math. Dokl., 7, pp. 414-417, 1966.
V. A. Morozov, â€œMethods for Solving Incorrectly Posed Problems, â€œ Springer-Verlag, New York,
1984.

2188

Algorithmic Approach for Learning a Comprehensive View of Online Users
[65]
[66]
[67]

[68]
[69]
[70]
[71]
[72]

[73]
[74]
[75]
[76]
[77]

[78]
[79]
[80]
[81]
[82]
[83]

[84]
[85]

Kourosh Modarresi

A. Narayanan, V. Shmatikov, â€œRobust de-anonymization of large sparse datasets,â€ IEEE
Symposium on Security and Privacy, 2008, 111-125.
B. K. Natarajan, â€œSparse approximate solutions to linear systems,â€ SIAM J. Comput., 24(2):227â€“
234, 1995.
R. Otazo, E. J. CandÃ¨s and D. Sodickson, â€œLow-rank and sparse matrix decomposition for
accelerated dynamic MRI with separation of background and dynamic components,â€ To appear in
Magnetic Resonance in Medicine, 2013.
R. L. Parker , â€œUnderstanding inverse theory,â€ Ann. Rev. Earth Planet. Sci., No. 5, pp. 35-64, 1977.
T. Raus, â€œThe principle of the residual in the solution of ill-posed problems with nonselfadjoint
operator,â€ Uchen. Zap. Tartu Gos. Univ., 75, pp. 12-20, 1985.
T. Reginska, â€œA Regularization Parameter in Discrete Ill-Posed Problems,â€ SIAM J. Sci. Comput.,
No. 17, pp. 740-749, 1996.
F. Ricci, L. Rokach, B. Shapira. P.B. Kantor (Eds.), â€œRecommender Systems Handbook,â€ Springer,
New York, NY, USA, 2011.
E. Sadikov, M. Medina, J. Leskovec, H. Garcia-Molina.,â€œCorrecting for Missing Data in
Information Cascades,â€ ACM International Conference on Web Search and Data Mining (WSDM),
2011.
A. Tarantola and B. Valette , â€œGeneralized nonlinear inverse problems solved using the least
squares criterion,â€ Reviews of Geophysics and Space Physics, No. 20, pp. 219-232 , 1993.
A. Tarantola, â€œInverse Problem Theory, Elsevir, Amsterdam ,â€ 1987.
Tibshirani, R., â€œRegression Shrinkage and Selection via the Lasso,â€ Journal of the Royal Statistical
Society, Series B, 58, 267â€“288, 1996.
R. Tibshirani, â€œRegression shrinkage and selection via the LASSO,â€ Journal of the Royal statistical
society, series B, 58(1):267â€“288, 1996.
A. N. Tikhonov, â€œSolution of Incorectly Formulated Problems and the
Regularization Method,â€ Soviet Math. Dokl., 4(1963), pp. 1035-1038; English translation of Dokl.
Akad. Nauk. SSSR, 151(1963), pp. 501-504, 1963.
A. N. Tikhonov, â€œRegularization of incorrectly posed problems,â€ Dokl. Akad. Nauk. SSSSR, 153,
(1963), pp. 49-52= Soviet Math. Dokl., 4, 1963.
A. N. Tikhonov, V. Y. Arsenin, â€œSolutions of Ill-Posed Problems,â€ Winston, Washington,
D.C.(1977).
A. N. Tikhonov, A. V. Goncharsky(Eds), â€œIll-Posed Problems in the Natural Sciences,â€,MIR,
Moscow, 1987.
A. N. Tikhonov, A. V. Goncharsky, V. V. Stepanov, A. G. Yagola, â€œNumerical Methods for the
Solution of Ill-Posed Problems,â€ Kluwer, Dordrecht, the Netherlands, 1995.
R. Witten and E. J. CandÃ¨s, â€œ Randomized algorithms for low-rank matrix factorizations: sharp
performance bounds,â€ To appear in Algorithmica, 2013.
Z. Zhang, H. Zha, and H. Simon, â€œLow rank approximations with sparse
factors I: basic algorithms and error analysis,â€ SIAM journal on matrix
analysis and its applications, 23(3):706â€“727, 2002.
Z Zhou, J. Wright, X. Li, E. J. CandÃ¨s and Y. Ma, â€œStable Principal Component Pursuit,â€
Proceedings of International Symposium on Information Theory, June 2010.
H. Zou, T. Hastie, and R. Tibshirani, â€œSparse Principal Component Analysis,â€ Journal of
Computational & Graphical Statistics, 15(2):265â€“286, 2006.

2189

