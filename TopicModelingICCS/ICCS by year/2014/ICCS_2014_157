Procedia Computer Science
Volume 29, 2014, Pages 102‚Äì112
ICCS 2014. 14th International Conference on Computational Science

Blood Flow Arterial Network Simulation with the Implicit
Parallelism Library SkelGIS
H¬¥el`ene Coullon1 ,3 , Jose-Maria Fullana2 , Pierre-Yves Lagr¬¥ee2 , S¬¥ebastien Limet1 ,
and Xiaofei Wang2
1

Univ. Orl¬¥eans, INSA Centre Val de Loire, LIFO EA 4022, F-45067 Orl¬¥eans, France
helene.coullon@univ-orleans.fr, sebastien.limet@univ-orleans.fr
2
CNRS & UPMC Univ Paris 06, UMR 7190,
Institut Jean Le Rond d‚ÄôAlembert, BoÀÜƒ±te 162, F-75005 Paris, France
fullana@ida.upmc.fr, pierre-yves.lagree@upmc.fr, xfwang0219@gmail.com
3
Antea G¬¥eo-Hyd, 101 rue Jacques Charles, 45160 Olivet, France

Abstract
Implicit parallelism computing is an active research domain of computer science. Most implicit
parallelism solutions to solve partial diÔ¨Äerential equations, and scientiÔ¨Åc simulations, are based
on the speciÔ¨Åcity of numerical methods, where the user has to call speciÔ¨Åc functions which embed
parallelism. This paper presents the implicit parallel library SkelGIS which allows the user to
freely write its numerical method in a sequential programming style in C++. This library relies
on four concepts which are applied, in this paper, to the speciÔ¨Åc case of network simulations.
SkelGIS is evaluated on a blood Ô¨Çow simulation in arterial networks. Benchmarks are Ô¨Årst
performed to compare the performance and the coding diÔ¨Éculty of two implementations of the
simulation, one using SkelGIS, and one using OpenMP. Finally, the scalability of the SkelGIS
implementation, on a cluster, is studied up to 1024 cores.
Keywords: Implicit parallelism, network simulations, PDEs, Blood Ô¨Çow

1

Introduction

A scientiÔ¨Åc simulation program is an imitation of a process over time on a computer. Most of the
time, the real phenomena is modeled by a system of partial diÔ¨Äerential equations which are timeand space-dependent (PDEs). In almost all cases, it is not possible to directly solve analytically
these equations. Hence, approximated solutions of the system are computed using numerical
methods [14, 15] which discretize time and space. In a program, the discrete representation of
space, called a mesh, is implemented by a data structure which models the connectivity between
space elements. Time discretization, on the other hand, is translated to a main loop for time
iterations. Three numerical methods with space discretization are more popular. The Ô¨Årst one,
based on the derivation from Taylors polynomial, is called Ô¨Ånite diÔ¨Äerence method. In this
102

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.010

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

method the spatial domain is discretized by a regular mesh, typically a Cartesian grid. In the
second method, called Ô¨Ånite volume method, the domain can be discretized by a general mesh
where elements are called volume controls. This method computes the averaged value of the
exact solution on each mesh element by integrating the given system on volume controls. The
third resolution method is called Ô¨Ånite element method. The exact solution is approximated
by a continuous and piecewise polynomial function and the domain is often discretized by a
triangulation. Each time iteration applies the scheme, obtained by the resolution method, to
the mesh. The type of scientiÔ¨Åc simulations we are interested in can be represented by the
explicit scheme
{Ut‚àí1 (x), Ut‚àí1 (y); y ‚àà N (x)} ‚àí‚Üí Ut (x),
(1)
where x represents an element of the mesh (i.e. discretized space domain), Ut (x) is the set
of quantities to compute for element x at the time iteration t, N (x) is the neighborhood of x
required to compute Ut (x) in the mesh. The precision of a simulation is deÔ¨Åned by its order
which gives information on needed neighborhood. In computer-science, such a computation is
called a stencil, and this domain is currently arousing a great deal of interest. The last important
point concerning scientiÔ¨Åc simulations is the concept of physical border of the domain. In a
real-world process, the domain is not limited, however, in a program, the discretized mesh has
to be Ô¨Ånite. Thus, speciÔ¨Åc elements have to be created, to deÔ¨Åne the behavior of the physical
border of the mesh.
Complexity of scientiÔ¨Åc models and precision of data to compute in simulations are growing.
Moreover, access to super-computers is becoming easier with a growing number of clusters over
the world. As a result, it becomes signiÔ¨Åcant for scientists of all domains to write parallel
programs. For this reason, and because of lack of time and resources to get eÔ¨Écient parallel
programs, implicit parallelism research is an active domain of computer-science. To be used,
an implicit parallelism solution has to Ô¨Ånd a good abstraction level for the user. Indeed, if the
solution is too generic, it works for most scientiÔ¨Åc problems, but it could be diÔ¨Écult to use. On
the other hand, if a solution is too speciÔ¨Åc, it could be too limited for most users. In this paper
is presented the implicit parallelism library SkelGIS [6, 7]. This library is speciÔ¨Åc to scientiÔ¨Åc
simulations which can be represented by the scheme (1). However, this simulation class is very
large, as most scientiÔ¨Åc simulations are solved with explicit numerical schemes.
The implicit parallelism library odeint [1] solves ordinary diÔ¨Äerential equations and proposes
implicit parallelism on GPUs with CUDA. PETSc [2] solves partial diÔ¨Äerential equations and
proposes implicit parallelism for GPUs, with CUDA, CPUs, with MPI and pthreads programming, and hybrid systems. The main diÔ¨Äerence with SkelGIS is the view of the user on the
simulation, and his programming freedom. SkelGIS does not provide predeÔ¨Åned tools to solve
diÔ¨Äerential equations, as for example interpolate or jacobi functions etc. The abstraction level
proposed by SkelGIS is closer to the OP2 framework [16] and the domain speciÔ¨Åc language
Liszt [9]. OP2 and Liszt both are implicit parallelism solutions to solve partial diÔ¨Äerential
equations based on unstructured-meshes solutions. In SkelGIS, the user has the total control
on the produced code. However, all complex data structures, optimizations and parallelizations
are hidden from the user through four macroscopic concepts: distributed data structures, data
mapping, applier and programming interface. Finally, SkelGIS can be compared to generic
libraries as for example STAPL [5] which is a parallel version of the C++ standard library
(STL). However, SkelGIS is speciÔ¨Åc to scientiÔ¨Åc simulations, while STAPL is as generic as the
STL.
The four concepts of SkelGIS have already been applied to the case of two-dimensional
regular meshes. Both heat equation and shallow-water equations have been solved using SkelGIS [6, 7]. EÔ¨Éciency of SkelGIS programs were convincing compared to the equivalent MPI
103

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

codes, and with a smaller coding eÔ¨Äort, according to Halstead metrics [11]. SkelGIS aims to
apply its four concepts to diÔ¨Äerent kinds of simulations and to diÔ¨Äerent kinds of meshes. This
paper deals with the case of network simulations which highlights diÔ¨Éculties which do not occur in a simulation on a two-dimensional regular mesh. In a network simulation, the domain
is divided in two diÔ¨Äerent kinds of elements with two diÔ¨Äerent behaviors (diÔ¨Äerent schemes):
nodes and edges. A network also represents the connectivity between diÔ¨Äerent elements of the
domain, as does a mesh. A network is typically used for arterial or vein simulations, road or
rail traÔ¨Éc simulations, water-Ô¨Çow or pollutant transfer simulations etc. A network can be represented as a graph and in most cases as a directed acyclic graph (DAG). The physical border
of a network is simulated by the speciÔ¨Åc behaviors of roots and leaves of the DAG. As far as we
know, no speciÔ¨Åc implicit parallelism solutions exist to solve network simulations. Libraries like
PETSc, which implement sparse matrices, could be used since a network can be represented
by a sparse matrix. However, two diÔ¨Äerent schemes have to be computed, then multiple kinds
of sparse matrices, for nodes and edges, have to be managed by the user. On the other hand,
using OP2 and Liszt, an unstructured mesh has to be created by the user, however a network
is not an unstructured mesh and is not composed of faces and cells. Thus, it seems impossible
to deÔ¨Åne a network with those solutions. The work presented in this paper oÔ¨Äers an intuitive
way to write network simulations and to obtain eÔ¨Écient parallel programs. The paper studies implementation of a complex network simulation using SkelGIS and is organized as follow.
Concepts of the SkelGIS library as well as details on the speciÔ¨Åc case of networks are explained
in the next section. Then, the blood-Ô¨Çow arterial simulation and its numerical resolution are
detailed. Performance results are compared to an OpenMP version of the same simulation, and
eÔ¨Éciency on a big network is evaluated on a cluster. Finally, conclusion and perspectives on
this work are given.

2

SkelGIS implicit parallelism library

The SkelGIS library is an implicit parallelism solution for scientiÔ¨Åc simulations. It aims to oÔ¨Äer a
transparent access to parallel computing. Niklaus Wirth‚Äôs aphorism [18] ‚ÄúProgram = Algorithm
+ Data Structure‚Äù (1) can be transposed to SPMD parallel programs (Single Program Multiple
Data) on distributed memory architectures: ‚ÄúParallel program = Distributed data + Algorithm
+ communications‚Äù (2). SkelGIS generates MPI parallel programs of type (2), however, it aims
at providing a sequential programming style of type (1). Figure 1 illustrates the SkelGIS
principles that relies on four concepts named DDS, DPMap, AP and PI. DDS is a distributed
data structure which manages the mesh and its connectivity and distributes automatically the
mesh among processors. Most of the eÔ¨Éciency of SkelGIS relies on the DDS. DPMap is a data
mapping. Each of its instantiation represents data of the simulation (quantities it uses) and its
mapping on the DDS. AP is an applier. It is used to apply a sequential user function, called an
operation, to a set of DPMaps. An applier also transparently proceeds MPI communications
between processors. Finally, PI represents the programming interface of SkelGIS. PI is used
by the programmer to navigate through the data, read and update them. This interface is
based on iterators and speciÔ¨Åc functions to access the neighborhood of mesh‚Äôs elements (see
Equation (1)).
As illustrated in Figure 1, SkelGIS users write an operation OP using the programming
interface PI. Then, the operation OP is called through an applier AP. SkelGIS supports transparently data distribution and communications. This way, a sequential view of the program is
provided to the user (Niklaus Wirth‚Äôs aphorism), while a parallel program is actually written.
SkelGIS implements several DDSs and their related DPMaps, AP and I. In this paper,
104

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

Proc1
AP

OP

comm

PI

DDS

PI

OP

DS

User's view

Proc2
comm

DDS

PI

OP

DPMap

DPMap

Real view

Figure 1: SkelGIS user‚Äôs view and its actual parallel execution

parallelization of an arterial blood-Ô¨Çow simulation using SkelGIS is studied, thus the rest of
this section describes SkelGIS components used to represent networks.
DDS. The DDS which represents a network is called distributed DAG and is denoted DDAG.
The implementation of the DDAG object is not detailed in this paper, but it uses a modiÔ¨Åed
and parallel version of the ‚ÄúCompressed Sparse Row‚Äù storage (CSR) [3]. CSR is a light data
structure to store sparse matrices. SkelGIS uses a modiÔ¨Åed CSR method optimized to store
distributed DAGs. This representation is also speciÔ¨Åcally improved for scientiÔ¨Åc simulations to
optimize accesses to physical border elements and neighborhood elements. the DDAG object
manages eÔ¨Écient communications between processors including an overlap of computations with
communications. Since a DDAG instantiation is an irregular data structure, its distribution is
very important to obtain a good load balancing between processors. This data distribution can
be represented as a graph partitioning problem, and is solved in the current version of SkelGIS
using a sibling-edges heuristic, adapted to scientiÔ¨Åc simulations. This data distribution produces
sensible results but will be improved in future work using hypergraph partitioning [12] and the
partitioner Mondriaan [17].
DPMap. An eÔ¨Écient access to data in an irregular structure, as a network, leads to a complex
data structure which is heavy to store. Thus, to minimize the memory footprint of the whole
data storage, SkelGIS clearly separates the DDAG data structure from data mapped on it. This
way, the DDAG is created once and data are stored in lighter objects: DPMaps. Two kinds
of DPMaps are available for the DDAG DDS. DPMap Nodes and DPMap Edges to map data
respectively on nodes and edges of the network.
AP. A SkelGIS applier performs a communication phase, to exchange ghost data between
processors, overlapped by a computation phase where each processor applies the operation
deÔ¨Åned by the user. The prototype of the applier is the following
apply list : {DP M ap Edges} √ó {DP M ap N odes} √ó OP

(2)

where {DP M ap Edges} and {DP M ap N odes} denote two sets of DP M ap Edges and
DP M ap N odes instantiations used by the operation OP .
105

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

PI. An operation OP is a sequential function written by the user. It manipulates
DPMap Edges and DPMap Nodes instantiations through some programming interfaces. First,
three kinds of iterators allow the user to navigate through DPmaps. According to Equation (1),
there is no computational dependency between elements of the mesh in the same time iteration. Thus, the Ô¨Årst kind of iterators moves through nodes or edges and guarantees that the
whole DPMap is parsed in an unknown order. Two other iterators are used to parse physical
border elements of the network. As already explained, physical border elements of a network
are roots and leaves of the DAG. The bracket operator [] is used with an iterator it, [it], to
access and update values in DPMaps. Finally, SkelGIS provides some methods to access the
neighborhood of an element in the DDAG. Figure 2(a) shows the neighborhood respectively for
nodes and edges of the DDAG. This Ô¨Ågure illustrates that computations on a node may depend
on incoming nodes and edges, and outgoing nodes and edges. Computations on an edge, on
the other hand, could be only impacted by its source and destination nodes. As a consequence,
DPMap Edges have two methods which return an iterator to access, respectively, the source
and the destination node. In the case of DPMap Nodes, two methods return a list of iterators
to access the incoming and outgoing nodes of the current node, and two methods return the
equivalent incoming and outgoing lists for edges. In a distributed simulation, the neighborhood implies some MPI communications between processors. To automatically proceed those
communications, before applying an operation, the applier uses the information given by the
user when deÔ¨Åning the DDS and its DPMaps. In many network scientiÔ¨Åc simulations, edges
represent a part of the space (a section of a river or an artery for example) which is discretized
by a one-dimensional mesh as illustrated in Figure 2(b). In such a case, the order of the onedimensional scheme has to be speciÔ¨Åed by the user to automatically optimize communications.
Thus, for example, in Figure 2(b), only information on two values at the beginning and at the
end of the mesh are needed by other processors.









EDGE ORDER=2




	


	
	


(a) Neighborhood of nodes and edges in a DDAG.

(b) 2nd order scheme on edges.

Figure 2: Neighborhood and communications for networks

Thanks to the four SkelGIS concepts dedicated to network simulations, it is possible to
totally hide parallelization of codes from the user. Moreover, programming freedom is preserved
through a sequential programming style. In the next section, the complex simulation of arterial
blood-Ô¨Çow is explained, and the parallelization of this simulation, using SkelGIS, is described.
106

Blood Ô¨Çow with SkelGIS

3
3.1

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

The 1D model of arterial blood-Ô¨Çow
1D mathematical model

The 1D model of blood-Ô¨Çow in large arteries is usually derived with two main assumptions:
axisymmetric velocity proÔ¨Åle and large wave length compared with the radius of the vessel, see
literature such as [10, 13]. By integrating the Navier Stokes equations across the cross section
of the artery, one obtains two PDEs, which link the cross sectional area A, the volumetric Ô¨Çow
rate Q and the internal pressure P :
A ‚àÇP
Q
‚àÇA ‚àÇQ
‚àÇQ
‚àÇ Q2
+
= 0, and
+
( )+
= ‚àí8œÄŒΩ ,
‚àÇt
‚àÇx
‚àÇt
‚àÇx A
œÅ ‚àÇx
A

(3)

where x is the longitudinal axis of the artery, t is time and ‚àí8œÄŒΩ is a coeÔ¨Écient of the friction.
The Ô¨Årst PDE is the conservation law of mass, and the second is the balance of the momentum.
The blood density œÅ is assumed a constant, ŒΩ is the kinematic viscosity (the modeling of shear
stress and inertia is discussed in [13]). To close the system we assume that the arterial wall is
thin, isotropic, homogeneous, incompressible, and moreover that it deforms axisymmetrically
with each circular cross-section independently of the others and it behaves like a Kelvin-Voigt
model. We denote the undeformed cross-sectional area by A0 and the external pressure of the
vessel by Pext . Then, the relation linking A and P is:
‚àö
P = Pext + Œ≤( A ‚àí

A0 ) + ŒΩ s

‚àÇA
,
‚àÇt

(4)

with the stiÔ¨Äness coeÔ¨Écient Œ≤, and the viscosity (of the arterial wall) coeÔ¨Écient ŒΩs .

3.2

Numerical resolution

A conservation law can be written in the general form
‚àÇF
‚àÇU
+
= S,
‚àÇt
‚àÇx
where U is the conservation variable, F the Ô¨Çux and S the source term. For Ô¨Ånite volume
method, the domain is decomposed into Ô¨Ånite volumes or cells with vertex xi as the center of
cell [xi‚àí1/2 , xi+1/2 ]. In every cell, the conservation law must hold (see [8]),
xi+1/2
xi‚àí1/2

‚àÇU
dx +
‚àÇt

xi+1/2
xi‚àí1/2

‚àÇF
dx =
‚àÇx

xi+1/2

Sdx.
xi‚àí1/2

Gauss‚Äôs theorem is used on the second term and variables are approximated by the averaged
xi+1/2
xi+1/2
1
1
U (x)dx, Si = Œîx
S(x)dx. After the discretization
values in each cell: Ui = Œîx
xi‚àí1/2
xi‚àí1/2
in space, we have the semi-discrete form,
‚àó
‚àó
‚àí Fi‚àí1/2
)
(Fi+1/2
dUi
= Œ¶(Ui‚àí2 , ...Ui+2 ) were Œ¶(Ui‚àí2 , ...Ui+2 ) = ‚àí
+ Si .
dt
Œîx
‚àó
‚àó
The numerical Ô¨Çuxes Fi+1/2
and Fi‚àí1/2
are given by Rusanov Ô¨Çux (Depending on the approximate approaches on solving the Riemann problem, diÔ¨Äerent numerical Ô¨Çuxes are possible, with
slightly diÔ¨Äerent numerical diÔ¨Äusivity. This one is widely used because it is simple and robust, according to [4]). For second order accuracy, a MUSCL (monotonic upwind scheme for

107

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

conservation law) linear reconstruction technique is used. This is a scheme with Ô¨Åve stencils.
The values at x1 and xN +1 are determined by the characteristic method. For the temporal
integration, we may apply a 2-step second order Adams-Bashforth (A-B) scheme,
Un+1 = Un + Œît

3
1
Œ¶(Un ) ‚àí Œ¶(Un‚àí1 ) .
2
2

This scheme can be initiated by a forward Euler method.
Arteries are joined at conjunctions: the nodes of the network. We take a simple branching
with two daughter arteries as an example. At the node, there are then six boundary conditions,
n+1
n+1
and Qn+1
for the outlet of the parent artery and An+1
and Qn+1
for the
An+1
p
p
d1 , Qd1 ,Ad2
d2
inlets of the two daughter arteries. The conservation law of mass and momentum Ô¨Çuxes reads
n+1
= 0,
‚àí Qn+1
Qn+1
p
d1 ‚àí Qd2
n+1
1 Qn+1
1 Qdi 2
p
œÅ( n+1 )2 + Ppn+1 ‚àí œÅ( n+1
) ‚àí Pdn+1
=0
i
2 Ap
2 Ad i

(5)
i = 1, 2.

(6)

shall be expressed in cross-sectional area A by the constituThe pressures Ppn+1 and Pdn+1
i
tive relation (4). Moreover, the unknowns should match the three outgoing characteristics of
the joined arteries [10]. Thus we obtain a nonlinear algebraic system of 6 equations with 6
unknowns, which can be readily solved by Newton-Raphson iterative method with U n as the
initial guess.

3.3

Parallelization of the simulation

This simulation has Ô¨Årst been implemented sequentially using C++. Then, two parallel programs have been derived from the sequential code. The Ô¨Årst one is an OpenMP version of the
sequential code, which applies a coarse-grain parallelization model. This produces a SPMD
parallel program (Single Program, Multiple Data) where each processor is in charge of a subpart of the network. Since this program is a shared memory solution, no communications are
needed between processors which can directly access values managed by other processors. The
advantage of this kind of parallelization is that the sequential code is almost not modiÔ¨Åed.
However, coarse-grain parallelization is not completely implicit as the user has to manage the
distribution of the network among processors and specify whether a variable is shared or local.
The second parallel version of the simulation has been implemented with SkelGIS. The main
function pseudo-code of the simulation is shown in Algorithm 1. This code is very close to the
sequential code, but it uses SkelGIS objects and tools instead of homemade data structures.
The blood-Ô¨Çow operation pseudo-code is described in Algorithm 2. The operation is organized
exactly as the sequential code. Thus, it Ô¨Årst deals with roots and leaves of the network to
simulate the physical border behavior (lines 1 to 13). Then, as in the sequential code, conjunction nodes of the arterial network (nodes of the DAG) are solved. Finally arteries of the
network (edges of the DAG) are solved. For each of these steps, iterators are used to move
through elements of the network. Most of the sequential code can be kept because an object
obtained from a SkelGIS iterator is a plain C++ variable as in the sequential code. Thus, as
the SkelGIS program keeps the sequential code structure and most of the sequential code itself,
the eÔ¨Äort to code the blood-Ô¨Çow simulation with SkelGIS is quite light. In addition to this, the
use of SkelGIS data structures avoids the user to implement its own containers. As a result,
the SkelGIS code is even simpler than the sequential code.
108

Blood Ô¨Çow with SkelGIS

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

Algorithm 1: Main function of the blood-Ô¨Çow simulation with SkelGIS
1
2
3
4
5
6

Instantiation of the distributed network DDAG
Instantiation of DP M aps using DDAG
Initializations
while not end of time iteration do
applier({DP M ap},bloodÔ¨Çow)
end

Algorithm 2: Sequential SkelGIS bloodÔ¨Çow operation

1
2
3
4
5
6
7
8
9
10
11
12
13

4

Data: {DPMap}
Result: ModiÔ¨Åcation of {DPMap}
ItR := beginning iterator on roots
endItR := end iterator of on roots
while ItR‚â§endItR do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItR++
end
ItL := beginning iterator on leaves
endItL := end iterator of on leaves
while ItL‚â§endItL do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItL++

14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

end
ItC= beginning iterator on conjunctions
endItC := end iterator on conjunctions
while ItC‚â§endItC do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItC++
end
ItA= beginning iterator on arteries (edges)
endItA := end iterator on arteries
while ItA‚â§endItA do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItA++
end

Experiments

This section Ô¨Årst presents performance comparisons between OpenMP and SkelGIS parallelizations of the 1D arterial blood-Ô¨Çow simulation. Then, both versions are compared in terms
of programming eÔ¨Äorts. Finally, as the SkelGIS version is written in MPI and made for distributed memory architectures, it is evaluated on a bigger network with more processors of a
cluster. Experiments have been computed on thin/standard nodes of the TGCC-Curie cluster
(20th cluster in the top500 list of November 2013). Each of those nodes is equipped with two
eight-cores CPU Sandy Bridge clocked at 2.7GHz, 64Go of RAM DDR3 and a local SSD disk.
Each experiment has been compiled with the Ô¨Çag -O3, and has been computed 4 times. The
average execution time is used in the evaluations of this section.
As explained in the previous section, the OpenMP version uses a coarse-grain parallelization
which, most of the time, produces better performance than a Ô¨Åne-grain parallelization but does
not totally hide parallelization of codes. The SkelGIS version, on the other hand, produces an
MPI program, where parallelization of codes is totally hidden from the user. As the OpenMP
version is a shared memory solution, it runs on a single machine. Thus, a single standard
node of TGCC-Curie, with 16 cores, has been used for evaluations. An artery network of 4.000
arteries and conjunction nodes has been used for this experiment. Speedups of both versions
are presented in Figure 3(a), while Figure 3(b) represents execution times with a logarithmic
scale.
First, execution times and speedup of the SkelGIS version are better than the OpenMP
109

Blood Ô¨Çow with SkelGIS

(a) Speedups of OpenMP and SkelGIS versions

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

(b) Execution time with logarithmic scale

Figure 3: Experimental comparisons between OpenMP and SkelGIS implementations

version. However, one can note that the diÔ¨Äerence is exaggerated by the knee at two cores in
the scaling graph of the OpenMP version. However, even if this weakness is not taken into
account, speedup of the OpenMP version is less steep than the SkelGIS one.
Thus the SkelGIS version of the arterial blood-Ô¨Çow network has interesting performance
results compared to an OpenMP equivalent version even if this OpenMP version could probably
be improved. Moreover, the SkelGIS version totally hides parallelization of codes while the
coarse-grain parallelization of the OpenMP version does not. Thus, it is interesting to evaluate,
with metrics, the eÔ¨Äort needed to code both parallel simulations. Halstead metrics [11] have
been used to estimate the diÔ¨Éculty and the eÔ¨Äort to write both source codes. It results that
the OpenMP code is 20% more diÔ¨Écult to write than the SkelGIS code, and it requires a
programming eÔ¨Äort 80% greater than the SkelGIS version. As explained previously, using
SkelGIS, the data structure, which represents the network and its connectivity, does not have
to be implemented by the user. The user code is thus drastically simpliÔ¨Åed. As a result, the
length and the diÔ¨Éculty of the SkelGIS implementation is more interesting than the OpenMP
implementation.
SkelGIS, is a parallel library to create parallel scientiÔ¨Åc simulations for distributed memory
architectures. SkelGIS programs are designed to run on clusters. Thus, the SkelGIS 1D arterial
blood-Ô¨Çow simulation has been evaluated on the TGCC-Curie cluster. To show the scalability
of the SkelGIS program, a bigger network than the one used in the previous experiment, has
been used. It contains 15.000 arteries and 15.000 conjunction nodes. The speedup presented on
Figure 4(a) illustrates the speedup from 1 to 128 cores and the speedup presented on Figure 4(b)
represents evaluations from 1 to 1024 cores. From 1 to 128 cores, the speedup is almost ideal and
linear, which is a very good result for network simulations, especially for an implicit parallelism
solution where no parallel code has been written by the user. With more cores, the speedup
begins to fall. This is probably due to a weakness in the data distribution implemented. This
implementation should be improved to avoid load imbalance and to decrease the communication
cost. Two improved data distributions are under progress.
110

Blood Ô¨Çow with SkelGIS

(a) SkelGIS speedup from 8 to 128 cores

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

(b) SkelGIS speedup from 8 to 1024 cores

Figure 4: Speedups of SkelGIS implementation on a network of 15000 arteries

5

Conclusion

In this paper has been presented the implicit parallelism library SkelGIS for the speciÔ¨Åc case
of network simulations. Indeed, SkelGIS can be used for networks which can be represented
as directed acyclic graphs, and the four concepts of SkelGIS have been explained for this case.
A real case-study: the blood-Ô¨Çow simulation in an arterial network, has been described, and
numerical schemes of both conjunction nodes and arteries have been explained. Three experiments illustrate that, Ô¨Årst, the obtained SkelGIS code is eÔ¨Écient on a single multi-core machine
compared to an equivalent OpenMP program. Second, the eÔ¨Äort needed to implement the simulation is greater with OpenMP than with SkelGIS. Finally, the SkelGIS simulation has a very
good scalability on clusters. As explained previously, SkelGIS has already been implemented for
two-dimensional regular meshes. A work in progress is to propose a SkelGIS implementation
for general networks, thus the work presented in this paper on DAGs will be generalized to
graphs. After this, SkelGIS will be implemented for unstructured meshes and later for adaptive
meshes to be able to solve most scientiÔ¨Åc simulations in parallel.

References
[1] Karsten Ahnert and Mario Mulansky. Odeint - solving ordinary diÔ¨Äerential equations in C++.
CoRR, abs/1110.3397, 2011.
[2] S. Balay, W. D. Gropp, L. Curfman McInnes, and B. F. Smith. EÔ¨Écient management of parallelism
in object oriented numerical software libraries. In Modern Software Tools in ScientiÔ¨Åc Computing,
pages 163‚Äì202. Birkh¬®
auser Press, 1997.
[3] R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo,
C. Romine, and H. Van der Vorst. Templates for the Solution of Linear Systems: Building Blocks
for Iterative Methods, 2nd Edition. SIAM, Philadelphia, PA, 1994.
[4] F. Bouchut. Nonlinear stability of Ô¨Ånite volume methods for hyperbolic conservation laws and
well-balanced schemes for sources. Birkh¬®
auser, 2004.
[5] A. Buss, Harshvardhan, I. Papadopoulos, O. Pearce, T. Smith, G. Tanase, N. Thomas, X. Xu,
M. Bianco, N. M. Amato, and L. Rauchwerger. Stapl: standard template adaptive parallel library.

111

Blood Ô¨Çow with SkelGIS

[6]

[7]
[8]
[9]

[10]
[11]
[12]
[13]
[14]
[15]
[16]

[17]
[18]

112

Coullon, Fullana, Lagr¬¥ee, Limet and Wang

In Proceedings of the 3rd Annual Haifa Experimental Systems Conference, SYSTOR ‚Äô10, pages
14:1‚Äì14:10. ACM, 2010.
H¬¥el`ene Coullon, Minh-Hoang Le, and S¬¥ebastien Limet. Parallelization of shallow-water equations
with the algorithmic skeleton library skelgis. In ICCS, volume 18 of Procedia Computer Science,
pages 591‚Äì600. Elsevier, 2013.
H¬¥el`ene Coullon and S¬¥ebastien Limet. Algorithmic skeleton library for scientiÔ¨Åc simulations: Skelgis. In HPCS, pages 429‚Äì436. IEEE, 2013.
O. Delestre and P.-Y. Lagr¬¥ee. A well balanced Ô¨Ånite volume scheme for blood Ô¨Çow simulation.
International Journal for Numerical Methods in Fluids, page doi: 10.1002/Ô¨Çd.3736, 2012.
Z. DeVito, N. Joubert, F. Palacios, S. Oakley, M. Medina, M. Barrientos, E. Elsen, F. Ham,
A. Aiken, K. Duraisamy, E. Darve, J. Alonso, and P. Hanrahan. Liszt: A domain speciÔ¨Åc language
for building portable mesh-based pde solvers. In Proceedings of 2011 International Conference for
High Performance Computing, Networking, Storage and Analysis, SC ‚Äô11, pages 1‚Äì12. ACM, 2011.
L. Formaggia, A. Quarteroni, and A. Veneziani. Cardiovascular Mathematics: Modeling and
simulation of the circulatory system, volume 1. Springer, 2009.
Maurice H. Halstead. Elements of Software Science (Operating and programming systems series).
Elsevier Science, New York, USA, 1977.
Bruce Hendrickson and Tamara G. Kolda. Graph partitioning models for parallel computing.
Parallel Comput., 26(12):1519‚Äì1534, November 2000.
P.-Y. Lagr¬¥ee. An inverse technique to deduce the elasticity of a large artery. EPJ Applied Physics,
9(2):153‚Äì164, 2000.
Brigitte Lucquin and Olivier Pironneau. Introduction to ScientiÔ¨Åc Computing. John Wiley & Sons,
Inc., New York, NY, USA, 1997.
W.E. Milne. Numerical Solution of DiÔ¨Äerential Equations. Applied Mathematics Series. John
Wiley and Sons, 1953.
GR Mudalige, MB Giles, I. Reguly, C. Bertolli, and PHJ Kelly. Op2: An active library framework
for solving unstructured mesh-based applications on multi-core and many-core architectures. In
Innovative Parallel Computing (InPar), pages 1‚Äì12. IEEE, 2012.
Brendan Vastenhouw and Rob H. Bisseling. A two-dimensional data distribution method for
parallel sparse matrix-vector multiplication. SIAM Review, 47(1):67‚Äì95, 2005.
Niklaus Wirth. Algorithms + Data Structures = Programs. Prentice Hall PTR, Upper Saddle
River, NJ, USA, 1978.

