Procedia Computer Science
Volume 29, 2014, Pages 246‚Äì255
ICCS 2014. 14th International Conference on Computational Science

EÔ¨Écient Global Element Indexing for Parallel Adaptive
Flow Solvers
Michael Lieb, Tobias Neckel, Thomas Sch¬®ops, and Hans-Joachim Bungartz
Technische Universit¬®
at M¬®
unchen, Garching, Germany
bungartz@in.tum.de, liebm@in.tum.de, neckel@in.tum.de, schoeps@in.tum.de

Abstract
Many grid-based solvers for partial diÔ¨Äerential equations (PDE) assemble matrices explicitly for
discretizing the underlying PDE operators and/or for the underlying (non-) linear systems of
equations. Often, the data structures or solver packages require a consecutive global numbering
of the degrees of freedom across the boundaries of diÔ¨Äerent parallel subdomains. Straightforward approaches to realize this global indexing in parallel frequently result in serial parts of
the assembling algorithms which causes a considerable bottleneck, in particular in large-scale
applications.
We present an eÔ¨Écient way to set up such a global indexing numbering scheme for large
conÔ¨Ågurations via a position-based numeration on all parallel processes locally. The global
number of shared nodes is determined via a tree-based communication pattern. We veriÔ¨Åed our
implementation via state-of-the-art benchmark scenarios for incompressible Ô¨Çow simulations.
A small performance study shows the parallel capability of our approach. The corresponding
results can be generalized to other grid-based solvers that demand for global indexing in the
context of large-scale parallelization.
Keywords:

1

Introduction

Most of the existing grid-based PDE solvers rely on parallelization to realize eÔ¨Écient simulations
of accurately reÔ¨Åned simulations. At the edge of exascale computing in HPC, every workaround
that introduces only small serial parts in algorithms needs to be closely inspected and eventually
removed or adapted to allow for performant scaling on an ever growing number of processors.
In this contribution, we focus on a very speciÔ¨Åc aspect of grid-based PDE algorithms: the
generation of global indices of degrees of freedom (DoF) across parallel domains. A typical
example of the usage of such global indices is the assembly of global matrices for operators or
corresponding system matrices. Matrix-free iterative solvers such as multigrid do not need such
a global numbering, neither in serial nor in parallel context. While these methods perform very
well in a variety of scenarios diÔ¨Äerent arguments exist for an explicit assembly depending on the
246

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.022

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

settings, the scenario, etc. One reason is for instance the advantage or necessity to play with
diÔ¨Äerent solver-preconditioner conÔ¨Ågurations of existing solver libraries (such as PETSc [2], e.g.)
to identify optimal behavior with respect to robustness and, in particular, to performance. In
the context of parallelization, such global assembly has to cope with diÔ¨Äerent challenges. One
issue is how to enhance parallel performance and avoid severe bottlenecks due to considerable
serial parts, in particular for high numbers of processors used (as it is the case on modern HPC
systems). Furthermore, multiple instances of DoF (vertices, e.g.) that have the identical global
index id have to be handled technically.
The method we present in this paper has been developed for hierarchically structured grids
as they are used in the context of spacetrees (see [9, 1, 10, 5], e.g.).
The remainder of this paper is organized as follows. In Section 2 introduce our incompressible Ô¨Çow solver Peano Fluid in which the new method is realized. The communication patterns
and general parallel scheme of the element numeration is discussed in Section 3. The parallel
performance of this approach is brieÔ¨Çy investigated in Section 4 whereas Section 5 summarizes
the results and gives an outlook on potential application Ô¨Åelds.

2

Incompressible Flow in the PDE-framework Peano

The presented algorithm is implemented in the parallel adaptive Ô¨Çow solver Peano Fluid which
is an application of the PDE framework Peano (cf. [3, 8, 11]). The whole Peano Framework is
developed in C++ and realizes Cartesian grids via k-spacetrees, space-Ô¨Ålling curves and stack
data structures. The adaptive mesh is generated automatically by a recursively subdividing the
hypercube root cell which embeds the geometry along each coordinate axis. The discrete iterate
of the Peano space-Ô¨Ålling curve on a corresponding adaptive Cartesian grid is shown in Figure
1b1 The framework supports automatic grid generation, data management and encapsulates
most of the parallel aspects such as exchange of boundary elements and synchronization of
global variables. Using the regular grid backbone, the solver runs on 4,000 processes at a
pressure
5
3
1.5
0.5
-0.12
-0.5
-2
-4

(a)

(b)

Figure 1: (a) Pressure distribution of the FSI benchmark CFD 2 (cf. [7]) at a Reynolds number
of Re=200. (b) Example of a two-dimensional adaptive Cartesian grid and its corresponding
discrete iteration of the Peano curve.
eÔ¨Éciency of more than 80%. Peano Fluid is able to process incompressible Ô¨Çow on complex
geometries with low memory demands and a high cache hit-rate (cf. [3, 4, 8], e.g.).
1 Note

the division into three child cells along the coordinate axis due to requirements of the Peano curve.

247

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

The solver supports 2D and 3D spatial discritization with Q1P 0 Ô¨Ånite elements and higherorder interpolated diÔ¨Äerential operators resulting in the following general form of the semidiscrete Navier-Stokes equations
AvÀô h + Dvh + C(vh )vh ‚àí M T ph

=

f,

M vh

=

0,

where A represents the mass matrix, D and C denote the discrete diÔ¨Äusion and convection
operators, and M is the discrete divergence.
Among the various implicit and explicit time integration methods available, the workhorse
is the explicit Euler scheme in combination with the Chorin projection method [6]. The basic
idea of this method is to derive the semi-discrete continuity equation (1) with respect to time:
M vÀô h = 0.

(1)

Formally inverting the semi-discrete momentum equations (1) with respect to vÀô h and inserting
them into (1) results in the discrete pressure Poisson equation (PPE),
(M A‚àí1 M T ) ph = M A‚àí1 (‚àíf + Dvh + C(vh )vh ),

(2)

=:Q

where Q represents a discrete analogon of the continuous Laplacian. In fact, Q results of
a summation of diÔ¨Äerent operator contributions of diÔ¨Äerent dimensions. In the 2D case, for
example, Q has the following form:
Q = Qx + Qy = Mx A‚àí1 MxT +My A‚àí1 MyT .
‚àá¬∑

(3)

‚àá

In each time step of the simulation, one linear system of equations has to be solved for
the pressure using the PPE (2) to update the velocities. The explicit time integration scheme
imposes a restriction on the time step size via the well-known CFL condition to assure stability
of the numerical scheme.
Peano‚Äôs Ô¨Çow solver capability in the context of dynamically changing adaptive grids has
been proven by simulating partitioned Ô¨Çuid-structure interaction scenarios such as proposed in
[7], see Figure 1a).
In order to explain an explicit example of a matrix assembly using global indexing, we have
a closer look into the construction of the PPE matrix Q in case of adaptive grids. An adaptive
mesh with leaf cells of diÔ¨Äerent resolutions contains hanging nodes, cf. Figure 2. The hanging
nodes are no real degrees of freedoms but get interpolated by respective parent vertices of the
grid hierarchy. Hence, hanging nodes make the parallel matrix setup more complicated.
In a two-dimensional setup, a usual vertex (which is not at the domain boundary) is related to four adjacent cells of equal size. In contrast, hanging nodes are located by deÔ¨Ånition
between cells of diÔ¨Äerent resolution on the edge of a coarse grid cell and have fewer then four
corresponding Ô¨Åne grid neighbor cells.
By storing the divergence contributions of each cell on the nodes, the divergence operator
Àú . M stores the
is setup directly. Technically, it is a combination of two matrices M and M
Àú
weights that cells on persistent nodes have, whereas M is an appendix to this matrix which
stores the weights of cells on hanging nodes. In a separate step we restrict the information of
the hanging nodes to their corresponding parental nodes resulting in the complete divergence
operator. Utilizing the inverted, lumped mass matrix A‚àí1 , the overall PPE system matrix
248

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

Figure 2: 2D Sketch of hanging nodes (red) and corresponding parent DoF (black) at the
interface of coarse and Ô¨Åne cells in the context of hierarchical Cartesian grids used in Peano.
is computed via global operations according to Q = M A‚àí1 M T (see Equation (3)). Hence,
this represents a typical case of global numerical linear algebra computations in the context of
PDEs. All these steps require a consistent and contiguous numbering of the degrees of freedom
(vertices and/or cells) to map the correct contributions in the calculations.

3

Tree-based Parallel Grid Element Numeration

When setting up the matrix, all nodes of the grid must have a unique number. This holds for
the nodes at the boundary of two adjacent processes, too. In the numeration process, we must
identify these process boundary nodes and assign the same id to them on all parallel processes.
For a clear matrix structure without empty spots and to avoid huge memory overheads
during the memory preallocation, it is essential that the numeration is consecutive at the end
of the numeration process. A non-consecutive numeration scheme would result in a matrix with
a size potentially much bigger than its rank due too many empty rows.
In contrast to the serial setup of the matrix, simultaneous a linear numbering of grid elements
is not possible in the parallel grid as the processes have no access to the status of other processes
or the global number of grid elements. Thus, every process can only numerate its own elements.
We propose a two-stage process:
1. We introduce a numeration scheme for all grid elements of the adaptive grid on the
diÔ¨Äerent reÔ¨Ånement levels which ensures a unique numeration for all DoF in our case
depending on their coordinates on the individual grid levels.
2. These numbers are sequenced in a global synchronization process where we create a mapping of the global elements to a unique but consecutive vertex index numeration.
The numeration scheme of Step 1 uses the position of the vertex and the grid level to Ô¨Ånd
a globally unique number (id) for it. The underlying principle numerates the vertices as in a
full grid (regularly reÔ¨Åned) starting from level 0 to level m. The level k of a regularly resolved
Peano grid of dimension d contains
#nodes = (3k + 1)d .

(4)

Within level k, we deÔ¨Åne the number of a vertex by its discrete position x
ÀÜ ‚àà Nd , x
ÀÜi being the
position in dimension i. A continuous numeration within one level results in the level vertex
249

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

‚éõ‚éõ

number
d‚àí1

‚éù‚éù

idnode,level =

i‚àí1

‚éû

‚éû

ÀÜi ‚é† + x
ÀÜ0 .
nj ‚é† x

(5)

j=0

i=1

The preceding grid levels of our hierarchical spacetree grid are considered as oÔ¨Äset o. On level
k, the oÔ¨Äset ok is the sum of the nodes of the preceding levels which are individually computed
via Eq. (4):
k

(3k + 1)d .

ok =

(6)

l=0

Hence, the global initial vertex number can be computed as
idnode,global = idnode,level + ok .

(7)

Nodes at the same position, but on a diÔ¨Äerent level get diÔ¨Äerent numbers assigned which
is important to handle fully hierarchical algorithms that demand for DoF on each level. The
sketch of a two level and two dimensional adaptive grid in Figure 3 shows the resulting vertex
numerations. The 4 nodes on level zero have numbers 0 ‚àí 3, the numbering for the 16 nodes on


	

















	

























Figure 3: Lexicographic numeration of vertex elements.
level one continues with a numbering from 4‚àí16. Inside the global domain the nearest neighbor
of a given discrete position on a certain level is determined and the corresponding vertex number
is returned. This way, we prevent problems due to possible numerical inaccuracies between grid
vertex coordinates and the discrete positions of Eq. (7) for diÔ¨Äerences of less than 28 levels
between the Ô¨Ånest and the coarsest grid2 .
In the parallel execution, vertices with the same number idnode,global are identical which
has to be respected as one in the matrix setup stage. With this Ô¨Årst stage of the numeration
scheme a unique cell-vertex mapping would be possible. This numeration scheme, however, has
the drawback that it contains gaps in the adaptive case.
PETSc uses the vertex numbers as column or row indices (see Section 2) in the data structure
management. Thus, the given vertex numbers have to be transformed in a continuous numbering
among all levels as it is depicted in Figure 3. The continuous numbering automatically reduces
the maximum vertex number to the total number of nodes.
2 In our Ô¨Çow simulation, we usually have a level diÔ¨Äerence of two to four between the Ô¨Ånest and the coarsest
grid level.

250

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

We map its functionality via a standard mapping to grid callbacks. Every time it encounters
a vertex adjacent to a cell the cell handler routine registers the vertex and assigns the global
level-based number to it. At the end of the operation the transformation is performed. Two
versions of it are available:
1. The compact implementation sends all its data to the global master process. Here, the
vertex containers of all processes are merged. The order of elements in the resulting order
in the vector gives us directly a consecutive numeration mapping which is sent back to the
workers. We don‚Äôt send the full mapping but only the new numbers which are mapped to
the old ones. After one empty grid iteration these numbers can be directly overwritten
from this array as the call order remains unchanged in static Peano grids. The advantage
is that we do not have to sort or search indices in the mappings. The disadvantage is that
we have to collect all indices at the master process and to send big data packages among
the complete system.
2. The second version performs the same sequence of operations but performs the communication following a tree-pattern. The leaf processes send their data to their parental
nodes. The distribution of the new keys follows again the tree structure but in reverse
order. The parent-child relationship can be speciÔ¨Åed in the corresponding class. It avoids
the agglomeration of big data packages occurring in the the compact implementation.
Details of the Tree-Based Approach In both cases we count how often an id is visited
locally by building a hash map from vertex id to a visit counter. Every call increments the
complementary visit counter. The synchronization between processes and the calculation of the
Ô¨Ånal vertex numbers is performed in Ô¨Åve steps:
1. The Ô¨Årst local operation is the processing of the visit map. All parallel processes iterate
independently over their elements and identify the nodes lying at the boundary of the local
subdomain. The inner nodes, which are marked yellow in Figure 4 can be synchronized
by a simple oÔ¨Äset calculation.

Figure 4: Illustration of an initial conÔ¨Åguration. The domain is split up among the green, purple
and blue process. Yellow nodes are visited only locally, red nodes are boundary nodes which
are visited by multiple processes. The tree on the right hand side shows the communication
pattern with blue as master process.
Boundary nodes are duplicated on parallel processes, if they lie on a parallel boundary3 .
In a regular grid nodes are visited once for each adjacent cell. The illustration in Figure 4
shows a two-dimensional grid. Here, inner nodes are visited four times in total. The yellow
3 Nodes

on the domain boundary are not shared, unless we have periodic boundaries.

251

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

dots represent inner nodes or domain boundary nodes which are only visited by the process
they belong to. The red dots are nodes which exist as technical vertex duplicates multiple
times among diÔ¨Äerent processes. If they lie on the border of exactly two neighboring
processes, they are visited two times by one process and two times by another, e. g. the
middle pair of red nodes between the purple and the green process.
The inner and domain boundary nodes are entered into the Ô¨Ånal map. It maps the id of
nodes to sequential numbers. This consecutive numbering is performed on all processes
locally. The local ids are temporary ids which are shifted by an oÔ¨Äset later on. Boundary
nodes are entered into the boundary vertex map which has the same structure as the Ô¨Ånal
map.
2. With the previous step generated a local numeration. This must now be transformed into
a global numbering. All processes send their local total number of nodes and the local
boundary vertex index information to the parental process. In the following we call this
complete block of information subset data.
3. In this step the parent nodes receive the subset data from their children and merge it with
the local data. The number of inner nodes are summed up and stored as count for the new
merged subset information. For all boundary vertex numbers we check whether the vertex
is already known to the process. If it is not known, it is added as a new boundary vertex
to the boundary vertex map. Otherwise, the visit counts are incremented and compared
against the required visit count. If the required visit count is reached, the vertex id is
entered into one of the following two maps:
(a) If the vertex is visited by this (i. e. the receiving master) process as well, it is added
to the Ô¨Ånal map.
(b) Otherwise, it is added to the boundary Ô¨Ånal map which contains sequence numbers
for boundary nodes received by the process but not visited by it. This way the
information can simply be cleaned up again as it is not needed by this process
anymore after the completion of the synchronization process.
This step is illustrated in Figure 5a and Figure 5b. The received boundary vertex numbers
are stored in a map of child numbers. This way we keep track of the vertex numbers and
can send only required data back to the child processes.
4. This is the Ô¨Årst downward communication step: the master vertex sends the merged
information of the previous step to its children, see Figure 6a. The package which is sent
contains the oÔ¨Äset for the inner cells, and for all boundary nodes the corresponding Ô¨Ånal
number. If the child process is a master of further processes again we automatically send
the information for the subsequent processes as part of the message to it.
5. The last step is illustrated in Figure 6b. At this point the packages are received from the
parental nodes. In a local iteration we add to all preliminary numbers contained in the
Ô¨Ånal map and the boundary map the oÔ¨Äset to Ô¨Ånalize them. All boundary vertex numbers
received form the parental vertex are merged into the Ô¨Ånal map and the boundary Ô¨Ånal
map. The other data structures are cleaned up. Now, the Ô¨Ånal map contains the synchronized sequence of numbers for all nodes visited by this process. The vertex numbers
of the boundary nodes are retrieved from the this hash map4 .
4 All hash maps in this context are realized as C++11 hash map data structures granting data access in
O(1) in average.

252

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

(a)

(b)

Figure 5: (a) Visualization of Ô¨Årst upward communication step. Green sends its border vertex
information to blue, thus completing information about blue‚Äôs two upper border nodes. (b) Visualization of second upward communication step. Purple sends its border vertex information
to blue, thus completing the subset vertex data sending process. Blue has all required data, so
it starts assigning Ô¨Ånal numbers to its local nodes, which is depicted by green nodes.

(a)

(b)

Figure 6: (a) Visualization of Ô¨Årst downward communication step. Blue sends a correction
oÔ¨Äset and Ô¨Ånal numbers for border nodes back to purple. Purple takes over the oÔ¨Äset for all of
its previously yellow nodes, and assigns numbers to all previously red nodes. (b) Visualization
of second downward communication step. Final number assignments are done analogously to
the previous step, completing the numbering.

At the end of this procedure, cells and nodes in our parallel adaptive domain have a consecutive
Àú correctly.
numeration starting from 0. With this we are able to setup the matrices M and M
The new routine for performing this action is described in the next section.
With the consecutive numbering of all grid elements generated by this algorithm we are
now able to eÔ¨Éciently assemble the matrix Q for parallel adaptive grids as it is described in
Section 2.

4

Performance

In order to amplify the dominance of the number generation we demonstrate the approaches
capability at a simple reference scenario using the regular grid kernel. The matrix setup and
element numbering is done as described in the previous sections (tree-based). The scenario is a
two-dimensional empty channel with a constant inÔ¨Çow proÔ¨Åle and a resolution of 10000 √ó 10000
cells. The tests were executed on a Xeon-E5-2670 based cluster system, the connection inbetween the dual-sockel nodes is realized via QDR inÔ¨Åniband. Our numbering scheme is a
253

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

integral part of grid initialization and matrix setup. The strong scaling behavior of this stage
our from eight to 256 processes is depicted in Figure 7. The observed scaling behavior is nearly
perfect.

Figure 7: Strong scaling behavior of the grid setup stage.

5

Conclusion

The given numeration generates in a general way a consecutive numbering of grid elements.
The numbering scheme merges elements of adjacent processes along the communication tree,
and thus, avoids the continuous growth of the data packages to be synchronized. The concept
showed very good scaling behavior on a small cluster conÔ¨Åguration. Our method can be directly
applied in all kinds of (adaptive) Cartesian grids for a fast global element or vertex indexing.
Further investigations have to show if the Ô¨Ånal setup stage in the root cell might become a
bottle neck as well. In this case, it is possible to parallelize also the Ô¨Ånal merging steps which
are currently executed on a subset of processes.

6

Acknowledgements

The work presented in this paper has been supported by the International Graduate School
of Science and Engineering (IGSSE) of the Technische Universit¬®at M¬®
unchen. This support is
gratefully acknowledged. Furthermore, this publication is based on work supported by the King
Abdullah University of Science and Technology (KAUST).

References
[1] M. Bader, H.-J. Bungartz, A. Frank, and R.-P. Mundani. Space tree structures for PDE software.
Proc. of the Int. Conf. on Comp. Science (3), 2331:662, 2002.
[2] Satish Balay, Jed Brown, Kris Buschelman, William D. Gropp, Dinesh Kaushik, Matthew G.
Knepley, Lois Curfman McInnes, Barry F. Smith, and Hong Zhang. PETSc Web page, 2013.
http://www.mcs.anl.gov/petsc.

254

EÔ¨Écient Global Indexing

Lieb, Neckel, Sch¬®
ops, Bungartz

[3] H.-J. Bungartz, M. Mehl, T. Neckel, and T. Weinzierl. The PDE framework Peano applied to Ô¨Çuid
dynamics: An eÔ¨Écient implementation of a parallel multiscale Ô¨Çuid dynamics solver on octree-like
adaptive Cartesian grids. Computational Mechanics, 46(1):103‚Äì114, June 2010.
[4] Hans-Joachim Bungartz, Bernhard Gatzhammer, Michael Lieb, Miriam Mehl, and Tobias Neckel.
Towards multi-phase Ô¨Çow simulations in the pde framework peano. Computational Mechanics,
48(3):365‚Äì376, 2011.
[5] Carsten Burstedde, Lucas C. Wilcox, and Omar Ghattas. p4est: Scalable algorithms for parallel
adaptive mesh reÔ¨Ånement on forests of octrees. SIAM J. Sci. Comput., 33(3):1103‚Äì1133, May
2011.
[6] A. J. Chorin. Numerical solution of the Navier-Stokes equations. Math. Comp., 22:745‚Äì762, 1968.
[7] J. Hron and S. Turek. Numerical benchmarking of Ô¨Çuid-structure interaction between elastic
object and laminar incompressible Ô¨Çow. In H.-J. Bungartz and M. Sch¬®
afer, editors, Fluid-Structure
Interaction, Part II, Lecture Notes in Computational Science and Engineering. Springer-Verlag,
2010.
[8] T. Neckel. The PDE Framework Peano: An Environment for EÔ¨Écient Flow Simulations. Verlag
Dr. Hut, 2009.
[9] H. Samet. The quadtree and related hierarchical data structures. ACM Computing Surveys,
16(2):187‚Äì260, 1984.
[10] T. Tu, D. R. O‚ÄôHallaron, and O. Ghattas. Scalable parallel octree meshing for terascale applications. In SC ‚Äô05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing, page 4,
Washington, DC, USA, 2005. IEEE Computer Society.
[11] T. Weinzierl. A Framework for Parallel PDE Solvers on Multiscale Adaptive Cartesian Grids.
Verlag Dr. Hut, 2009.

255

