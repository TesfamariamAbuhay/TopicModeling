Procedia Computer Science
Volume 51, 2015, Pages 2923‚Äì2927
ICCS 2015 International Conference On Computational Science

Helsim: a particle-in-cell simulator
for highly imbalanced particle distributions
Roel Wuyts1,2,3 , Tom Haber1,4 , and Giovanni Lapenta3
1

4

ExaScience Life Lab, Leuven, Belgium
2
imec, Leuven, Belgium
3
KU Leuven, Leuven, Belgium
Universiteit Hasselt, Hasselt, Belgium

Abstract
Helsim is a 3D electro-magnetic particle-in-cell simulator used to simulate the behaviour of
plasma in space. Particle-in-cell simulators track the movement of particles through space,
with the particles generating and being subjected to various Ô¨Åelds (electric, magnetic and or
gravitational). Helsim dissociates the particles data structure from the Ô¨Åelds, allowing them to
be distributed and load- balanced independently and can simulate experiments with highly imbalanced particle distributions with ease. This paper shows weak scaling results of a highly imbalanced particle setup on up to 32 thousand cores. The results validate the basic claims for scalability for imbalanced particle distributions, but also highlights a problem with a workaround
we had to implement to circumvent an OpenMPI bug we encountered.
Keywords: Particle-in-cell, Computational Science, High Performance Computing

1

Introduction

Space weather refers to conditions on the Sun, in the interplanetary space and in the Earth space
environment. These conditions can inÔ¨Çuence the performance and reliability of space-borne and
ground-based technological systems, and can endanger human life or health [5].
Astrophysicists researching space weather study the behavior of plasma, highly energetic
and highly conductive gas where the atoms have been broken into their nuclei and their freely
moving electrons. The small-scale kinetic behaviour of plasma can be described by kinetic
approaches described by the Vlasov equation, self consistently coupled with Maxwell‚Äôs equations [6]. Solving the Vlasov equation is intractable for all but the smallest of simulations. It
is therefore commonly solved using the particle-in-cell (hereafter PIC) method.
In the PIC method, individual macro-particles in a Lagrangian frame, which mimic the
behaviour of the distribution function, are tracked in continuous phase space. Moments of the
distribution function, such as charge densities for plasma physics simulations, are computed
simultaneously on a Eulerian frame (Ô¨Åxed cells).
Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.479

2923

The Helsim simulator

Wuyts, Haber, and Lapenta

The explicit electromagnetic PIC method has Ô¨Åve main steps that are executed in a time-step
loop that runs for a desired number of iterations (See Figure 1):
‚Ä¢ Step 1. The charge density and current are interpolated on the grid from the particles
position and velocity.
‚Ä¢ Step 2. The electric Ô¨Åeld is adjusted to match the Poisson equation (divergence cleaning
using a Poisson solver).
‚Ä¢ Step 3. The electric Ô¨Åeld and magnetic Ô¨Åeld of the next time step are computed using the
discretized Maxwell equations.
‚Ä¢ Step 4. The electric and magnetic Ô¨Åelds are interpolated from the grid to the particle
positions. The same interpolation as in step 1 is used.
‚Ä¢ Step 5. The particle velocities and positions are advanced in time with Newton‚Äôs equations
knowing the electric Ô¨Åeld at the particle positions.
Many PIC implementations opt to have a single data structure that stores both the information regarding Ô¨Åelds (charges, electric, and magnetic Ô¨Åeld information) and the particles
together. As a result, depending on the astrophysical experiment carried out, there may be
cells that end up with far more particles than others. Simulations that involve gravity are one
example. Other examples are simulations that deal with starting conditions where particles are
initially condensed and then explode, for example with laser-induced plasma bubbles such as
those described in [3]. Indeed, computational particle densities can vary by order of magnitude
in time and space and particle density inhomogeneities can have serious impacts on the code‚Äôs
scalability. This is why dynamic load balancing is needed [2, 7, 8].
The solution taken by Helsim is to use entirely separated data structures for the particles
and the Ô¨Åelds. The particles can then be distributed over the cluster independently of the
Ô¨Åelds. Making it possible to use diÔ¨Äerent distributions for particles and Ô¨Åelds enables Helsim
to easily simulate experimental conÔ¨Ågurations with highly imbalanced particle distributions.
These highly imbalanced particle distributions are dynamically distributed across the cluster
according to some load balancing criteria, and hence the processing of the particles is carried
out by all compute nodes of the cluster. Moreover aggregation steps (the grey boxes in Figure
1) are used to locally cache only the part of the grid relevant for the particles on that node.
The advantage of Helsim is that it supports balanced as well as imbalanced distributions of
particles. The price paid for imbalanced distributions is that remote communication may be

1
Deposition

5
Particle
Moving

Field
Aggregation

Field
Aggregation

2
Divergence
Cleaning

3
Field
Updates

4
Interpolation

Figure 1: The 5 main steps in an electromagnetic particle-in-cell method. The grey boxes are
two extra steps in Helsim where local Ô¨Åeld information is globally aggregated.

2924

The Helsim simulator

Wuyts, Haber, and Lapenta

needed in the grid-to-particle and the particle-to-grid operations, because there is no guarantee
that they will be located on the same compute node. While this may seem like a serious
drawback, classic PIC simulations can only deal either very badly with imbalanced particle
distributions (when the particles are processed by only one or few nodes), or not at all (as
soon as the total amount of particles for the few nodes that process them exceeds the memory
capacity of those nodes).
Helsim uses the ExaShark Library1 to store all of its distributed data structures, including
the particles and the various grids. ExaShark is a high-level library for programming highperformance distributed n-dimensional grids. ExaShark manages the bookkeeping and distribution of grid data structures, and oÔ¨Äers speciÔ¨Åc computation and communication operations
to work with the grid data [1]. ExaShark, and as a result Helsim can be conÔ¨Ågured to use only
MPI, only shared memory parallelism, or a combination of both. The advantage for Helsim is
that this choice has no eÔ¨Äect on the code: the abstraction facilities of ExaShark hide most, if
not all, of the diÔ¨Äerences between these technologies. As a result we regularly switch between
purely MPI or hybrid modes depending on the cluster we are running on.

2

Validation

The goal of Helsim is to be able to run PIC simulations with highly imbalanced particle distributions. This drove the main architectural choice in Helsim, namely to dissociate the data
structure that holds the particles and the data structures for the Ô¨Åelds. To validate this decision we ran scalability experiments for a PIC simulation that has a highly imbalanced particle
distribution that is essentially a particle explosion (a much higher density of particles in the
center of the simulation space that spread out during the simulation).
We ran all experiments on the T0 high-performance computer CURIE using the thin nodes
(two eight-core Intel Xeon E5-2680 processors and 64 GB of RAM). Helsim was compiled with
GNU C++ version 4.6.3, and conÔ¨Ågured to use a hybrid back-end combining OpenMP and
BullxMPI 1.1.16.5. Experiments were run using two MPI processes per compute node (each
process pinned to one processor) and running eight threads within each process (with each
thread being pinned to a core).

2.1

OpenMPI problems and work-around

BullxMPI (version 1.1.16.5) was the only available MPI2 implementation on Curie. This proved
problematic: while running experiments we experienced crashes. Looking into this we found
that BullxMPI inherits a bug from OpenMPI, on which it is based: when using one-sided
communication with user-deÔ¨Åned datatypes OpenMPI crashes. We reproduced this problem
on various other systems and submitted a bug report through the OpenMPI bug tracker. To
run our experiments on Curie we were therefore forced to implement a quick work-around,
which unfortunately resulted in resorting to an MPI all-to-all communication for the two main
communication steps in Helsim: the charge aggregation step and the electric Ô¨Åeld aggregation
step. As we will see in the performance results this had a huge eÔ¨Äect on performance when
scaling up to thousands of nodes.
2925

The Helsim simulator

2.2

Wuyts, Haber, and Lapenta

Weak Scaling Results

In a weak scaling experiment the number of computational resources are varied for a Ô¨Åxed
problem size per resource. Ideally the runtime should then remain constant, meaning that
ever larger experiments could be run by increasing the computational resources with the same
amount. We decided to always double the problem size in each dimension (so we could keep
on using a cube as domain so as not to disturb the simulation) while therefore using 23 = 8
times more cores. We used 432 particles per cell (remember that with Helsim particles can
be distributed over the cluster even if they are concentrated in the simulation space).The size
of the initial, smallest, grid was 643 , which we simulated on 4 nodes (in total using 64 cores).
The following grids were 643 (on 32 nodes, using 512 threads), 1283 (on 256 nodes, using 4096
threads), and Ô¨Ånally 2563 (on 4096 nodes, using 32,768 threads). This last experiment uses
nearly half of all of the available thin nodes on Curie.
Figure 2 shows the results for the weak scalability experiments. On the positive side it
shows that Helsim successfully runs on up to 32k cores, handling an experiment that is highly
imbalanced. It also shows that weak scalability is relatively good when we go from 64 over 512
to 4096 cores. Moreover it shows that the computational intensive steps on particles when using
imbalanced particle distributions (step 1 and 5, or charge interpolation and particle moving in
the Ô¨Ågure) are scaling well. This validates part of our claim that we support such imbalanced
particle distributions and run successfully.
On the negative side we see a very large increase (about a factor of 4,5) going from 4096
to 32 thousands cores. To investigate this problem Figure 2 also shows the breakdown in the
six diÔ¨Äerent phases in a Helsim simulation2 . This clearly shows that the two aggregation steps
scale very badly. This is likely due to our workaround for the OpenMPI/BullxMPI problem (see
Section 2.1), since an MPI all-to-all communication can not scale well on 4096 nodes. However
it could also (partially) be due to our algorithmic choices of dissociating the grids from the
particles. Given the scaling up to 4096 nodes we are however hopeful that our work-around is
the main cause for the scalability loss. We are therefore in the process of running experiments
on a cluster that does not exhibit the OpenMPI bug. We are also integrating the state-of-the-art
pipelined CG solvers developed at the lab [4] to improve the divergence cleaning step.
To summarise the weak scaling experiments we can say that our expectations were partially
fulÔ¨Ålled: we successfully ran on up to 32k cores with a highly imbalanced particle distribution.
However, due to the work-around we needed for the OpenMPI problem, we were not yet able

	


	























 




  
  



  




Figure 2: Weak scaling results on the Curie system for the overall Helsim simulation, with the
breakdown per phase
1 Freely

available under its previous name, Shark, on github: https://github.com/ExaScience/shark
correspond to the general PIC steps outlined in section 1 with the addition of a charge aggregation
step before the divergence cleaning.
2 they

2926

The Helsim simulator

Wuyts, Haber, and Lapenta

to prove beyond doubt that we have good scalability on truly large-scale simulations.

3

Conclusion

This paper describes Helsim, a 3D electromagnetic particle-in-cell simulation with in-situ visualization. It gives an architectural overview of Helsim, focusing on its major design choice of
dissociating the data structure for storing the particles from the data structures for storing the
grids. This should enable Helsim to simulate PIC experiments with highly imbalanced particle
distributions with ease. We have implemented Helsim and show weak scalability results simulating a problem with a highly imbalanced particle setup on the T0 supercomputer Curie. The
results show that we can run on up to 32 thousands cores but due to a problem with OpenMPI,
for which we had to implement a work-around, weak scalability was not so good as we hoped.
We therefore plan to rerun experiments on a cluster with an MPI implementation that does
not exhibit this problem (either an updated version of OpenMPI/BullxMPI or an alternative
implementation like MVAPICH2) and then use Helsim for magnetic reconnection simulations.
Acknowledgements The authors acknowledge useful discussions with Pierre Henri, Stefano Markidis and Bart Verleye on PIC codes. This work is funded by the Institute for the
Promotion of Innovation through Science and Technology in Flanders (IWT), and by Intel.
Scaling experiments were run on the T0 system Curie TN, GENCI@CEA, France under a
PRACE Preparatory Access grant with number 2010PA1716.

References
[1] Imen Chakroun, Tom Vander Aa, Bruno De Fraine, Pascal Costanza, Tom Haber, Roel Wuyts, and
Wolfgang Demeuter. Exashark: A scalable hybrid array kit for exascale simulation. In 23rd High
Performance Computing Symposium (HPC 2015), April 2015 (conditionally accepted).
[2] R.D. Ferraro, P.C. Liewer, and V.K. Decyk. Dynamic load balancing for a 2d concurrent plasma
pic code. Journal of Computational Physics, 109(2):329‚Äì341, December 1993.
[3] W. Fox, A. Bhattacharjee, and K. Germaschewski. Fast magnetic reconnection in laser-produced
plasma bubbles. Phys. Rev. Lett., 106:215003, May 2011.
[4] Pieter Ghysels and Wim Vanroose. Hiding global synchronization latency in the preconditioned
conjugate gradient algorithm. Parallel Computing, 40(7):224‚Äì238, 2014.
[5] J. Kappenman. Geomagnetic storms and their impacts on the u.s. power grid. Technical Report
Meta-R-319, Metatech Corporation, January 2010.
[6] N.A. Krall and A.W. Trivelpiece. Principles of plasma physics. International Series in Pure and
Applied Physics. McGraw-Hill, 1973.
[7] Steven J. Plimpton, David B. Seidel, Michael F. Pasik, Rebecca S. Coats, and Gary R. Montry.
A load-balancing algorithm for a parallel electromagnetic particle-in-cell code. Computer Physics
Communications, 152(3):227 ‚Äì 241, 2003.
[8] B. Verleye, P. Henri, R. Wuyts, G. Lapenta, and K. Meerbergen. Implementation of a 2d electrostatic particle-in-cell algorithm in uniÔ¨Åed parallel c with dynamic load-balancing. Computers &
Fluids, 80(0):10 ‚Äì 16, 2013. Selected contributions of the 23rd International Conference on Parallel
Fluid Dynamics ParCFD2011.

2927

