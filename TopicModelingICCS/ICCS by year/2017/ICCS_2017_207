Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it
Science
(2017) 1793â€“1802
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Parallel Modularity Clustering
Parallel Modularity Clustering
1,2,3
2,4
Alexandre Fender Parallel
, Nahid Modularity
Emad2,3 , Serge Petiton
, and Maxim Naumov1
Clustering
1,2,3
2,3
2,4
1
Alexandre Fender
, Nahid Emad , Serge Petiton , and Maxim Naumov
1
Nvidia Corporation,
USA
1,2,3
2,3
2,4
Alexandre Fender
, Nahid
, Serge Petiton
, and Maxim Naumov1
2
1 Emad
Maison
de Corporation,
la Simulation,
France
Nvidia
USA
4
4
4

3
2
LI-PaRAD,
University
of Versailles,
France
Maison
de la Simulation,
France
1
Nvidia Corporation, USA
3
University
I, Sciencesof&Versailles,
Technologies,
France
LI-PaRAD,
University
France
2 of Lille
Maison de la Simulation, France
University
of
Lille
I,
Sciences
&
Technologies,
France
3
LI-PaRAD, University of Versailles, France
University of Lille I, Sciences & Technologies, France

Abstract
Abstract
In this paper we develop a parallel approach for computing the modularity clustering often
In
this
weand
develop
a parallel
approach
for computing
used
to paper
identify
analyse
communities
in social
networks. the
Wemodularity
show that clustering
modularityoften
can
Abstract
used
to identify and
analyse at
communities
in
social networks.
We show
thatadjacency
modularity
can
be
approximated
by
looking
the
largest
eigenpairs
of
the
weighted
graph
matrix
In this paper we develop a parallel approach for computing the modularity clustering often
be
approximated
by looking
at
theone
largest
eigenpairs
of generalize
the weighted
graph
adjacency
matrix
that
has
been
perturbed
by
a
rank
update.
Also,
we
this
formulation
to
identify
used to identify and analyse communities in social networks. We show that modularity can
that
has clusters
been perturbed
by
rank onea update.
Also,implementation
we generalize this
formulation
identify
multiple
at
Weaat
develop
fast eigenpairs
parallel
forgraph
it
thatadjacency
takes to
advantage
be approximated
byonce.
looking
the largest
of the weighted
matrix
multiple
clusters
at once. We
develop
a fast parallel
implementation
forFinally,
it that we
takes
advantage
of
the
Lanczos
eigenvalue
solver
and
k-means
algorithm
on
the
GPU.
highlight
the
that has been perturbed by a rank one update. Also, we generalize this formulation to identify
of
the Lanczos
eigenvalue
solver
and k-means
algorithm
on
the GPU. Finally,
we highlight the
performance
and
quality
of
our
approach
versus
existing
state-of-the-art
techniques.
multiple clusters at once. We develop a fast parallel implementation for it that takes advantage
performance and quality of our approach versus existing state-of-the-art techniques.
of2017
the The
Lanczos
eigenvalue
solver
and
k-means
algorithm
on thecommunity
GPU. Finally,
we highlight
the
Keywords:
modularity,
assortativity
coefficient,
spectral,
clustering,
detection,
graphs, parÂ©
Authors.
Published
by Elsevier
B.V.
Peer-review
under
responsibility
the scientific
committee
ofexisting
theclustering,
International
Conferencetechniques.
on
Computational
Science
Keywords:
modularity,
assortativity
coefficient,
spectral,
community
detection,
graphs,
parperformance
and
quality
ofofour
approach
versus
state-of-the-art
allel
algorithms,
Lanczos,
k-means,
CUDA,
GPU
allel algorithms, Lanczos, k-means, CUDA, GPU
Keywords: modularity, assortativity coefficient, spectral, clustering, community detection, graphs, parallel algorithms, Lanczos, k-means, CUDA, GPU

1 Introduction
1 Introduction
The graph clustering technique can often be used to identify and analyse communities in social
1
Introduction
The
graph
clustering
technique
can often beThe
usedgraph
to identify
and analyse
communities
in social
networks
among
many
other applications.
is usually
split into
clusters based
on a
networks
among many
other
applications.
The graph
is usually
split
intoorclusters
based
on
particular
such
as
infomap
19], be
minimum
cut analyse
[12, 13]
modularity
17].a
The graph metric,
clustering
technique
can[6,
often
used to balanced
identify and
communities
in[5,
social
particular
metric,
such
as
infomap
[6,
19],
minimum
balanced
cut
[12,
13]
or
modularity
[5,
17].
In thisamong
paper we
focus
on the
latter metric
that
measures
how well
given
clustering
applies
networks
many
other
applications.
The
graph
is usually
splitainto
clusters
based
on a
In
this
paper
we
focus
on
the
latter
metric
that
measures
how
well
a
given
clustering
applies
to
a particular
graph
a random
graph
[15, 16].balanced
We notecut
that
modularity
is an important
particular
metric,
suchversus
as infomap
[6, 19],
minimum
[12,
13] or modularity
[5, 17].
to
a particular
graph
versusinapractice
random to
graph
[15,
16]. Wediseases
note that
modularity
isAlso
an important
metric
and
has
been
used
study
different
epidemics
[15].
weapplies
point
In this paper we focus on the latter metric that measures how well a given clustering
metric
and
has
beenisused
in practice
tothe
study
different diseases
epidemics
[15].asAlso
we point
out
that
this
metric
closely
related
to
assortativity
coefficient
[14]
as
well
the
algebraic
to a particular graph versus a random graph [15, 16]. We note that modularity is an important
out
that this of
metric
is closely
related
to the assortativity
coefficient
[14]review
as wellgiven
as the algebraic
connectivity
graphs
[7, 8],
a comprehensive
scientific
literature
[17,point
21].
metric and has
been used
in with
practice
to study different
diseases
epidemics
[15]. Alsoinwe
connectivity
of graphs
[7, 8], with
a comprehensive
scientific
literature review
given
in [17, 21].
As
the
main
contributions
of
this
paper
we
extended
the
modularity
theory
by
providing
an
out that this metric is closely related to the assortativity coefficient [14] as well as the algebraic
As theformulation
main contributions
of this
paper
we extended
the identifies
modularity
theory clusters
by providing
an
algebraic
that
works
with
weighted
graphs
and
multiple
at
once.
connectivity of graphs [7, 8], with a comprehensive scientific literature review given in [17, 21].
algebraic
formulation
that
worksimplementation
with weighted graphs
and identifies
multiple
clusters
at once.
Also,
we
develop
a
novel
parallel
of
the
algorithm
on
the
GPU
that
outperforms
As the main contributions of this paper we extended the modularity theory by providing an
Also,
we
develop a novelapproaches.
parallel implementation of the algorithm on the GPU that outperforms
earlier
state-of-the-art
algebraic
formulation that works with weighted graphs and identifies multiple clusters at once.
earlier
state-of-the-art
approaches.
First,
we
develop
an
algebraic
formaulation and
show
how toon
setup
an eigenvalue
problem
Also, we develop a novel parallel
implementation
of the
algorithm
the GPU
that outperforms
First,
weuse
develop
an algebraic
formaulation
andthe
show
how to setup
an eigenvalue
problem
and
how
to
the
k-means
algorithm
to
transform
eigenvectors
corresponding
to
its
largest
earlier state-of-the-art approaches.
and
how to use
the
k-meansassignment
algorithm to
transform
the
eigenvectors
corresponding
toframework
its largest
eigenvalues
into
a
discrete
of
nodes
into
clusters.
We
closely
follow
the
First, we develop an algebraic formaulation and show how to setup an eigenvalue problem
eigenvalues
into a discrete
assignment
of nodes into clusters.
Weallows
closelyusfollow
themany
framework
developed
clustering
and partitioning
[13],
which
to reuse
of its
and how tofor
usespectral
the k-means
algorithm
to transforminthe
eigenvectors
corresponding
to its largest
developed for spectral clustering and partitioning in [13], which allows us to reuse many of its
eigenvalues into a discrete assignment of nodes into clusters. We closely follow the framework
1
developed for spectral clustering and partitioning in [13], which allows us to reuse many of its
1
1877-0509 Â© 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.198

1

1794	

Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)
Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov

features. We note that in our approach the number of fixed clusters p is arbitrary. It does not
need to be a power of two, i.e. p = 2k , as when repeated bisection is used k times.
Also, we analyse the effects of using single and double precision to solve the problem. The
latter some times being faster, but requiring more iterations to convergence. Moreover, we
outline how the proposed approach could be modified to find an adaptive number of clusters.
In particular, we show that the clustering information could be derived from the smaller, same
or larger number of eigenvectors, with the former case often exchanging lower quality for higher
performance.
Finally, in our experiments we compare the clustering obtained by the modularity approach
developed in this paper to previous work. We comment on the quality and performance tradeoffs
when they are applied to large social network graphs that often have power law-like distribution
of edges per node. Also, we highlight the performance obtained by our novel parallel approach
on the GPU. For example, it can find 7 clusters with a modularity score over 0.5 in about 0.8
seconds for hollywood-2009 network with over a hundred million undirected edges.

2

Graph Clustering

Let a graph G = (V, E) be defined by its vertex V and edge E sets. The vertex set V = {1, ..., n}
represents n nodes in a graph, with each node identified by a unique integer number i âˆˆ V . The
edge set E = {wi1 ,j1 , ..., wim ,jm } represents m weighted edges in a graph, with each positive
wi,j â‰¥ 0 undirected edge identified by wi,j âˆˆ E.
Let the weighted adjacency matrix A = [ai,j ] of a graph G = (V, E) be defined through its
elements ai,j = wi,j if there is an edge connecting i to j, and 0 otherwise. Notice that matrix
A is symmetric because graph is assumed to be undirected and therefore wi,j â‰¡ wj,i . Also,
assume that we do not include self-edges, diagonal elements, in the definition of the weighted
adjacency matrix A.
In graph clustering we
p are often interested in finding a partitioning of vertices V into disjoint
sets Sk âŠ† V such that k=1 Sk = V . Notice that we can equivalently express this partitioning
as a function c(i) = k specifying the assignment of nodes i âˆˆ V into clusters k = 1, ..., p.
In the following discussion, let |.| denote cardinality (number of elements) of a set and di
denote the
degree (number of edges) of the vertex i âˆˆ V . Also, let us define the volume of a
n
node vi = j=1 ai,j and volume of a set of vertices
vol(V ) =

n

i=1

vi =

n 
n


ai,j = 2Ï‰

(1)

i=1 j=1

Notice that for unweighted graphs ai,j = 1 and therefore vi = di and 2Ï‰ = 2m.

3

Modularity

An intuitive way to identify structure in a graph is to assume that similar vertices are connected
by more edges in the current graph than if they were randomly connected. The modularity
measures the difference between how well vertices are assigned into clusters for the current
graph G = (V, E), when compared to a random graph R = (V, F ) [16, 17].
The reference random graph R = (V, F ) is constructed with the same set of vertices, but
a different set of edges as the current graph. The set of edges F of the random graph is
2

	

Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)
Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov

constructed such that the number of edges |F | = |E| = m and degree di of each vertex is the
same, but the edges themselves are rewired randomly between vertices in V .
Notice that every broken edge, generates two edge ends that are available for rewiring. Then,
the weighted probability of a particular edge end to be connected with some edge end at node
i is vi /2Ï‰. Therefore, the probability of node i and j to be connected during the rewiring is
(vi vj )/2Ï‰.
The modularity is the difference between existing edges and the probabilities of edges in
random graph across all nodes that belong to a given set of clusters.
Definition 1. Let graph G = (V, E) and c(i) be an assignment of nodes into clusters. Then,
modularity Q can be expressed as
Q=

n
n
1 
vi vj 
Î´c(i),c(j)
ai,j âˆ’
2Ï‰ i=1 j=1
2Ï‰

where

Î´c(i),c(j) =



1 if c(i) = c(j)
0 otherwise

(2)

The above definition can be reduced to the special case in [16, 17] if we choose to ignore the
vi vj
di dj
edge weights during rewiring or work with unweighted graphs, in which case 2Ï‰
= 2m
.
Lemma 1. The modularity Q is bounded, as shown in [5],
âˆ’

1
â‰¤Qâ‰¤1
2

(3)

Let us now define the modularity matrix, state its properties and show its relationship to
modularity metric.
Definition 2. Let volume vector vT = [v1 , ..., vn ], then the modularity matrix can be written as
B =Aâˆ’

1
vvT
2Ï‰

(4)

Lemma 2. The modularity and adjacency matrices have the following properties
Be = 0,

Ae = v,

vT e = eT Ae = 2Ï‰

(5)

where e = [1, ..., 1]T .
T

Proof. The latter two follow from (1). The former follows from Be = Ae âˆ’ ( v2Ï‰e )v = 0.
Notice that the modularity matrix B is symmetric indefinite. Also, using Lemma 2 we may
conclude that it is singular, with an eigenvalue 0 and corresponding eigenvector e = [1, ..., 1]T .
Let us now define a tall matrix U = [ui,k ], that can be interpreted as a set of vectors
U = [u1 , ..., up ] where each vector uk corresponds to a cluster Sk for k = 1, ..., p, with elements
ui,k = 1 if c(i) = k and 0 otherwise.
Theorem 1. Let the matrix U = [u1 , ..., up ] be specified above. Then,
Q=

1
Tr(U T BU )
2Ï‰

(6)
3

1795

1796	

Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)
Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov

Proof. Notice that
Q

=

p
1  
2Ï‰

 

k=1 âˆ€i s.t. âˆ€j s.t.
c(i)=k c(j)=k

ai,j

p

1  T
vi vj 
1
=
Tr(U T BU )
âˆ’
uk Buk =
2Ï‰
2Ï‰
2Ï‰

(7)

k=1

with elements of U constrained to be in set C = {0, 1}.
Notice that ultimately we are interested in finding the cluster assignment c that achieves
the maximum modularity
1
max Q =
max Tr(U T BU )
(8)
c
2Ï‰ U âˆˆC
The exact solution to the modularity maximization problem stated in (8) is NP-complete
[5]. However, we can find an approximation by relaxing the requirement that elements of matrix
U take discrete values [12, 13].
Notice that U T U = D, where D = [dk,k ] is a pÃ—p diagonal matrix with elements dk,k = |Sk |.
 = U Dâˆ’1/2 âˆˆ RnÃ—p , we can start by looking for
Then, introducing auxiliary matrix U
 T BU
)
max Tr(U

T U
 =I
U

(9)

Notice that by the Courant-Fischer theorem [9] this maximum is achieved by the largest
eigenpairs of the modularity matrix. Now, we still need to convert the real values obtained in
(9) back into the discrete assignment into clusters.
Since we are working in multiple dimensions, it is natural to use the distance between points
as a metric of how to group them. In this case, if we interpret each row of the matrix U as a
point in a p-dimensional space then it becomes natural to use a clustering algorithm, such as
k-means [1, 11] to identify the p distinct partitions. We are not aware of a theoretical result
guaranteeing that the obtained approximate solution will closely match the optimal discrete
solution, but in practice we often do obtain a good approximation.

4

Algorithm

The outline of the modularity clustering technique is described in Alg. 1.
Algorithm 1 Modularity Clustering
1:
2:
3:
4:
5:
6:

Let G = (V, E) be an input graph and A be its weighted adjacency matrix.
Let p be the number of desired clusters.
1
Set the modularity matrix B = A âˆ’ 2Ï‰
vvT .
Find p largest eigenpairs BU = U Î£, where Î£ = diag(Î»1 , ..., Î»p ).
Scale eigenvectors U by row or by column (optional).
Run clustering algorithm, such as k-means, on points defined by rows of U .

Notice that the general outline of the modularity clustering closely resembles the spectral
clustering in [13]. The main difference is that in the former case we use the modularity matrix
B and find its largest eigenpairs, while in the latter case we use the Laplacian matrix L and find
its smallest eigenpairs. The properties of modularity and Laplacian matrices are also different,
requiring a different choice of eigenvalue problem solvers.
4

	

Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)
Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov

4.1

Eigenvalue Problem

In our numerical experiments we have found that the eigenvalue solver is the most critical part
of the algorithm because the accuracy of the solution of the eigenvalue problem has a significant
impact on the quality of the obtained clustering. The insufficient accuracy or a failure of the
method usually results in poor approximation to the original discrete problem or even a failure
of the entire algorithm, respectively.
The eigenvalues solver is also the most time consuming part of the computation, as shown
in Fig. 1a. Therefore, we chose to use the implicitly restarted Lanczos method [3, 20], which
is one of the most efficient eigenvalue solvers for finding the largest eigenvalues of symmetric
problems. Notice that the most time consuming part of Lanczos is the sparse matrix-vector
multiplication (csrmv) with remaining time consumed by BLAS operations [23], as shown in
Fig. 1b.

(a) Profiling of the modularity algorithm

(b) Profiling of the Lanczos eigensolver

Figure 1: Profiles

4.2

Clustering Problem

Let us now find an approximation to the discrete problem (8) based on the solution of the realvalued optimization problem (9). Let us interpret each row of U as a point xi in p-dimensional
space. Then, we
can find
 sets Sk for k = 1, ..., p, each with a centroid (point in the center) yk ,
p
such that minSk k=1 iâˆˆSk ||xi âˆ’yk ||22 . The exact solution of this problem is NP-complete, but
we can find an approximation to it using many variations of the k-means clustering algorithm
[1, 11], which will define the assignment of nodes into clusters.
Notice that the number of partitions identified by the clustering algorithm does not necessarily need to match the number of computed eigenvectors. In fact it can be chosen adaptively
based on modularity score [22] or x-means algorithm [18].

4.3

Parallelism and Energy Efficiency

The performance and scalability are often the primary concerns for a graph clustering algorithm
because the analysis of the graph structure is usually on the critical path of complex data
analytics. The modularity approach we propose is limited by memory bandwidth, which is
higher on the GPU than on the CPU. Therefore, taking advantage of the parallelism available
on the GPU is critical for the successful application of the algorithm in practice.
In our implementation all building blocks in Alg. 1, including Lanczos and k-means, are
implemented on the GPU. Also, all data structures, including the adjacency matrix A, are
stored in the GPU memory. The action of the matrix B on a vector is computed implicitly
using sparse matrix-vector multiplication (csrmv) with A and rank-one update with v.
We can expect that using GPU would also be advantageous from the energy efficiency
perspective. While we do not measure the energy consumed by the algorithm directly, in broad
5

1797

1798	

Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov
Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)

terms we can relate it to the difference in TDP1 and time consumed by the algorithm on
different hardware platforms. For instance, in the next section we will perform experiments on
Nvidia Titan X (Pascal) GPU and Intel Core i7-3930K CPU with 250 and 130 Watts TDP,
respectively. Also, we will show that our algorithm on the GPU outperforms the state-of-the-art
implementation on the CPU by âˆ¼ 3Ã— on average. Since the ratio between the speedup and
ratio of TDP (250/130 âˆ¼ 2Ã—) on these platforms is 3/2 > 1, we can in general expect to achieve
a better power efficiency on the GPU.

5

Numerical Experiments

Let us now study the performance and quality of the clustering obtained by the proposed
modularity algorithm Alg. 1 on a relevant sample of graphs from the DIMACS10, LAW and SNAP
graph collections [24], shown in Tab. 1.
In the modularity algorithm, we let the stopping criteria for the Lanczos solver be based on
the norm of the residual of the largest eigenpair ||r1 ||2 = ||Bu1 âˆ’ Î»1 u1 ||2 â‰¤ 10âˆ’3 and maximum
# of iterations 800 (with restart at every 20 iterations), while for the k-means we let it be based
on the scaled error difference |l âˆ’ lâˆ’1 |/n < 10âˆ’2 and maximum # of iterations 20.
Also, all numerical experiments are performed on a workstation with Ubuntu 14.04 operating
system, gcc 4.8.4 compiler, CUDA Toolkit 8.0 software and Intel Core i7-3930K CPU 3.2 GHz
and Nvidia Titan X (Pascal) GPU hardware. The performance of the algorithms was always
measured across multiple runs to ensure consistency.

5.1

Clustering and Effects of Precision

First, let the number of cluster into which we would like to partition the graph be fixed. For
instance, suppose that we have decided to partition the graph into 7 clusters. We show the
corresponding time, number of iterations and modularity score in Tab. 1.
Matrix
Name
preferentialA...
caidaRouterLevel
coAuthorsDBLP
citationCiteseer
coPapersDBLP
coPapersCiteseer
as-Skitter
hollywood-2009

n = |V|
100,000
192,244
299,067
268,495
540,486
434,102
1,696,415
1,139,905

m = |E|
499,985
609,066
977,676
1,156,647
15,245,729
16,036,720
22,190,596
113,891,327

Rand
0.00006
-0.00008
-0.00042
-0.00011
0.00000
-0.00001
-0.00002
-0.00136

64 bits
Mod
0.147
0.397
0.392
0.417
0.326
0.319
0.407
0.544

32 bits
T
It Mod
82
92 0.108
74
92 0.233
62
44 0.297
108 81 0.417
318 80 0.201
168 56 0.092
1104 104 0.223
796 69 0.187

T
80
59
45
64
514
1206
2001
973

It
140
141
81
92
188
681
230
116

Table 1: The modularity (Mod), time (T) in milliseconds and # of iterations (It) achieved for
64 and 32 bit precision, when splitting the graph into 7 clusters. The column (Rand) contains
the modularity score resulting of random cluster assignments
First, notice that the modularity algorithm is robust and converged to the solution on all
networks of interest. Also, notice that the computed modularity score remained in the interval
[âˆ’0.5, 1] as predicted by the theory for all the problems. Moreover, the modularity score
1 Thermal Design Power (TDP) measures the average power a processor dissipates when operating with all
cores active. The real energy usage may be different and may change depending on the hardware generation.

6

	

Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov
Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)

computed by random assignment of nodes into clusters was 0 for all the networks as expected.
It is an important baseline for comparing attained modularity scores.
Second, notice the difference in behaviour of the algorithm when the computation is performed using single (32 bit) and double (64 bit) floating point arithmetic. In particular, notice
that the total time to the solution can be significantly better in 64 bit than in 32 bit precision as
shown in the time column of Tab. 1. Indeed, single precision can result in unwanted perturbations during the computation of the Krylov subspace by the Lanczos eigenvalue solver. Those
perturbations can impact the number of iterations and the overall quality of the approximation.
Therefore, we have found that using 64 bit precision is a safer option.

5.2

Adaptive Clustering

In the previous experiments we have kept the number of eigenpairs and k-means clusters the
same as suggested by the theory developed in earlier sections. Next, we investigate what
happens when we decouple these parameters. On one hand, we would expect that by selecting
more k-means clusters than eigenpairs we would trade lower quality for higher performance. On
the other hand, we could interpret selecting less k-means clusters than eigenpairs as filtering
the noise in the data and perhaps obtaining a better solution.
In our experiments we have indeed found that for most networks it is possible to maximize
the modularity by varying the number of k-means clusters independently of the number of
eigenpairs as shown in Fig. 2a. In this experiments we have computed 2 to 8 eigenpairs, and
afterwards we have continued to increase only the number of k-means clusters. Notice that the
plot demonstrates how the choice of the number of eigenpairs impacts the modularity score.
Based on the formulation of the modularity problem one can expect that the best case scenario
would be to compute a number of eigenpairs equal to the number of clusters. This is mostly,
but not always, true, with the exceptions often due to loss of orthogonality or low quality
approximation of the latest eigenvectors.
We also see that it is possible to compute less eigenpairs for an equivalent clustering quality
as shown in Fig. 2a. Indeed, modularity values seem to follow a trend set by the number of
clusters and the selected number of eigenpairs is secondary. Hence, given a constant number
of eigenpairs it is possible to maximize modularity by only increasing the number of k-means
clusters.

(a) Comparing the impact of varying the # of
clusters used for assignment for different # of
computed eigenvectors

(b) The modularity achieved when changing the
# of clusters for citationCiteseer network in 64
bit precision

Figure 2: Varying the # of clusters
7

1799

1800	

Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov
Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)

The above experiments lead us to propose the following method for discovering an approximation to the natural number of clusters, which has also been proposed for small networks in
[22]. We propose computing as many eigenpairs as clusters up to a fixed point, such as 7, and
afterwards continuing to increase the number of k-means clusters only, while keeping track of
modularity score as shown on Fig. 2b. Since the plotted modularity score curve has a Gaussian
shape it is straight forward to detect that its maximum is at 17 clusters on x-axis. A similar
trend can be seen for several other networks in our experiments. Moreover, we also found that
it is better to over than under estimate the number of clusters.
Also, notice on Fig. 2b that when we increase the number of clusters by 10Ã— from 2 to 20
the time to compute them only increases by about 20% from 95 ms to 120 ms. The plotted time
line has a very low slope with respect to x-axis, because the number of computed eigenpairs
does not increase past 7 in this experiment. Hence, the time growth shown on the figure only
reflects additional time spent in the k-means step.
Using this technique we were able to detect the best clustering for all of the networks in
Tab. 1. The resulting number of clusters and the modularity score found by our method are
shown in Tab. 2.
In general, our algorithm can very quickly compute modularity for many clusters with only
limited memory requirements. For example, we computed 53 clusters in half a second for
coPapersCiteseer network with 16 million edges. Also, it takes only 0.8 seconds to find a
clustering with a modularity score over 0.5 for hollywood-2009 which has 1, 139, 905 vertices
and 113, 891, 327 edges.

5.3

Related Work

The earlier work on modularity was based on an implementation of a greedy algorithm on the
GPU [2]. The performance of our approach versus it is plotted in Fig. 3a. Our implementation
achieves speedups of up to 8Ã— compared to previous results, even when compensating for
bandwidth difference of up to âˆ¼ 3Ã— due to difference in the hardware resources 2 . Also our
approach allows us to handle networks with up to hundred million edges in less than a second.
However, as a tradeoff in some large cases our approach obtains a lower modularity score, see
Tab. 2a.
Matrix
preferentialAtt...
caidaRouterLevel
coAuthorsDBLP
citationCiteseer
coPapersDBLP
coPapersCiteseer

Clu
7
11
7
17
73
53

Mod
0.147
0.397
0.392
0.506
0.540
0.636

(a) Large data sets in [2]

RMod
0.214
0.768
0.748
0.643
0.640
0.746

Matrix
karate
dolphins
lesmis
adjnoun
polbooks
football

Clu
3
5
11
5
3
7

Mod
0.390
0.509
0.250
0.255
0.504
0.575

RMod
0.363
0.453
0.444
0.247
0.437
0.412

(b) Small data sets in [2]

Table 2: Modularity (Mod) for a given # of clusters (Clu) vs. reference results (Rmod) in [2]
On the other hand, for the small cases the modularity algorithm we propose often attains a
better modularity score than the reference results in [2]. In fact its score is better in 5 out of 6
2 We do not have access to the corresponding code and are forced to make the comparisons with the results
obtained on Tesla C2075 GPU in [2]. Since in both algorithms the execution time is limited by memory
bandwidth, we estimate a factor of âˆ¼ 3Ã— as baseline performance difference between the Tesla C2075 with
144GB/s and TitanX with 337GB/s bandwidth.

8

	

Parallel Modularity Clustering
A. Fender,
Emad,
S. 1793â€“1802
Petiton and M. Naumov
Alexandre Fender et al. / Procedia Computer
ScienceN.108C
(2017)

(a) Large data sets on GPU in [2]

(b) Large data sets on CPU in [10]

Figure 3: The speedup and relative quality when compared to the reference results

cases considered in the study, as shown in Tab. 2b. Since we implement different algorithms for
computing modularity, it is not completely surprising that their behavior varies on different data
sets. Unfortunately, we could not identify any particular trends that would tell us when one
algorithm would be better than the other in terms of quality. However, we always outperform
the reference approach on large cases.
Another more recent work on modularity developed a hierarchical algorithm for computing
it on the CPU [10]. We have experimented with this algorithm by computing 7 clusters in 64 bit
precision and using all CPU cores available on the machine. The performance of our approach
versus these results is plotted in Fig. 3b. Notice that on average our algorithm outperforms
the hierarchical approach by about âˆ¼ 3Ã—, but it has the same quality tradeoffs.

6

Conclusion and Future Work

In this paper we extended the modularity theory by developing an algebraic formulation that
works with weighted graphs and identifies multiple clusters at once. Also, we have implemented
a parallel variation of the technique that computes multiple eigenpairs with implicitly restarted
Lanczos algorithm and perform a multidimensional k-means clustering on the obtained eigenvectors on the GPU.
This approach allowed us to achieve speedups of up to 8Ã— compared to previous state-ofthe-art results, even when compensating for bandwidth difference of up to 3Ã— due to difference
in the hardware resources. It also allowed us to handle networks with up to hundred million
edges in less than a second.
In our experiments on real networks we have shown that modularity benefits from more
accurate and stable approximation of the eigenpairs, often requiring use of 64 bit precision
floating point arithmetic. Also, we have shown that to compute l clusters for the original graph
more quickly, at the expense of lower quality, we may use l k-means centroids, while computing
only k  l eigenvectors. Moreover, we have used this observation to develop a technique to
detect and adaptively select the natural number of clusters in a graph.
In the future, we plan to investigate the impact of different graph features as well as eigensolver parameters on modularity computation.
9

1801

1802	

Parallel Modularity Clustering
A. Fender,
N.108C
Emad,
S. Petiton
and M. Naumov
Alexandre Fender et al. / Procedia Computer
Science
(2017)
1793â€“1802

7

Acknowledgements

The authors would like to acknowledge Steven Dalton, Joe Eaton, Alex Fit-Florea and Michael
Garland for their useful comments and suggestions.

References
[1] D. Arthur and S. Vassilvitskii, K-means++: The Advantages of Careful Seeding, Proc. 18th Annual
ACM-SIAM Symposium on Discrete algorithms, pp. 1027-1035, 2007.
[2] B. O. F. Auer, GPU Acceleration of Graph Matching, Clustering and Partitioning, Ph.D. Thesis,
Utrecht University, 2013.
[3] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe and H. van der Vorst, Templates for the solution of
Algebraic Eigenvalue Problems: A Practical Guide, SIAM, Philadelphia, PA, 2000.
[4] N. Bell and M. Garland, Implementing Sparse Matrix-Vector Multiplication on ThroughputOriented Processors, Proc. SC09, 2009.
[5] U. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski and D. Wagner, On
Modularity Clustering, IEEE Trans. Knowledge and Data Engineering, Vol. 20, pp. 172-188, 2008.
[6] W. M. Campbell, C. K. Dagli, and C. J. Weinstein, Social Network Analysis with Content and
Graphs, Lincoln Lab. Journal, Vol. 20, 2013.
[7] W. E. Donath and A. J. Hoffman, Lower Bounds for the Partitioning of Graphs, IBM Journal of
Research and Development, Vol. 17, pp. 420-425, 1973.
[8] M. Fiedler, Algebraic Connectivity of Graphs, Czechoslovak Mathematical Journal, Vol. 23, pp.
298-305, 1973.
[9] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, NY, 1999.
[10] D. LaSalle and G. Karypis Multi-threaded Modularity Based Graph Clustering Using the Multilevel
Paradigm, Parallel Distrib. Comput., Vol. 76, pp. 66-80, 2015.
[11] S. P. Lloyd, Least Square Quantization in PCM, IEEE Trans. Information Theory, Vol. 28, pp.
129-137, 1982.
[12] U. von Luxburg, A Tutorial on Spectral Clustering, Technical Report No. TR-149, Max Planck
Institute, 2007.
[13] M. Naumov and T. Moon, Parallel Spectral Graph Partitioning, NVIDIA Technical Report, NVR2016-001, 2016.
[14] M. E. J. Newman, Assortative Mixing in Networks, Phys. Rev. Lett., Vol. 89, pp. 208701, 2002.
[15] M. E. J. Newman, The Structure and Function of Complex Networks, SIAM Review, Vol. 45, pp.
167-256, 2003.
[16] M. E. J. Newman and M. Girvan, Finding and Evaluating Community Structure in Networks,
Phys. Rev. E, Vol. 69, pp. 026113, 2004.
[17] M. E. J. Newman, Networks: An Introduction, Oxford University Press, New York, NY, 2010.
[18] D. Pelleg and A. Moore, X-means: Extending K-means with Efficient Estimation of the Number
of Clusters, Proc. 17th Int. Conf. on Machine Learning, pp. 727-734, 2000.
[19] M. Rosvall and C. T. Bergstrom, Maps of Random Walks on Complex Networks Reveal Community
Structure, Proc. Natl. Acad. Sci. USA, Vol. 105, pp. 1118-1123, 2008.
[20] Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, Philadelphia, PA, 2nd Ed., 2003.
[21] M. Chen, K. Kuzmin and B. K. Szymanski, Community Detection via Maximization of Modularity
and Its Variants, IEEE Trans. Comput. Social System, Vol. 1, pp. 46-65, 2014.
[22] S. White and P. Smyth, A Spectral Approach to Finding Communities in Graphs, SIAM Conf.
Data Mining, 2005.
[23] Nvidia, CUDA Toolkit, http://developer.nvidia.com/cuda-downloads
[24] The University of Florida Sparse Matrix Collection, http://www.cise.ufl.edu/research/sparse/matrices

10

