Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 394â€“403

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Switzerland
DiscriminativeZurich,
Learning
from Selective

Discriminative
from
Recommendation
andLearning
Its Application
in AdaBoost
Discriminative
Learning
from Selective
Selective
Recommendation
and
Its
Application in
AdaBoost
Recommendation
and
Its
in
AdaBoost
1
1* Application
2â€ 
Xiao-Yu Zhang , Shupeng Wang , Chao Li , Shiming Ge1â€ , Yong Wang1,

1ï€  2â€ 
1*
1â€ 
1
and Binbin
Xiao-Yu Zhang11, Shupeng Wang
1*, ChaoLiLi2â€ , Shiming Ge1â€ , Yong Wang1,
Xiao-Yu
Zhang
,
Shupeng
Wang
,
Chao
Li
,
Shiming
Ge
,
Yong
Wang
,
1
1ï€ 
Institute of Information Engineering,
ChineseLiAcademy
of Sciences, Beijing, China
and
Binbin
1ï€ 
2
andResponse
Binbin
LiAcademyTeam/Coordination
National
Computer
Network Emergency
Technical
Center
of China,
1
Institute
of Information
Engineering,
Chinese
of Sciences, Beijing,
China
2
2

1
Institute
of Information
Engineering,
Chinese
Academy
of Sciences, Beijing,
China
Beijing,
China
National
Computer
Network Emergency
Response
Technical
Team/Coordination
Center
of China,
National
Computer
Network Emergency
Response
Technical
Team/Coordination
Center
of China,
{zhangxiaoyu,
wangshupeng,
geshiming,
wangyong,
libinbin}@iie.ac.cn,
lichao@cert.org.cn
Beijing, China
Beijing,
China
{zhangxiaoyu, wangshupeng, geshiming, wangyong, libinbin}@iie.ac.cn, lichao@cert.org.cn
{zhangxiaoyu, wangshupeng, geshiming, wangyong, libinbin}@iie.ac.cn, lichao@cert.org.cn

Abstract
The
integration of semi-supervised learning and ensemble learning has been a promising research area.
Abstract
It
is
a typical ofprocedure
that one
learnerandrecommends
the pseudo-labeled
instances
with area.
high
Abstract
The integration
semi-supervised
learning
ensemble learning
has been a promising
research
The
integration
of
semi-supervised
learning
and
ensemble
learning
has
been
a
promising
research
area.
predictive
confidence
to
another,
so
that
the
training
dataset
is
expanded.
However,
the
new
learnerâ€™s
It is a typical procedure that one learner recommends the pseudo-labeled instances with high
demand
onconfidence
recommendation
as one
well
as the
the training
possibility
of incorrect
recommendation
neglected,
It
is a typical
procedure
that
learner
recommends
the
pseudo-labeled
instances
high
predictive
to another,
so that
dataset
is expanded.
However,
theare
newwith
learnerâ€™s
which
inevitably
jeopardize
the
learning
performance.
To
address
these
issues,
this
paper
proposes
the
predictive
confidence
to
another,
so
that
the
training
dataset
is
expanded.
However,
the
new
learnerâ€™s
demand on recommendation as well as the possibility of incorrect recommendation are neglected,
demand
on
recommendation
as
well
as
the
possibility
of
incorrect
recommendation
are
neglected,
Discriminative
Learning
from
Selective
Recommendation
(DLSR)
method.
On
one
hand,
both
which inevitably jeopardize the learning performance. To address these issues, this paper proposes the
which
inevitably
jeopardizefrom
the learning
performance.
To
address
these
issues,
proposes
the
reliability
and informativeness
of
the pseudo-labeled
instances
are
taken
intothis
account
selective
Discriminative
Learning
Selective
Recommendation
(DLSR)
method.
Onpaper
onevia
hand,
both
Discriminative
Learning
from
Selective
Recommendation
(DLSR)
method.
On
one
hand,
both
recommendation.
On
the
other
hand,
the
potential
in
both
correct
and
incorrect
recommendation
are
reliability and informativeness of the pseudo-labeled instances are taken into account via selective
reliability
and
of thethe
pseudo-labeled
instances
areand
taken
into account
via selective
formulated
in informativeness
discriminative
Based
on inDLSR,
we further
propose
the selective
semirecommendation.
On the otherlearning.
hand,
potential
both
correct
incorrect
recommendation
are
supervised
AdaBoost.
With
both
recommending
and
receiving
learners
engaged
in
ensemble
model
recommendation.
On
the
other
hand,
the
potential
in
both
correct
and
incorrect
recommendation
are
formulated in discriminative learning. Based on DLSR, we further propose the selective semilearning,
the
unlabeled
instances
are
explored
in
a
more
effective
way.
formulated
in
discriminative
learning.
Based
on
DLSR,
we
further
propose
the
selective
semisupervised AdaBoost. With both recommending and receiving learners engaged in ensemble model
supervised
AdaBoost.
With
bothare
recommending
engaged in ensemble model
learning,
the
unlabeled
instances
explored
in a and
morereceiving
effective learners
way.
Â©
2017 The
Authors.
Published
by Elsevier
B.V. learning,
Keywords:
semi-supervised
learning,
ensemble
selective
recommendation,
discriminative learning,
learning,
the
unlabeled
instances
are
explored
in
a
more
effective
way.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
AdaBoost
Keywords: semi-supervised learning, ensemble learning, selective recommendation, discriminative learning,
Keywords:
AdaBoost semi-supervised learning, ensemble learning, selective recommendation, discriminative learning,
AdaBoost

1 Introduction
1
Introduction
the past decades, machine learning has been thriving as an effective way to explore the world
1 Over
Introduction

by Over
extracting
patterns
in data
and learning
making has
predictions
basedason
experience
these
the past
decades,
machine
been thriving
an the
effective
way togained
explorefrom
the world
Over
the
past
decades,
machine
learning
has
been
thriving
as
an
effective
way
to
explore
the
world
patterns
[1]-[4].
Traditionally,
machine
learning
tasks
are
either
supervised
or
unsupervised.
The
by extracting patterns in data and making predictions based on the experience gained from these
former
aims
at
inferring
a
function
from
labeled
instances
which
can
generalize
to
unlabeled
instances,
by
extracting
patterns
in
data
and
making
predictions
based
on
the
experience
gained
from
these
patterns [1]-[4]. Traditionally, machine learning tasks are either supervised or unsupervised. The
whereas
the
latter
tries toa reveal
hidden
from
instances
based
some measure
of
patternsaims
[1]-[4].
Traditionally,
machine
learning
tasksunlabeled
are
either
supervised
oron
unsupervised.
The
former
at inferring
function
fromstructures
labeled
instances
which
can
generalize
to
unlabeled
instances,
former
aims
at
inferring
a
function
from
labeled
instances
which
can
generalize
to
unlabeled
instances,
whereas
the latter tries to reveal hidden structures from unlabeled instances based on some measure of
*
ï€ S. Wang
the joint
firsttoauthor
with
X.-Y. Zhang.
whereas
the islatter
tries
reveal
hidden
structures from unlabeled instances based on some measure of
â€ 
C. Li and S. Ge are the corresponding authors.
*
ï€ S. Wang is the joint first author with X.-Y. Zhang.
â€ *
â€ 

ï€ S.
firstcorresponding
author with X.-Y.
Zhang.
C. Wang
Li andisS.the
Gejoint
are the
authors.
C. Li and S. Ge are the corresponding authors.

1877-0509 Â© 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.080

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

inherent similarity or distance. In real-world applications, however, labeled instances are often
difficult, expensive, or time consuming to obtain, as they require the efforts of human annotators with
specific domain experience. Meanwhile, unlabeled instances are relatively easy to collect. As a result,
the typical dataset available consists of a small amount of labeled instances and a large amount of
unlabeled instances. Semi-supervised learning addresses this bottleneck by leveraging both labeled
and unlabeled instances to improve learning performance, and has been studied widely both in theory
and in practice [5]-[7].
Apart from incorporating more instances, higher learning performance can also be achieved by
creating more learners. This type of solution is referred to as ensemble method, which trains multiple
learners to solve the same problem [8][9]. It is well known that an ensemble is usually significantly
more accurate than any single learner. One of the most influential ensemble methods is the AdaBoost
algorithm, which converts multiple weak learners to a strong one [10]. AdaBoost is adaptive in the
sense that subsequent weak learners are tweaked in favor of the instances misclassified by previous
leaners. In AdaBoost, the individual leaners can be weak, but as long as the performance of each one
is slightly better than random guessing, the final model can be proven to converge to a strong learner.
Because of this attractive property, AdaBoost has been successfully applied to various real-world
applications.
Traditionally, AdaBoost falls into the supervised learning category. It is a natural consideration to
extend AdaBoost to semi-supervised scenario. Several algorithms have been proposed in this direction,
including ASSEMBLE [11], MarginBoost [12], SemiBoost [13], RegBoost [14], etc. In these
algorithms, unlabeled instances with high predictive confidence are recommended as the pseudolabeled ones between the weak learners. Using this additional information as guidance, the weak
leaners can be further updated to obtain higher performance. However, as will be indicated in the
following text, the most confident pseudo-labeled instances from a given learner are not necessarily
demanded most by the new learner. Furthermore, for the sake of predictive accuracy, the learner
receiving recommendation should treat the pseudo-labeled instances discriminatively according to
their potential values in case of both correct and incorrect prediction, instead of accepting them
unreservedly and equally.
To address the aforementioned issues, we propose in this paper a novel Discriminative Learning
from Selective Recommendation (DLSR) method. As the name indicates, DLSR consists of two stages.
In the pseudo-labeled instance recommendation stage, we incorporate both reliability and
informativeness into a comprehensive measurement to select the most valuable recommendation
candidate. In the learner re-training stage, both the potential benefit in accepting the pseudo-labels and
the probability of denying it are jointly studied to formulate a discriminative way of learning. Then,
we apply DLSR to extend AdaBoost and propose the Selective Semi-supervised AdaBoost (SSAdaBoost) algorithm. In this way, the pseudo-labeled instances are placed under stricter inspections
from not only the learners initiating recommendation but also those receiving recommendation.
Encouraging results are received from experiments on real-world classification tasks.

2 Discriminative Learning from Selective Recommendation
This paper focuses on the classification problem in machine learning, which identifies the category
membership (label) of an observation (instance). In traditional supervised learning, the learner is
trained by passively accepting labeled instances from the oracle which is usually human expert. In
active learning, a learner can actively select several instances to query for their labels, thereby
achieving higher learning performance with limited number of training instances [15][16]. The family
of semi-supervised ensemble learning methods, collectively known as collaborative learning, assigns
pseudo-labels to unlabeled instances based on the predictions of an existing learner, and then trains a
new learner using the pseudo-labeled instances as additional training data. By leveraging both labeled

395

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

396	

(1)

(2)

(3)
(4)
Figure 1: Comparison of learning processes in (1) passive learning, (2) active learning, (3) collaborative
learning, and (4) DLSR.

and unlabeled instances, collaborative learning has shown its advantage in various applications. Both
passive learning and collaborative learning share the similar learning process where the learner
receives training data in a passive way, except that the former learns from an oracle whereas the latter
from a different learner. In order to further improve the performance of learning from recommendation,
we present a selective recommendation method to actively select pseudo-labeled instances for training.
Different from active learning that mainly focuses on the informativeness of instances, we also
incorporate the reliability of pseudo-labels that reflects the recommending learnerâ€™s predictive
confidence. Moreover, in the model training stage, a discriminative learning method is presented by
assigning weights to the pseudo-labeled instances based on their potential in model improvement.
Comparison of the proposed Discriminative Learning from Selective Recommendation (DLSR)
method with passive learning, active learning, and collaborative learning is illustrated in Figure 1.
In the text that follows, we let ğ’™ğ’™ âˆˆ ğ’³ğ’³ denote the input feature of an instance, and ğ‘¦ğ‘¦ âˆˆ ğ’´ğ’´ =
{1, â€¦ , ğ¾ğ¾} denote the label indicating the class that an instance falls into, where ğ¾ğ¾ is the number of
classes. ğ‘ˆğ‘ˆ and ğ¿ğ¿ stand for the unlabeled and labeled dataset, respectively. Probabilistic model is used
(ğ‘›ğ‘›)
for classification based on the posterior distribution ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒğ¿ğ¿ ) of label ğ‘¦ğ‘¦ conditioned on the input ğ’™ğ’™,
(ğ‘›ğ‘›)

where ğœƒğœƒğ¿ğ¿

is model parameter of the ğ‘›ğ‘›-th learner optimized for the corresponding labeled dataset ğ¿ğ¿.

2.1 Selective Recommendation

Typically, methods of learning from recommendation start with training a learner â„ğ‘›ğ‘› : ğ’³ğ’³ â†’ ğ’´ğ’´ using
the labeled dataset ğ¿ğ¿. After that, predictions can be made w.r.t. each unlabeled instance ğ’™ğ’™ âˆˆ ğ‘ˆğ‘ˆ:
with predictive confidence:

(ğ‘›ğ‘›)

ğ‘¦ğ‘¦ âˆ— = arg max ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒğ¿ğ¿ )
ğ‘¦ğ‘¦âˆˆğ’´ğ’´

(ğ‘›ğ‘›)

ğ’ğ’ğ‘›ğ‘› (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) = ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ âˆ— |ğ’™ğ’™; ğœƒğœƒğ¿ğ¿ )

(1)

(2)

When recommending the pseudo-labeled instances to the next learner â„ğ‘›ğ‘›+1 , those with high
confidence is preferable from the â„ğ‘›ğ‘› point of view.
However, the high-confident pseudo-labeled instances may provide little information, since they
are not necessarily needed most from the â„ğ‘›ğ‘›+1 point of view. Following the error reduction criterion
for decision-theoretic selective sampling, the informativeness of an unlabeled instance is negatively
correlated with the entropy over the rest unlabeled ones. As a result, when accepting (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) as
recommendation, its value in model update can be formulated as:

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

397

Algorithm 1: Discriminative Learning from Selective Recommendation (DLSR)
Input: labeled dataset ğ¿ğ¿ âŠ‚ (ğ’³ğ’³, ğ’´ğ’´);
unlabeled dataset ğ‘ˆğ‘ˆ âŠ‚ ğ’³ğ’³;
number of instances for recommendation ğ‘€ğ‘€;
leaner â„ğ‘›ğ‘› ~ğœƒğœƒ (ğ‘›ğ‘›) : ğ’³ğ’³ â†’ ğ’´ğ’´;
(ğ‘›ğ‘›+1)
: ğ’³ğ’³ â†’ ğ’´ğ’´ trained on ğ¿ğ¿.
learner â„ğ‘›ğ‘›+1 ~ğœƒğœƒğ¿ğ¿
Output: updated leaner â„ğ‘›ğ‘›+1 ~ğœƒğœƒ (ğ‘›ğ‘›+1) .
// Initialization:
1: Initialize recommendation dataset ğ‘…ğ‘…ğ‘›ğ‘›+1 = âˆ….
// Selective recommendation:
2: for ğ’™ğ’™ âˆˆ ğ‘ˆğ‘ˆ
3:
Predict label ğ‘¦ğ‘¦ âˆ— according to (1).
4:
Calculate ğ’ğ’ğ‘›ğ‘› , ğ’œğ’œğ‘›ğ‘›+1 and ğ’®ğ’®ğ‘›ğ‘›+1 according to (2), (3) and (5), respectively.
5: end for
6: Sort instances ğ’™ğ’™ âˆˆ ğ‘ˆğ‘ˆ in descending order w.r.t. ğ’®ğ’®ğ‘›ğ‘›+1 .
7: Add the top ğ‘€ğ‘€ ranked instances into ğ‘…ğ‘…ğ‘›ğ‘›+1 .
// Discriminative learning:
8: for ğ’™ğ’™ âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›+1
9:
Calculate ğ’Ÿğ’Ÿğ‘›ğ‘›+1 and ğ‘£ğ‘£ (ğ‘›ğ‘›+1) according to (6) and (8), respectively.
10: end for
11: Re-train â„ğ‘›ğ‘›+1 incrementally based on (ğ‘…ğ‘…ğ‘›ğ‘›+1 , ğ’—ğ’—(ğ‘›ğ‘›+1) ):
(ğ‘›ğ‘›+1)

â„ğ‘›ğ‘›+1 âˆ¼ ğœƒğœƒ (ğ‘›ğ‘›+1) â‡ ğœƒğœƒğ¿ğ¿+ğ‘…ğ‘…ğ‘›ğ‘›+1 .

(ğ‘›ğ‘›+1)

Ìƒ; ğœƒğœƒğ¿ğ¿+(ğ’™ğ’™,ğ‘¦ğ‘¦ âˆ—) )
ğ’œğ’œğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) = âˆ’ âˆ‘ ğ»ğ»(ğ‘¦ğ‘¦Ìƒ|ğ’™ğ’™

(3)

ğ»ğ»(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒ) = âˆ’ âˆ‘ ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒ) log ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒ)

(4)

ğ’®ğ’®ğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) = ğ’ğ’ğ‘›ğ‘› (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— )ğ’œğ’œğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— )

(5)

Ìƒâˆˆğ‘ˆğ‘ˆâˆ’ğ’™ğ’™
ğ’™ğ’™

where ğ¿ğ¿ + (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) stands for the expanded labeled dataset with a new instance (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) added, and
ğ‘¦ğ‘¦âˆˆğ’´ğ’´

represents the conditional entropy.
According to (2) and (3), ğ’ğ’ğ‘›ğ‘› measures the reliability of recommendation on a certain instance that
â„ğ‘›ğ‘› can supply, and ğ’œğ’œğ‘›ğ‘›+1 reflects the demand of â„ğ‘›ğ‘›+1 for the instance. In order to balance the â€œsupply
and demandâ€, we incorporate both factors into a comprehensive measurement:

Based on ğ’®ğ’®ğ‘›ğ‘›+1 , reliable and informative instances can be selected and recommended to â„ğ‘›ğ‘›+1 for
the following training stage.

2.2 Discriminative Learning

As the old saying goes, learning without thinking is useless. The information in hand may not
always lead to truth, and can sometimes be inaccurate or misleading. Inspired by this learning
principle underlying the cognitive process of humans, an effective machine learning paradigm should
be able to learn discriminatively. To be more specific, it is harmful to accept recommendation
unreservedly, since it comes from prediction of learner â„ğ‘›ğ‘› . Instead, the possibility that (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) is

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

398	

mislabeled by â„ğ‘›ğ‘› should be taken into account. In case that ğ’™ğ’™ adopts an alternative label ğ‘¦ğ‘¦ , its
usefulness can be quantified with the expected entropy over the remaining unlabeled instances.

where

ğ’Ÿğ’Ÿğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) = âˆ’

1

ğ‘ğ‘ (ğ‘›ğ‘›+1)

(ğ‘›ğ‘›+1)

âˆ‘ ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒğ¿ğ¿

ğ‘¦ğ‘¦â‰ ğ‘¦ğ‘¦ âˆ—

(ğ‘›ğ‘›+1)

ğ‘ğ‘ (ğ‘›ğ‘›+1) = âˆ‘ ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒğ¿ğ¿
ğ‘¦ğ‘¦â‰ ğ‘¦ğ‘¦ âˆ—

(ğ‘›ğ‘›+1)

Ìƒ; ğœƒğœƒğ¿ğ¿+(ğ’™ğ’™,ğ‘¦ğ‘¦) )
) âˆ‘ ğ»ğ»(ğ‘¦ğ‘¦Ìƒ|ğ’™ğ’™
Ìƒâˆˆğ‘ˆğ‘ˆâˆ’ğ’™ğ’™
ğ’™ğ’™

(ğ‘›ğ‘›+1)

) = 1 âˆ’ ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ âˆ— |ğ’™ğ’™; ğœƒğœƒğ¿ğ¿

)

(6)

(7)

(ğ‘›ğ‘›+1)
is the normalization factor which enables ğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦|ğ’™ğ’™; ğœƒğœƒğ¿ğ¿
)â„ğ‘ğ‘ (ğ‘›ğ‘›+1) to be a distribution on condition that
âˆ—
ğ‘¦ğ‘¦ â‰  ğ‘¦ğ‘¦ .
Compared with ğ’œğ’œğ‘›ğ‘›+1 that measures the potential benefit of accepting (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ), ğ’Ÿğ’Ÿğ‘›ğ‘›+1 tries to answer
the questions whether and to what extent the learner â„ğ‘›ğ‘›+1 would be improved if the instance ğ’™ğ’™ was
assigned with labels other than ğ‘¦ğ‘¦ âˆ— .
Subsequently, given a recommendation dataset ğ‘…ğ‘…ğ‘›ğ‘›+1 âŠ‚ ğ‘ˆğ‘ˆ, the weight associated with each pseudolabeled instance (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) is defined as:

where

ğ‘£ğ‘£ (ğ‘›ğ‘›+1) (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— ) =

ğ‘‰ğ‘‰

1

âˆ™
(ğ‘›ğ‘›+1)

ğ’œğ’œğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— )
ğ’Ÿğ’Ÿğ‘›ğ‘›+1 (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— )

ğ‘‰ğ‘‰ (ğ‘›ğ‘›+1) = âˆ‘ ğ‘£ğ‘£ (ğ‘›ğ‘›+1) (ğ’™ğ’™, ğ‘¦ğ‘¦ âˆ— )
ğ’™ğ’™âˆˆğ‘…ğ‘…ğ‘›ğ‘›+1

(8)

(9)

is the normalization term. With the recommendation dataset ğ‘…ğ‘…ğ‘›ğ‘›+1 and the corresponding weight vector
ğ’—ğ’—(ğ‘›ğ‘›+1) utilized as expansion of the existing training dataset, learner â„ğ‘›ğ‘›+1 can be further updated.
The detailed procedure of DLSR is summarized in Algorithm 1.

3 Selective Semi-supervised AdaBoost
The DLSR method proposed in the previous section can be applied to improve existing machine
learning algorithms where multiple learners are available. Specifically, as one of the most influential
ensemble methods, AdaBoost has achieved great success and received extensive attention in various
learning applications. The most attractive property of AdaBoost is that it naturally consists of a set of
weak learners trained sequentially and combined adaptively for prediction. Therefore, it is a desirable
container model to implement DLSR.
Traditionally, during the learning process of AdaBoost, information is passed on from one learner
to another in the form of weights on labeled instances. This process, as a whole, can be seen as a
single information propagation path from anterior learners to posterior ones through the labeled
dataset.
In order to fully exploit unlabeled instances, semi-supervised learning is introduced into AdaBoost
to motivate collaboration among the multiple learners. In this way, another information path is
generated through the unlabeled dataset, in which unlabeled instances are selected and recommended
with the corresponding weights. Collectively, we refer to this type of methods as Semi-supervised
AdaBoost (S-AdaBoost).
Since information on unlabeled instances is far less reliable than that on labeled instances, DLSR is
undoubtedly a better choice when initiating learning between different learners. With both

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

(1)

399

(2)

(3)
(4)
Figure 2: Illustration of information propagation path in (1) AdaBoost, (2) S-AdaBoost, (3) SS-AdaBoost
with 1-to-1 recommendation, and (4) SS-AdaBoost with m-to-1 recommendation.

recommending and receiving learners engaged in model learning, the proposed algorithm is named
Selective Semi-supervised AdaBoost (SS-AdaBoost).
Information propagation paths of AdaBoost, S-AdaBoost and SS-AdaBoost are illustrated in
Figure 2.

3.1 Labeled Path: Labeled Instances Re-weighting
Different types of AdaBoost methods share the identical labeled path, in which a weight vector ğ’˜ğ’˜
is maintained over the training dataset.
Initially, labeled instances (ğ’™ğ’™ğ‘–ğ‘– , ğ‘¦ğ‘¦ğ‘–ğ‘– ) âˆˆ ğ¿ğ¿ are equally weighted as
ğ‘¤ğ‘¤ (1) (ğ’™ğ’™ğ‘–ğ‘– , ğ‘¦ğ‘¦ğ‘–ğ‘– ) =

1
|ğ¿ğ¿|

(10)

After the training of weak learner â„ğ‘›ğ‘› , the labeled instances are re-weighted according to the
corresponding training error:
ğ‘¤ğ‘¤ (ğ‘›ğ‘›+1) (ğ’™ğ’™ğ‘–ğ‘– , ğ‘¦ğ‘¦ğ‘–ğ‘– ) =

ğ‘¤ğ‘¤ (ğ‘›ğ‘›) (ğ’™ğ’™ğ‘–ğ‘– , ğ‘¦ğ‘¦ğ‘–ğ‘– )exp(âˆ’ğ›¼ğ›¼ğ‘›ğ‘› ğ‘¦ğ‘¦ğ‘–ğ‘– â„ğ‘›ğ‘› (ğ’™ğ’™ğ‘–ğ‘– ))
ğ‘Šğ‘Š (ğ‘›ğ‘›)

where ğ‘Šğ‘Š (ğ‘›ğ‘›) is the normalization factor, and

1
1 âˆ’ ğœ–ğœ–ğ‘›ğ‘›
ğ›¼ğ›¼ğ‘›ğ‘› = ln (
)
2
ğœ–ğœ–ğ‘›ğ‘›

(11)

(12)

400	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

Algorithm 2: Selective Semi-supervised AdaBoost (SS-AdaBoost)
Input: labeled dataset ğ¿ğ¿ âŠ‚ (ğ’³ğ’³, ğ’´ğ’´);
unlabeled dataset ğ‘ˆğ‘ˆ âŠ‚ ğ’³ğ’³;
number of weak learners ğ‘ğ‘;
number of instances for recommendation ğ‘€ğ‘€;
type of recommendation ğ‘ğ‘ âˆˆ {1-to-1, m-to-1}.
Output: ensemble leaner ğ»ğ»ğ‘ğ‘ : ğ’³ğ’³ â†’ ğ’´ğ’´.
// Initialization:
1: Initialize labeled instance weight ğ’˜ğ’˜(1) according to (10).
(1)
2: Train weak learner â„1 based on (ğ¿ğ¿, ğ’˜ğ’˜(1) ): â„1 âˆ¼ ğœƒğœƒ (1) â‡ ğœƒğœƒğ¿ğ¿ .
3: Calculate learning error ğœ–ğœ–1 and learner weight ğ›¼ğ›¼1 according to (13) and (12), respectively.
4: for ğ‘›ğ‘› = 1 to ğ‘ğ‘ âˆ’ 1
// Labeled path:
5:
Update labeled instance weight ğ’˜ğ’˜(ğ‘›ğ‘›+1) according to (11).
(ğ‘›ğ‘›+1)
.
6:
Train weak learner â„ğ‘›ğ‘›+1 based on (ğ¿ğ¿, ğ’˜ğ’˜(ğ‘›ğ‘›+1) ): â„ğ‘›ğ‘›+1 âˆ¼ ğœƒğœƒğ¿ğ¿
// Unlabeled path:
7:
Construct ensemble learner ğ»ğ»ğ‘›ğ‘› ~ğ›©ğ›© (ğ‘›ğ‘›) according to (14), based on {â„1 , â€¦ , â„ğ‘›ğ‘› } and {ğ›¼ğ›¼1 , â€¦ , ğ›¼ğ›¼ğ‘›ğ‘› }
8:
case: ğ‘ğ‘ = 1-to-1
9:
Update weak learner â„ğ‘›ğ‘›+1 with â„ğ‘›ğ‘› according to Algorithm 1:
(ğ‘›ğ‘›+1)
).
â„ğ‘›ğ‘›+1 âˆ¼ ğœƒğœƒ (ğ‘›ğ‘›+1) â‡ DLSR(ğ¿ğ¿, ğ‘ˆğ‘ˆ, ğ‘€ğ‘€, â„ğ‘›ğ‘› ~ğœƒğœƒ (ğ‘›ğ‘›) , â„ğ‘›ğ‘›+1 ~ğœƒğœƒğ¿ğ¿
10:
end case
11:
case: ğ‘ğ‘ = m-to-1
12:
Update weak learner â„ğ‘›ğ‘›+1 with ğ»ğ»ğ‘›ğ‘› according to Algorithm 1:
(ğ‘›ğ‘›+1)
).
â„ğ‘›ğ‘›+1 âˆ¼ ğœƒğœƒ (ğ‘›ğ‘›+1) â‡ DLSR(ğ¿ğ¿, ğ‘ˆğ‘ˆ, , ğ»ğ»ğ‘›ğ‘› ~ğ›©ğ›© (ğ‘›ğ‘›) , â„ğ‘›ğ‘›+1 ~ğœƒğœƒğ¿ğ¿
13:
end case
14:
Calculate learning error ğœ–ğœ–ğ‘›ğ‘›+1 and learner weight ğ›¼ğ›¼ğ‘›ğ‘›+1 according to (13) and (12),
respectively.
15: end for
16: Construct ensemble learner ğ»ğ»ğ‘ğ‘ ~ğ›©ğ›© (ğ‘ğ‘) according to (14), based on {â„1 , â€¦ , â„ğ‘ğ‘ } and {ğ›¼ğ›¼1 , â€¦ , ğ›¼ğ›¼ğ‘ğ‘ }.
ğœ–ğœ–ğ‘›ğ‘› = Prğ’™ğ’™ ~ğ‘¤ğ‘¤ (ğ‘›ğ‘›) (â„ğ‘›ğ‘› (ğ’™ğ’™ğ‘–ğ‘– ) â‰  ğ‘¦ğ‘¦ğ‘–ğ‘– )
ğ‘–ğ‘–

(13)

are the weight and error of â„ğ‘›ğ‘› , respectively. In this re-weighting process, the misclassified instances
are inclined to be attached with larger weights, so that the following weak learner is forced to focus on
the hard instances in the training dataset. As instructive guidance, these weights are passed between
adjacent weak learners through the labeled path.

3.2 Unlabeled Path: Unlabeled Instances Recommending
Besides labeled path, S-AdaBoost and SS-AdaBoost further establish an unlabeled path, in which
unlabeled instances are explored by means of recommendation among the weak leaners.
Different from S-AdaBoost, unlabeled instances recommending is based on DLSR in SS-AdaBoost.
Given a trained learner, predictions can be made on unlabeled instances to obtain the pseudo-labeled
instances. Then, for the subsequent learner, the expanded training dataset is available with selective
recommendation. After that, the new learner is updated, or re-trained, via discriminative learning.
When ğ‘›ğ‘› weak learners are obtained, the ensemble learner is constructed using additive weighted
combination.

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

ğ‘›ğ‘›

ğ»ğ»ğ‘›ğ‘› (ğ’™ğ’™) = âˆ‘ ğ›¼ğ›¼ğ‘¡ğ‘¡ â„ğ‘¡ğ‘¡ (ğ’™ğ’™)

401

(14)

ğ‘¡ğ‘¡=1

According to the range of recommendation, SS-AdaBoost falls into two modes, i.e. 1-to-1 and mto-1. In 1-to-1 mode, the pseudo-labeled instances are recommended only between the immediately
adjacent weak learners, say â„ğ‘›ğ‘› and â„ğ‘›ğ‘›+1 ; whereas in m-to-1 mode, weak learner â„ğ‘›ğ‘›+1 receives
recommendation from all the anterior ones â„1 , â€¦ , â„ğ‘›ğ‘› , or equivalently from the ensemble learner ğ»ğ»ğ‘›ğ‘› .
The detailed procedure of SS-AdaBoost with both labeled and unlabeled paths is summarized in
Algorithm 2.

4 Experiments
In order to validate the performance of the DLSR method and its application in SS-AdaBoost, we
carry out classification tasks on datasets of malware and patent, respectively.

4.1 Datasets
Detailed description of the datasets are as follows.
Malware Classification. Malware classification is a basic application in information security. We
collect 3220 malware instances of 8 classes. Each instance is denoted with a 162-dimentional vector,
representing its API and key code fragment.
Patent Classification. As the basis for patent analysis, patent classification is indispensable for
effective management of patents and in-depth exploration of valuable information. Patent documents
come from the Innography database. 5000 patents on electric automobile are collected as the dataset,
all of which are classified manually by domain experts into 5 classes, i.e. battery, battery management,
motor, motor control, and vehicle control unit. Among the patents, 5484 terms are extracted as raw
text feature. The weight of a term within a patent is calculated with TF-IDF (term frequencyâ€“inverse
document frequency). Principal Component Analysis (PCA) is then used for dimension reduction,
arriving at a 300- dimensional feature vector.

4.2 Evaluation of DLSR
In this experiment, we compare DLSR with passive learning (PL) and collaborative learning (CL),
which were discussed theoretically in Section 2.
Each dataset is split into two equal parts: one for training and the other for testing. Independent
Component Analysis (ICA) is implemented on the input space to extract two sets of irrelevant features,
based on which two learners are trained correspondingly. Detailed setup about number of labeled and
unlabeled instances and dimension of sub-features is listed in Table 1. For classification, regularized
softmax regression is used to model the multiple classes simultaneously. Given a fixed number of
labeled instances, we examine the classification performance with gradually increasing number of
unlabeled instances recommended. Classification results are illustrated in Figure 3.
It is observed that both DLSR and CL outperform PL. It demonstrates that the exploration of
unlabeled instances can provide additional information for model improvement. Besides labeled
instance, the learners can learn from each other based on different point of view. In DLSR, unlabeled
instances are utilized in a more effective way by recommending selectively and learning
discriminatively, which brings about further improvement in the classification performance. The
highest classification performance indicates that DLSR is the most effective way of semi-supervised
ensemble learning.

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

402	

Malware
Patent

labeled
360
500

Instance
unlabeled
testing
1250
1610
2000
2500

Table 1: Detailed setup on experimental datasets.

total
3220
5000

feature1
81
150

Feature
feature2
81
150

total
162
300

Figure 3: Classification performance of passive learning (PL), collaborative learning (CL), and DLSR
with increasing number of unlabeled instances.

Figure 4: Classification performance of AdaBoost, S-AdaBoost, and SS-AdaBoost with 10 and 20 base
classifiers.

4.3 Evaluation of SS-AdaBoost
In this experiment, we compare SS-AdaBoost with AdaBoost and S-AdaBoost, which were
discussed in Section 3.
Detailed setup with instances and features are identical to that in Table 1. Regularized softmax
regression is adopted as base classifier in the three AdaBoost methods. The number of classifiers in
the ensemble is empirically chosen as 10 and 20. The odd-numbered classifiers are trained with feature
1, and the even-numbered with feature 2. According to experiment 1, we fix the number of unlabeled
instances for recommendation to 30% of the number of labeled, i.e. 108 and 150 respectively, which
brings about approximately the highest performance of DLSR. Classification results are illustrated in
Figure 4.
As we can see, both SS-AdaBoost and S-AdaBoost achieve better performance than the traditional
supervised AdaBoost, because they take full advantage of the information from both labeled and
unlabeled instances. With both recommending and receiving learners engaged in model learning, the
unlabeled instances selected by SS-AdaBoost are much more instructive than those by S-AdaBoost.
Therefore, as expected, SS-AdaBoost outperforms all the competitors and receives the highest
classification accuracy. As for the recommendation mode of SS-AdaBoost, m-to-1 outperforms 1-to-1
because the recommendation made collectively from multiple learners tends to be more reliable than
that from any individual learner.

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394â€“403

5 Conclusion
In this paper, we proposed an effective way of learning from recommendation. The contributions
of this work are three-fold. First, we integrate reliability and informativeness into a comprehensive
measurement for selective recommendation to balance the â€œsupply and demandâ€ between the learners.
Second, the potential in both correct and incorrect recommendation are formulated in discriminative
learning to weight the pseudo-labeled instances adaptively. Last but not least, the proposed method is
implemented to improve the existing AdaBoost into the selective semi-supervised AdaBoost, which is
effective in that both recommending and receiving learners are engaged in model learning.
Encouraging results are received from experiments conducted on real-world classification tasks.

6 Acknowledgment
This work was supported by National Natural Science Foundation of China (Grant 61501457), and
the National High Technology Research and Development Program of China (Grant 2013AA013204).

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]

C. M. Bishop, Pattern Recognition and Machine Learning. New York: Springer, 2006.
I. H. Witten, E. Frank, and M. A. Hall, Data Mining: Practical Machine Learning Tools and Techniques.
Burlington: Elsevier, 2011.
X. Y. Zhang, â€œSimultaneous Optimization for robust correlation estimation in partially observed social
network,â€ Neurocomputing, 205, pp. 455â€“462, 2016.
X. Zhang, C. Xu, J. Cheng, H. Lu, and S. Ma, â€œEffective annotation and search for video blogs with
integration of context and content analysis,â€ IEEE Transactions on Multimedia, 11(2), pp. 272â€“285, 2009.
X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning. Morgan & Claypool, 2009.
R. G. Soares, H. Chen, and X. Yao, â€œSemi-supervised classification with cluster regularization,â€ IEEE
Transactions on Neural Networks and Learning Systems, 23(11), pp. 1779â€“1792, 2012.
X. Zhang, â€œInteractive Patent classification based on multi-classifier fusion and active learning,â€
Neurocomputing, 127, pp. 200â€“205, 2014.
Z. H. Zhou, Ensemble methods: foundations and algorithms. CRC Press, 2012.
R. Polikar, â€œEnsemble based systems in decision making,â€ IEEE Circuits and systems magazine, 6(3), pp.
21â€“45, 2006.
R. Rojas, â€œAdaBoost and the super bowl of classifiers a tutorial introduction to adaptive boosting,â€ Freie
University, Berlin, Tech. Rep., 2009.
K. Bennett, A. Demiriz, and R. Maclin, â€œExploiting unlabeled data in ensemble methods,â€ In Proc. ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 289â€“296, 2002.
Y. Grandvalet, and C. Ambroise, â€œSemi-supervised marginboost,â€ In Advances in neural information
processing systems, pp. 553â€“560, 2001.
P. Mallapragada, R. Jin, A. Jain, and Y. Liu, â€œSemiboost: Boosting for semi-supervised learning,â€ IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(11), pp. 2000â€“2014, 2009.
K. Chen and S. Wang, â€œSemi-supervised learning via regularized boosting working on multiple semisupervised assumptions,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1), pp. 129â€“
143, 2011.
X. Y. Zhang, S. Wang, and X. Yun, â€œBidirectional active learning: a two-way exploration into unlabeled
and labeled data set,â€ IEEE Transactions on Neural Networks and Learning Systems, 26(12), pp. 3034â€“
3044, 2015.
X. Y. Zhang, S. Wang, X. Zhu, X. Yun, G. Wu, and Y. Wang, â€œUpdate vs. upgrade: modeling with
indeterminate multi-class active learning,â€ Neurocomputing, 162, pp. 163â€“170, 2015.

403

