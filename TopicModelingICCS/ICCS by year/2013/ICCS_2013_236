Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 169 ‚Äì 178

International Conference on Computational Science, ICCS 2013

Achieving Checkpointing Global Consistency through a Hybrid
Compile Time and Runtime Protocol
Iv¬¥an Cores1,2 , Gabriel Rodr¬¥ƒ±guez, Mar¬¥ƒ±a J. Mart¬¥ƒ±n, Patricia Gonz¬¥alez
Computer Architecture Group, University of A CoruÀúna, Spain

Abstract
The execution times of large-scale parallel applications on modern multi/many-core systems are usually longer than their mean
time between failures. Therefore, parallel applications must tolerate hardware failures to ensure that not all computation is lost
on machine failures. Checkpointing and rollback recovery are very useful techniques to implement fault-tolerant applications.
In parallel applications a checkpointing protocol is required to guarantee that individual checkpoints form a consistent global
state. Coordinated approaches are the most popular solution to achieve global checkpointing consistency. However, their main
drawback is their poor scalability due to the required runtime coordination. This work presents a new hybrid protocol that
combines the detection of valid recovery lines at compile time with a light and asynchronous protocol at runtime to negotiate
the closest valid recovery line. Experimental results prove the eÔ¨Éciency and scalability of the proposal.
Keywords: Checkpointing, Fault tolerance, Parallel programming, MPI

1. Introduction
The current trend in supercomputing environments is the use of large clusters, often heterogeneous, in which
the nodes are multi/many-core systems. One of the most important features of these computing environments is
their highly dynamic nature. Experimental data about failure rates on diÔ¨Äerent platforms [1] show that their mean
time between failures (MTBF) is quite low. Furthermore, as the number of cores increases so does the failure rate.
Since most of the compute-intensive applications that run in these environments are parallel programs, the low
MTBF limits the ability of these platforms for running such applications, which decreases their productivity.
Many fault tolerance methods for parallel applications exist in the literature, checkpoint and rollback recovery [2] being the most popular. It periodically saves the computation state to stable storage, so that the application
execution can be resumed by restoring such state. Though the checkpointing technique is easy to understand,
there are several issues to be solved in implementing practical checkpoint solutions for parallel applications, such
as checkpointing consistency. The basic diÔ¨Äerence between sequential and parallel applications in terms of failure
recovery is the existence of dependencies imposed by interprocess communications. If a checkpoint is placed in
the code between two matching communication statements, an inconsistency would occur upon recovery, since the
Ô¨Årst one will not be executed. Several solutions have been proposed to ensure the consistency of a checkpointing
1 Corresponding
2 Email

author
address:ivan.coresg@udc.es

1877-0509 ¬© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.180

170

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

scheme [2], being the coordinated approaches the most common practical choice [3, 4, 5, 6] due to the simplicity
of recovery. However, an important drawback of coordinated protocols is their lack of scalability [7].
An alternative, proposed by the authors in [8], is spatial coordination: checkpoints are taken at the same relative
locations on the code by all processes, but not necessarily at the same time. This approach avoids the runtime
overhead of the traditional coordinated protocols, transferring it to compile time, which improves scalability.
However, this solution does not allow the user to deÔ¨Åne a checkpointing frequency in temporal terms.
This work presents a new protocol, which is a hybrid approach between classical coordination protocols at
runtime and the authors‚Äô previous spatial coordination protocol at compile time. The new protocol allows to
specify a time step in the checkpointing operation, while preserving high scalability. The proposal has been
implemented in the CPPC checkpointing framework [9]. Experimental results prove the eÔ¨Éciency of the proposal
and its scalability.
The structure of this paper is as follows. Section 2 describes in detail the problem of achieving a consistent
global state to perform checkpointing. Section 3 details the proposed solution to ensure consistency. Section 4
describes the implementation of the proposal using the CPPC checkpointing framework. Section 5 shows and
discusses the experimental results. Finally, Section 6 concludes the paper.
2. State of the Art in Checkpointing Consistency
A system will recover properly after a failure if its internal state is consistent with the system behavior observed before the fault [10]. Rollback recovery protocols, therefore, must maintain information about the internal
interactions between processes. A global state of a parallel application is a collection of individual states of all
participating processes and the state of the communication channels between them. A consistent global state can
be seen as one that may occur during a fault-free execution. In a consistent global state, if the state of a process
reÔ¨Çects the reception of a message, then the state of the corresponding issuer reÔ¨Çects the sending of this message [11]. Figure 1 shows two examples of global states, one consistent and one inconsistent. In this Ô¨Ågure every
mk represents a communication between two processes. In the inconsistent example in Figure 1(a) the process p2
has received the message m4 while the state of p1 does not reÔ¨Çect its shipping. This state is inconsistent since it
can not occur in a fault-free situation. Message m4 is called an inconsistent or ghost message. A global state is
said to be consistent when it does not present inconsistent messages. In the consistent example in Figure 1(b) the
message m4 was sent but not received yet. It represents a situation where a message has left the issuer and is still
traveling through the network. This is called an in transit message. When a global state does not present in transit
messages, it is said to be transitless. A global state that is both consistent and transitless is said to be strongly
consistent.

p1

p1

p2

p2

p3

p3

(a) Inconsistent state

(b) Consistent state

Fig. 1. Examples of inconsistent and consistent states

Rollback recovery solutions must go back in the execution to achieve a consistent global state after a failure.
In checkpoint-based approaches the process state is retrieved correctly and consistently in the last valid recovery

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

line, deÔ¨Åned as the most recent set of consistent checkpoints. DiÔ¨Äerent solutions have been proposed to ensure
consistency [2].
In uncoordinated checkpoint protocols the checkpoint of each process is executed independently of the other
processes and no further information is stored. Uncoordinated solutions have low overhead in the case of fault-free
executions, but in case of failure they are susceptible to the so-called domino eÔ¨Äect 3 . Besides, since the processes
dump their states independently of each other, many of these checkpoints may be useless, as they will not be part
of any valid recovery line. For these reasons, uncoordinated protocols have not been widely used in practice. As
an alternative, uncoordinated checkpointing may be combined with message logging to avoid the domino eÔ¨Äect at
the expense of a high overhead in communication latencies [7].
In coordinated protocols all processes coordinate their checkpoints to produce a consistent global system state.
Coordinated checkpointing simpliÔ¨Åes recovery and prevents the domino eÔ¨Äect, since every process always restarts
from its most recent checkpoint. Also, coordinated checkpointing only requires each process to maintain one
checkpoint on stable storage, reducing storage overhead.
Straightforward approaches to coordinated checkpointing are the blocking coordinated solutions. In these
approaches an initiator, which may be an application process or an external entity, sends a broadcast message
requesting that the processes checkpoint. Upon receiving this message, each process stops its execution, Ô¨Çushes
all communication channels, and creates its local checkpoint. Then, each process sends an acknowledgment to
the initiator and waits for a commit message before resuming its execution. The initiator will broadcast a commit
message after receiving acknowledgments from all processes. This approach may lead to a signiÔ¨Åcant overhead
in fault-free executions.
To reduce this overhead, non-blocking coordinated solutions have been proposed, the most popular being the
distributed snapshots protocol by Chandy and Lamport [11]. This protocol starts with the initiator broadcasting
a checkpoint request. Upon receiving the request, each process takes its checkpoint and rebroadcasts the request
before sending any more application messages. Message logging is used to deal with in transit messages. The
goal is to ensure that processes do not receive any message that could make the global state inconsistent. However,
in order for this protocol to work, the communication channels need to be FIFO and reliable. For systems with
non-FIFO channels some approaches resort to techniques like piggybacking [12], which causes an unacceptably
high overhead in communication-intensive codes [13].
3. Hybrid Compile Time and Runtime Protocol
This section describes a new coordination protocol for MPI SPMD codes that determines how to perform the
dump of the application state each time an external checkpoint signal is received. The aim is to ensure a consistent
global state, maintaining the eÔ¨Éciency and scalability of uncoordinated solutions while ensuring the progress of
the execution in the presence of faults. One way to achieve this goal is through an application-level solution that
ensures that all processes create checkpoint Ô¨Åles not at the same time, but in particular locations in the code where
it is guaranteed that neither inconsistent nor in transit messages may exist. These locations are called safe points.
Our proposal consists of two phases. In the Ô¨Årst phase, safe points are identiÔ¨Åed during compile time. The second
phase takes place during runtime, when the checkpoint signal arrives, and consists in negotiating which set of safe
points constitutes the closest valid recovery line that has not yet been crossed by any of the processes. We call this
proposal hybrid compile time and runtime protocol.
In the example shown in Figure 2 all the checkpoints have been placed at safe points. In this Ô¨Ågure, ci,x
represents the checkpoint x-th in the process pi and every mk represents a communication between two processes.
Given two checkpoints, ci,x and c j,y , the consistency between them is deÔ¨Åned as a binary relation that is true as
long as there is no exchange of in transit or inconsistent messages between pi and p j in the space separating its
x-th and y-th checkpoints, respectively.
Even at safe points, checkpoints cannot be taken independently by each process in order to prevent the domino
eÔ¨Äect and the creation of useless state Ô¨Åles. Interprocess communication can occur in between two diÔ¨Äerent safe
points, as shown in Figure 3, causing consistency issues. During runtime processes must negotiate to decide the
3 Process

may be forced to rollback up to the beginning of the execution

171

172

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Fig. 2. Checkpoint placement at safe points

safe point in which to dump their states to create a valid recovery line. Henceforth this negotiated location will be
called the dumping point.
A new dumping point is determined each time an external checkpoint signal is received. In our proposal, this
point will be the next safe point to be reached by the process that has advanced the farthest in the execution (see
Figure 4). To determine the dumping point each process communicates to all other processes the last safe point it
has crossed before the checkpoint signal reception.
For the negotiation protocol to be eÔ¨Écient and scalable, the implementation should avoid blocking collective communications, and should try to overlap, as much as possible, execution progress and checkpoint-related
negotiations. The following section describes the implementation details of the proposal.
p1

p2

p3

Fig. 3. Inconsistent global state as a result of performing the checkpoint independently in processes running asynchronously

4. Implementation
The proposed protocol has been implemented on top of the CPPC framework, an application level checkpointing tool that implements eÔ¨Écient, scalable and portable checkpointing mechanisms. An overview of the CPPC
design can be seen in Figure 5. There are two diÔ¨Äerent
Ô¨Ä
phases when using CPPC: a compilation step and an execution step. In the compilation step the CPPC compiler automatically instruments the original application, inserting
calls to the CPPC library as necessary to obtain a fault-tolerant application. During the execution step the application sends checkpoint-related requests to the CPPC library. More information on CPPC and its functionality can
be found in [9].
CPPC identiÔ¨Åes safe points at compile time through a static analysis of messages between processes. Afterwards a heuristic analysis, based on code complexity metrics, selects the safe points that are better suited to
serve as dumping points and inserts checkpoint functions (CPPC Do checkpoint()) accordingly. Every time the

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

p1

p2

p3

Fig. 4. Result of the negotiation to Ô¨Ånd the dumping point

Fig. 5. Integration of a parallel application with the CPPC tool

checkpoint function is called, a CPPC parameter, called touchedCheckpoints, is increased. This parameter is
used to track the number of times that a checkpoint location is called at each process, and it is used for runtime
negotiations as will be subsequently explained.
It is assumed that the mpirun process will be responsible for propagating a checkpoint request to all MPI
processes when the checkpoint must be performed. The arrival of this request triggers a signal handler to proceed
with the negotiation phase.
As the touchedCheckpoints CPPC parameter stores the number of times this function has been called
for each process, a direct and simple solution is to use an MPI reduction operation inside the signal handler to
calculate the maximum touchedCheckpoints value. However, according to the MPI standard, implementations
may prohibit the use of MPI calls from within signal handlers. Thus, for the sake of robustness and portability, an
alternative solution was built outside of the signal handler.
Algorithm 1 shows the pseudocode of the negotiation algorithm. This code is included inside the checkpoint function and executed only when a checkpoint signal has arrived. One-sided MPI communications are
used so that processes may continue running asynchronously during the negotiation. Prior to invoking a onesided MPI communication operation, each process has to specify the memory region (window) that it exposes
to others. The window for the proposed negotiation algorithm comprises two values for each process: touched
and status. The touched value is kept up-to-date throughout the execution to reÔ¨Çect the current value of the
touchedCheckpoints parameter. This value allows each process to ascertain the progress of the remaining processes‚Äô execution, and therefore to determine which one is farthest. The status value is used to indicate the
diÔ¨Äerent
Ô¨Ä
stages of the negotiation process:
‚Ä¢ The process is not yet aware of the ongoing negotiation: The checkpoint signal has not been received, or
it has been received but the process has not reached a checkpoint call yet. In this case status has the

173

174

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Algorithm 1: Negotiation algorithm pseudocode
%p: local process identiÔ¨Åer
for each remote process q do
statusq = -1;
while statusq !=0 do
LockWindow();
GetRemoteWindow(statusq ,touchedq );
UnlockWindow();
if statusq >= touched p then
%dumping point negotiated
status p = statusq ;
return; %go directly to dumping point
else
if touchedq > touched p then
return; %continue to next ckpt call
end
end
end
end
status p =touched p ; %dumping point reached

value of the touchedCheckpoints parameter corresponding to the previous dumping point. For the Ô¨Årst
checkpointing operation in an execution status is initialized to ‚àí1.
‚Ä¢ The process is aware of the ongoing negotiation: It has reached a checkpoint call after a checkpoint signal
arrival. In this case status takes the value 0.
‚Ä¢ The process has Ô¨Ånished the negotiation and continues its normal execution: It has reached the dumping
point. In this case status takes the value of the touchedCheckpoints parameter at the dumping point.
During the negotiation, each process p reads the exposed status and touched values of every other process
q (for loop and GetRemoteWindow() in Algorithm 1). For each process p the for loop iterates from p + 1 to
reduce contention.
If the value of statusq indicates that process q has already reached the dumping point (that is, statusq >
touched p ), then process p stops reading the remote windows and advances straight to the dumping point (indicated by statusq ). Otherwise, p waits for q to put its statusq value to zero, unless q is more advanced than
process p. If q is more advanced than p (i.e. touchedq > touched p ), in order to avoid deadlocks, p advances
until the next checkpoint function, where it will continue reading the remote windows. Note that if p did not advance and process q were waiting for a message from process p sent after the checkpoint call number touched p , q
would be unable to reach the next checkpoint location and put its statusq to zero, coming to a deadlock. Once all
processes are veriÔ¨Åed to be aware of the negotiation process and not more advanced than process p, a consensus
dumping point has been discovered, and process p has arrived at it.
When a process arrives at the dumping point it can begin saving its state without waiting for other processes to
reach this point. In many cases the processes will be synchronized by the nature of the application and they will
reach the dumping point almost simultaneously. However, if the diÔ¨Äerent processes are not synchronized, they
will reach the dumping point at diÔ¨Äerent times, which will improve the checkpoint writing performance.
Local updates to the window values use exclusive locks (MPI LOCK EXCLUSIVE) to guarantee consistency,
whereas remote reads (GetRemoteWindow() operation in Figure 1) use shared locks (MPI LOCK SHARED), which
allow for concurrent read accesses.

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Fig. 6. Measured times

Table 1. Iteration times (in s)

NPB
BT
CG
EP
FT
IS
LU
MG
SP

16
0.83
0.66
0.00
2.52
0.46
0.54
0.75
0.64

32
0.37
0.37
0.00
1.87
0.38
0.26
0.40
0.32

64
0.22
0.22
0.00
1.19
0.25
0.12
0.21
0.14

128
0.12
0.17
0.00
0.68
0.16
0.07
0.14
0.08

256
0.07
0.18
0.00
0.46
0.11
0.05
0.10
0.06

5. Experimental Results
A multicore cluster, the Finis Terrae supercomputer, was used to evaluate our proposal. It consists of 142 HP
RX7640 nodes, each of them with 16 IA64 Itanium2 Montvale cores at 1.6 Ghz, 128 GB of memory and a dual
4X InÔ¨ÅniBand port. The working directory used for storing checkpoint Ô¨Åles is connected to the cluster through
the InÔ¨Åniband network and it consists of disks of 250GB conÔ¨Ågured in RAID 6 with an aggregate write speed of
6 GB/s.
The application testbed was comprised of the eight applications in the MPI version of the NAS Parallel Benchmarks v3.1 [14] (NPB from now on). These are well-known and widespread applications that provide a de-facto
test suite. All benchmarks were run using class C with 16, 32, 64, 128 and 256 processes (BT and SP with 16, 36,
64, 121 and 256 processes as they need a square number of processes).
The experiments can be divided into three blocks. The Ô¨Årst block analyzes the negotiation time, that is, the
time consumed by the negotiation algorithm. The second one analyzes the response time, that is, the time from
the signal reception to the actual checkpoint dumping. This time includes the time spent executing the application
until the dumping point is reached (see Figure 6). The last block measures the overhead introduced by the proposed
protocol.
All benchmarks execute one CPPC Do checkpoint() function call in each iteration of their main computational loop. Table 1 shows the iteration times for diÔ¨Äerent number of processes. All the results in this section
were obtained after performing at least 10 experiments per NPB application. Average times are reported. In
all experiments the checkpoint signal was sent randomly to the mpirun process by a external script. The MPI
implementation used was MVAPICH2-1.7 [15] built with the default conÔ¨Åguration.
5.1. Negotiation time
The negotiation time is the sum of the times consumed by all the CPPC Do checkpoint() calls executed from
the moment the signal is received until the dumping point is reached. As shown in Figure 6, the number of times

175

176

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Table 2. Negotiation time (in s)

NPB
BT
CG
EP
FT
IS
LU
MG
SP

16 processes
t¬Ø
œÉ(t)
0.01 0.01
0.00 0.00
0.01 0.00
0.51 0.50
0.02 0.00
0.03 0.00
0.06 0.02
0.01 0.00

32 processes
t¬Ø
œÉ(t)
0.01 0.00
0.01 0.00
0.03 0.01
0.51 0.07
0.09 0.01
0.02 0.01
0.03 0.00
0.01 0.00

64 processes
t¬Ø
œÉ(t)
0.02 0.00
0.01 0.00
0.07 0.01
0.27 0.04
0.05 0.00
0.02 0.00
0.02 0.00
0.02 0.00

128 processes
t¬Ø
œÉ(t)
0.21
0.01
0.23
0.01
0.30
0.05
0.19
0.06
0.04
0.00
0.26
0.02
0.26
0.04
0.20
0.01

256 processes
t¬Ø
œÉ(t)
0.68
0.05
0.65
0.09
0.81
0.10
0.12
0.01
0.09
0.04
0.75
0.08
0.80
0.05
0.70
0.11

a process needs to call CPPC Do checkpoint() depends on the point of the execution it is when the signal is
received. Table 2 shows the average and standard deviation for the maximum negotiation time for each execution.
The actual checkpoint Ô¨Åle creation was disabled during these experiments to avoid network overload. Note that
in the Finis Terrae the network used to send MPI messages is the same one used to store the checkpoint Ô¨Åles. In
general, negotiation times tend to increase when increasing the number of processes due to the higher number of
processes negotiating at the same time. The diÔ¨Äerences observed among the applications are mainly due to the
implementation of lock/unlock synchronizations in the MPI library. In the conÔ¨Åguration of MVAPICH used, the
target of a remote operation must enter the MPI library to progress lock/unlock calls. This causes, for instance,
a decrease in the negotiation time of the FT application as the number of processes increases, as MPI calls are
more frequent in this situation. If extra calls4 to MPI functions are introduced in the FT source code, negotiation
times are reduced signiÔ¨Åcantly as can be seen in Table 3. In any case, results show that the negotiation times are
practically negligible for all the applications. The negotiation process consumes less than one second in all the
experiments.
Table 3. Negotiation time (in s) with extra MPI functions calls in the FT benchmark source code

NPB
FT

16 processes
œÉ(t)
t¬Ø

32 processes
œÉ(t)
t¬Ø

64 processes
œÉ(t)
t¬Ø

128 processes
œÉ(t)
t¬Ø

256 processes
œÉ(t)
t¬Ø

0.02

0.12

0.26

0.15

0.12

0.00

0.02

0.05

0.00

0.01

5.2. Response time
The response time (T r) is deÔ¨Åned as the time from the checkpoint signal reception until the moment the
dumping point is reached (Figure 6 shows it graphically). As commented previously, all benchmarks execute one
CPPC Do checkpoint() function call per iteration. Thus, this time will depend on the time per iteration (T i) (see
Table 1) and the time spent on the negotiation algorithm (T n) (see Table 2).
Table 4 shows the average and standard deviation of the maximum response time for each execution. Actual
checkpoint Ô¨Åle creation was disabled to avoid network overload. Response times depend in great measure on the
moment the checkpoint signal is received. Thus, the standard deviation values are larger for these experiments.
All the response times are between T n and T n + T i. Furthermore, the average response time can be approximated
by T i/2 + T n, which means that only one iteration is needed to achieve the dumping point. As the number of
processes increases the time per iteration decreases and, consequently, the response time decreases in turn until it
comes to be dominated by the negotiation time.
5.3. Overhead
The total execution times were measured to analyze the overhead introduced by the proposed protocol. Figure 7 shows the average execution times normalized with respect to the original execution times (without CPPC)
4 MPI

Iprobe calls were introduced in the code for these experiments

177

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Table 4. Response time (in s)

NPB
BT
CG
EP
FT
IS
LU
MG
SP

16 processes
t¬Ø
œÉ(t)

32 processes
t¬Ø
œÉ(t)

64 processes
t¬Ø
œÉ(t)

128 processes
t¬Ø
œÉ(t)

256 processes
t¬Ø
œÉ(t)

0.48
0.45
0.00
1.74
0.22
0.30
0.46
0.30

0.21
0.19
0.08
1.48
0.21
0.15
0.19
0.14

0.11
0.11
0.09
0.74
0.16
0.06
0.14
0.09

0.26
0.33
0.30
0.57
0.13
0.28
0.33
0.24

0.73
0.72
0.81
0.26
0.11
0.78
0.84
0.74

0.21
0.17
0.02
0.65
0.11
0.15
0.28
0.23

0.09
0.13
0.01
0.41
0.07
0.09
0.13
0.10

0.06
0.08
0.01
0.29
0.08
0.02
0.10
0.04

0.03
0.05
0.05
0.33
0.00
0.04
0.05
0.02

0.06
0.12
0.11
0.10
0.04
0.08
0.05
0.11

with 128 processes. Bar Negotiation shows the normalized times using CPPC and the negotiation algorithm to
create the checkpoint Ô¨Åles but without writing them to disk. Bar Checkpointing shows the normalized execution
times writing one checkpoint Ô¨Åle per process. To assist in the analysis of the results, the average execution times
of the original version were included (line labeled Execution time). The normalized overhead is high for applications with short execution times, but the absolute overhead is always small (less than 6 seconds). Note that the
execution times are quite short for all the benchmarks, which contributes to increase the overhead percentage.
&!'()
"
	#$%

*!'+"
"
,!#-%

































*!'+"
"
,!#-%

.,/
0!1!*!'+"
"
,!#$%

!	"
"
#$%




















!'&,.(

Fig. 7. Overhead introduced by the hybrid protocol using 128 processes

6. Conclusions
A light and asynchronous protocol has been designed to achieve global consistent checkpointing avoiding,
when possible, operations that lead to execution stalls in the processes execution. The proposal consists of two
phases. In the Ô¨Årst one safe points in the code are identiÔ¨Åed at compile time. In the second phase a negotiation
algorithm decides where to dump the application state.
The implementation of the runtime negotiation avoids blocking communications and overlaps, as much as
possible, the progress in the execution with the coordination. The experimental validation performed has shown
the low overheads and scalability of the proposal.
The hybrid coordination protocol has been implemented in the CPPC checkpointing framework and, therefore, at the application level. In this way, it is independent of the hardware architecture, the OS or the MPI
implementation used.

178

Iv√°n Cores et al. / Procedia Computer Science 18 (2013) 169 ‚Äì 178

Acknowledgments
This research was supported by the Ministry of Science and Innovation of Spain (Project TIN2010-16735) and
by the Galician Government (Project 10PXIB105180PR). We gratefully thank CESGA (Galicia Supercomputing
Center, Santiago de Compostela, Spain) for providing access to the Finis Terrae supercomputer.
References
[1] A. Iosup, M. Jan, O. Sonmez, D. H. J. Epema, On the dynamic resource availability in grids, in: Proceedings of the 8th IEEE/ACM
International Conference on Grid Computing (GRID ‚Äô07), IEEE Computer Society, Washington, DC, USA (2007) 26‚Äì33.
[2] E. Elnozahy, L. Alvisi, Y.-M. Wang, D. Johnson, A survey of rollback-recovery protocols in message-passing systems, ACM Computing
Surveys 34 (3) (2002) 375‚Äì408.
[3] Y. Chen, J. Plank, K. Li, CLIP: A checkpointing tool for message-passing parallel programs, in: Proceedings of the 1997 ACM/IEEE
Conference on Supercomputing (SC‚Äô97), (1997) 33.
[4] J. Hursey, J. M. Squyres, T. I. Mattox, A. Lumsdaine, The design and implementation of checkpoint/restart process fault tolerance for
Open MPI, in: Proceedings of the International Parallel and Distributed Processing Symposium (IPDPS‚Äô07) (2007) 1‚Äì8.
[5] G. Stellner, Cocheck: Checkpointing and process migration for mpi, in: Proceedings of the 10th International Parallel Processing
Symposium (IPPS‚Äô96) IEEE Computer Society, Washington, DC, USA (1996) 526‚Äì531.
[6] M. Schulz, G. Bronevetsky, R. Fernandes, D. Marques, K. Pingali, P. Stodghill, Implementation and evaluation of a scalable applicationlevel checkpoint-recovery scheme for mpi programs, in: Proceedings of the 2004 ACM/IEEE conference on Supercomputing (SC ‚Äô04),
IEEE Computer Society, Washington, DC, USA (2004) 38.
[7] E. Elnozahy, J. Plank, Checkpointing for peta-scale systems: a look into the future of practical rollback-recovery, IEEE Transactions on
Dependable and Secure Computing 1 (2) (2004) 97‚Äì108.
[8] G. Rodr¬¥ƒ±guez, M. Mart¬¥ƒ±n, P. Gonz¬¥alez, J. TouriÀúno, A heuristic approach for the automatic insertion of checkpoints in message-passing
codes, Journal of Universal Computer Science 15 (14) (2009) 2894‚Äì2911.
[9] G. Rodr¬¥ƒ±guez, M. Mart¬¥ƒ±n, P. Gonz¬¥alez, J. TouriÀúno, R. Doallo, CPPC: A compiler-assisted tool for portable checkpointing of messagepassing applications, Concurrency and Computation: Practice and Experience 22 (6) (2010) 749‚Äì766.
[10] R. E. Strom, S. Yemini, Optimistic recovery in distributed systems, ACM Transactions on Computer Systems 3 (1985) 204‚Äì226.
[11] K. Chandy, L. Lamport, Distributed Snapshots: Determining Global States of Distributed Systems, ACM Trans. Comput. Syst. 3 (1)
(1985) 63‚Äì75.
[12] G. Bronevetsky, D. Marques, K. Pingali, P. Stodghill, Automated application-level checkpointing of MPI programs, in: Proceedings of
the 2003 ACM Symposium on Principles and Practice of Parallel Programming (PPoPP‚Äô03) (2003) 84‚Äì94.
[13] M. Schulz, G. Bronevetsky, B. Supinski, On the performance of transparent MPI piggyback messages, in: A. Lastovetsky, T. Kechadi,
J. Dongarra (Eds.), Recent Advances in Parallel Virtual Machine and Message Passing Interface, Vol. 5205 of Lecture Notes in Computer
Science, Springer Berlin Heidelberg (2008) 194‚Äì201.
[14] National Aeronautics and Space Administration, The NAS Parallel Benchmarks, http://www.nas.nasa.gov/Software/NPB, last
accessed December 2012.
[15] MVAPICH: MPI over InÔ¨ÅniBand, 10GigE/iWARP and RoCE, http://mvapich.cse.ohio-state.edu/, last accessed December
2012.

