Procedia Computer Science
Volume 80, 2016, Pages 1497‚Äì1506
ICCS 2016. The International Conference on Computational
Science

Online MPI Trace Compression using Event Flow Graphs
and Wavelets
Xavier Aguilar1 , Karl F¬®
urlinger2 , and Erwin Laure1
1

KTH Royal Institute of Technology,
Department for Computational Science and Technology (CST)
and Swedish e-Science Research Center (SeRC),
Lindstedv¬®
agen 5, 10044 Stockholm, Sweden
xaguilar@pdc.kth.se, erwinl@pdc.kth.se
2
Ludwig-Maximilians-Universit¬®
at (LMU) Munich,
Computer Science Department, MNM Team,
Oettingenstr. 67, 80538 Munich, Germany
fuerling@nm.ifi.lmu.de

Abstract
Performance analysis of scientiÔ¨Åc parallel applications is essential to use High Performance
Computing (HPC) infrastructures eÔ¨Éciently. Nevertheless, collecting detailed data of largescale parallel programs and long-running applications is infeasible due to the huge amount
of performance information generated. Even though there are no technological constraints in
storing Terabytes of performance data, the constant Ô¨Çushing of such data to disk introduces
a massive overhead into the application that makes the performance measurements worthless.
This paper explores the use of Event Ô¨Çow graphs together with wavelet analysis and EZWencoding to provide MPI event traces that are orders of magnitude smaller while preserving
accurate information on timestamped events. Our mechanism compresses the performance data
online while the application runs, thus, reducing the pressure put on the I/O system due to
buÔ¨Äer Ô¨Çushing. As a result, we achieve lower application perturbation, reduced performance
data output, and the possibility to monitor longer application runs.
Keywords: MPI performance monitoring, event Ô¨Çow graphs, trace compression, wavelets, EZW coding

1

Introduction

High Performance Computing (HPC) infrastructures provide an enormous amount of computational resources, however, this computational power comes with an increase in software
complexity. Applications have to be tuned to highly-complex underlying hardware, combine
several programming models such as MPI, or OpenMP, and utilize accelerator speciÔ¨Åc directives
to exploit parallelism. Therefore, performance analysis and optimization is an essential task
during application development.
Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.471

1497

Online MPI Trace Compression using EFGs and Wavelets

Aguilar X., F¬®
urlinger K., and Laure E.

Performance analysis tools assist developers in the task of understanding application behavior as well as Ô¨Ånding program bottlenecks on complex hardware conÔ¨Ågurations. Such tools
follow mainly two approaches when monitoring the performance of an application. Either they
intercept speciÔ¨Åc application events at runtime or they sample the state of the application periodically. This performance information can be presented in the form of reports with aggregated
statistics or log Ô¨Åles with events (or samples) ordered in time with timestamps. Even though
performance aggregates have a small memory footprint, they lack the temporal information
about the monitored events, and therefore, they prevent the detection of certain performance
issues such as the late sender problem. In contrast, event traces contain every single event performed by the application with corresponding timestamps, capturing thereby a detailed picture
of the application‚Äôs performance behavior. Nevertheless, collecting Ô¨Åne-grained event traces
naively is infeasible for large-scale runs due to the perturbation caused in the application from
the excessive I/O use.
In this paper we address three strongly interrelated challenges commonly occurring when
monitoring large-scale long-running parallel applications: large data volumes produced, large
bias introduced into the application due to tool perturbation when Ô¨Çushing data buÔ¨Äers to disk,
and considerably application slow down.
In order to lessen these three problems, we evaluate the use of event Ô¨Çow graphs together
with discrete wavelet transformations and Embedded Zerotree Wavelet (EZW) encoding to
compress performance data online while it is generated. Thereby, reducing considerably the
pressure on the I/O system, minimizing application interruption due to buÔ¨Äer Ô¨Çushing, reducing
the bias introduced in the measurements, and allowing the monitoring of longer runs.
The remainder of this paper is organized as follows: Section 2 provides background on our
previous work on event Ô¨Çow graphs. Section 3 describes our trace compression mechanism and
gives a brief introduction to wavelet theory and EZW encoding. Section 4 presents several
experiments performed to test our approach. Section 5 surveys related work. Finally, Section
6 discusses conclusions and future work.

2

Event Flow Graphs

An event Ô¨Çow graph of an MPI process is a weighted directed graph in which nodes are the MPI
calls executed by the process, and edges are the transitions between such calls. In other words,
graph edges represent the code executed between two MPI calls within the process. Event Ô¨Çow
graphs capture the execution Ô¨Çow of applications, maintaining the inherent order of execution
among events. Thus, graphs can be used to reconstruct the full sequence of events performed
by the program without storing any explicit timing information.
Event Ô¨Çow graphs have many uses in the context of performance monitoring and analysis.
In [2] the authors investigated the use of graphs for trace compression (without timestamps),
achieving compression ratios up to 120x when storing event traces in the form of event Ô¨Çow
graphs. In [3], event Ô¨Çow graphs were used to detect the loop structure of MPI iterative
applications. Finally, the authors explored in [4] several techniques to include event Ô¨Çow graphs
in the process of performance analysis. For instance, using graph cycle detection and coloring
together with performance metrics to show the most time-consuming parts of an application to
the user.
1498

Online MPI Trace Compression using EFGs and Wavelets

3

Aguilar X., F¬®
urlinger K., and Laure E.

Wavelet Transform and EZW-coding for Trace Compression

A limitation of our previous work in trace compression [2] was the fact that graphs could
not encode continuous numerical data such as timestamps, and therefore, we were not able to
reconstruct timestamped traces from our event Ô¨Çow graphs.
In this paper, we have extended our event Ô¨Çow graph framework in IPM, a lightweight MPI
proÔ¨Åler, to include compression of numerical data using wavelets and the Embedded Zerotree
Wavelet (EZW) encoding. This new framework is able to collect detailed MPI performance
data, i.e., MPI events and their corresponding timestamps, and compress them online while
the application runs. Thereby, we reduce considerably the number of Ô¨Çushes to disk required,
obtaining as a result less application perturbation due to I/O operations. In addition, we
generate traces orders of magnitude smaller, and we are able to monitor longer runs.
Our performance data compression algorithm divides the data in two groups. On one hand,
the events, which are encoded with event Ô¨Çow graphs as described in [2]. On the other hand,
the numerical information, i.e., the event timestamps (encoded as a sequence of Ô¨Çoats) that
are compressed with wavelets and EZW, followed by run-length encoding, and HuÔ¨Äman encoding [13]. Due to space constrains, we have to reduce to a minimum the overview on wavelets
and EZW-encoding. Giving only the essential information to understand our mechanism the
reader is referred to a comprehensive bibliography on these topics [18, 9].
In brief, the wavelet transform decomposes a signal and concentrates its information on a
small set of coeÔ¨Écients with large magnitudes. These large-magnitude coeÔ¨Écients contain more
energy than small ones, and thus, they are more important to the reconstruction quality of the
signal. The EZW algorithm uses this wavelet multi resolution representation to predict the lack
of information across scales and encode the data in a hierarchical manner. One of the main
advantages of the algorithm is that the accuracy of the compressed data can be deÔ¨Åned with
the number of passes that the EZW algorithm performs over the data. This number of passes
is customizable by the user within our framework.
Our implementation is based in a modiÔ¨Åed version of the Shapiro EZW algorithm [20] that
works with 1D signals together with biorthogonal CDF 9/7 wavelets, using the fast wavelet
transform with lifting. The whole encoding task is performed locally per process every time the
memory buÔ¨Äer for timestamps is full, and does not require any communication at all.
This compression process can afterwards be inverted to obtain an approximation of the
original signal before compression. First, applying HuÔ¨Äman decoding, then run length decoding,
EZW decoding, and Ô¨Ånally applying the inverse wavelet transform to the uncompressed data.

4

Experiments

To evaluate the presented mechanism we used two in-production MPI applications, Gromacs [12]
and MILC [8]. We run the codes in a Cray XC40 system based in Intel Haswell processors and
Cray Aries interconnect. Each node had 32 cores in 2 sockets, with a total memory of 64 GB.
Gromacs is a molecular dynamics package that simulates the Newtonian equations of motion
for systems with hundreds to millions of particles. While designed primarily for biochemical
molecules such as proteins, lipids, and nucleic acids, its fast calculation of non-bonded interactions makes Gromacs suitable for non-biological systems as well. MILC is a set of codes
developed by the MIMD Lattice Computation (MILC) collaboration to study quantum chromodynamics (QCD). It performs simulations of four SU(3) lattice gauge theory on MIMD parallel
1499

Online MPI Trace Compression using EFGs and Wavelets

Aguilar X., F¬®
urlinger K., and Laure E.

machines. In the experiments, we used IPM to monitor the application and generate all the
performance data, either regular event traces or our new compressed trace approach with Ô¨Çow
graphs and wavelets.

4.1

Overhead

We performed scaling runs of Gromacs and MILC up to 2048 cores, and measured the amount
of overhead introduced by our mechanism. This overhead includes intercepting MPI calls,
building event Ô¨Çow graphs, generating signals from the timestamps collected, compressing such
signals, and Ô¨Ånally, writing all the data to disk, i.e., graphs and compressed signals. The wavelet
transform used had 5 levels of recursion, and the EZW algorithm stopped after 8 and 16 passes.
Figure 1 shows the percentage of overhead introduced over total application time. As it
can be seen, the overhead in Gromacs is small, never surpassing 3.8%. In contrast, MILC has
an extremely high frequency of MPI calls that trigger our compression mechanism very often,
making it unfeasible due to the amount of overhead introduced (from 7% to 20%). Therefore,
we have added the possibility in each MPI process to have an additional worker thread that
compresses the data in parallel to the monitoring of the application. The overhead introduced
in MILC with this new approach is depicted in Figure 1. As it can be seen, overlapping the
data compression with the normal execution of the application reduces the overhead to less than
4.8% (with almost no error in the compressed data as we will see in the following sections).

4.2

Data Compression

This section presents a comparison between the volume of data generated with the original
tracing of IPM against our compression mechanism. As we are evaluating the EZW algorithm
implemented, and due to space restrictions, we only show the compression ratios achieved
over the signals that we compress, that is, the timestamps collected from the application.
Compression of events with event Ô¨Çow graphs is further explored in [2].
In this experiment we used the 1024 cores test-case for Gromacs, and a test case with
128 cores for MILC. In the case of MILC, 128 cores were enough to evaluate the compression
algorithm since the volume of data generated was quite big already (50 GB). Initially, the traces
created by IPM were ASCII Ô¨Åles whereas our compressed traces are binary. Therefore, in order
to have a much fairer comparison, we extended IPM to generate binary traces too.
As previously mentioned in Section 3, the EZW algorithm allows to trade compression for
precision by adjusting the number of passes that the algorithm performs. Less algorithm passes
leads to higher compression ratios but higher error. We deÔ¨Åne compression ratio as the size
of the original Ô¨Åle divided by the size of the compressed version. Figure 2 shows the ratios
achieved by our algorithm when varying the number of EZW passes from 1 to 64. The Ô¨Årst
passes of the algorithm generate very compact data, having for Gromacs a compression ratio of
86:1, and a ratio of 23:1 for MILC. As we increase the number of EZW passes, the compressed
Ô¨Åle grows in size, having with 8 EZW passes a ratio of 9:1 for Gromacs and a ratio of 18:1 for
MILC. In terms of Ô¨Åle size, for Gromacs we only need 250MB to store 2.3GB of uncompressed
data. In the case of MILC, the compressed data is 1.1 GB against 20 GB of uncompressed
timings. The compression ratio decreases until 28 EZW passes where it becomes stable. Being
2.3:1 for Gromacs and 2:1 for MILC. This entails that with the maximum precision possible in
our data, i.e., having almost an exact copy of the original data, we still can reduce the size of
it by half.
1500

Online MPI Trace Compression using EFGs and Wavelets








	 !

 !
"#	 !$
"#
 !$









	







	










	

























Figure 1: Percentage of overhead.

4.3


	





	





Aguilar X., F¬®
urlinger K., and Laure E.

Figure 2: Compression vs EZW passes.

Quantitative Evaluation of the Trace Reconstruction

The encoding process performed by our algorithm can be reversed to obtain an approximation of
the original trace. These reconstructed traces contain certain amount of error introduced mainly
from three diÔ¨Äerent sources. First, the rounding error in the CDF 9/7 transform. Second, the
quantization error in the encoding process, and third, the number of passes performed by the
EZW algorithm. The number of EZW passes can be adjusted by the user, however, the other
two sources of error are unavoidable. Nevertheless, their contribution to the total error is small,
being the number of EZW passes the main factor of reconstruction quality.
In order to asses the amount of error between an original trace and its reconstructed version,
we use the percentage root-mean-square diÔ¨Äerence (PRD) over their timestamps. This error
estimate is one of the most frequently used in other scientiÔ¨Åc research Ô¨Åelds such as ECG signal
compression. However, as mentioned in [5], the PRD estimate is heavily inÔ¨Çuenced by the mean
value of the signal, it is thus more appropriate to use a modiÔ¨Åed version of it, which is

P RD1 =

N
n=1 (x(n)
N
n=1 (x(n)

‚àíx
ÀÜ(n))2
‚àíx
¬Ø(n))2

√ó 100

(1)

where x(n) is the real value, x
ÀÜ(n) the reconstructed value, and x
¬Ø(n) the mean of the original
signal.
Figure 3 shows the average, maximum and minimum PRD1 values for Gromacs (1024 cores)
and for MILC (128 cores), respectively. As the compression is performed locally per MPI task,
we can calculate the PRD1 per MPI process and show average, minimum and maximum values
for the whole application, as displayed in the plot. The average is the central horizontal line
in each point, and the maximum and minimum values are the whiskers displayed around such
line. The Ô¨Ågure shows that for the Ô¨Årst passes of the EZW algorithm, the error is quite high,
being 76% for Gromacs and 90% for MILC. As we increase the number of EZW passes, the
error decreases quickly. For Gromacs the error goes from 76% to around 1% in the Ô¨Årst 8 EZW
passes. Then it continues approaching zero until its average stabilizes at 1 √ó 10‚àí5 % after 28
passes. With MILC the error also decreases quickly in the Ô¨Årst 8 passes, passing from 90% to
0.6%. Its average remains stable at 3.6 √ó 10‚àí7 % after 28 passes. Correlating this error with the
compression ratios previously seen, we achieve a 9:1 compression ratio for Gromacs with only
1.4% of error. With MILC the compression ratio is even higher and the error smaller. It has a
compression ratio of 18:1 with only a 0.6% of error.
1501

Online MPI Trace Compression using EFGs and Wavelets



















	
	
	

	








Aguilar X., F¬®
urlinger K., and Laure E.




	













	














(a) Gromacs
















(b) MILC

Figure 3: Percentage Root-Mean-Square DiÔ¨Äerence (PRD1) vs EZW passes.

Original
Reconstructed

Avg.
25.78
25.99

Max.
85.36
85.38

Idle
Min.
1.70
1.71

Std. Dev
34.12
34.13

Avg.
74.02
74.01

Running
Max. Min.
98.30 14.64
98.29 14.62

Std. Dev
34.12
34.13

Table 1: Statistics on the idle and running states in Gromacs. Values are in percentage.

4.4

Qualitative Evaluation of the Trace Reconstruction

In this section, we evaluate if one original and one reconstructed trace capture the same application behavior for performance analysis. With this aim, we wrote a trace converter to be able
to analyze our IPM traces in Paraver, a powerful trace visualizer.
We utilized Paraver to analyze two traces of Gromacs running in 128 cores. One of the
traces was an original uncompressed trace generated by IPM, and the other, a reconstruction
from a compressed version of such original trace. In other words, we took the original trace,
compressed it post-mortem with our algorithm and then decompressed it again. We followed this
process to avoid deviations caused by generating traces from two diÔ¨Äerent runs. For instance,
run variability, and even more important, the fact that the application behaves diÔ¨Äerently if we
perform normal tracing or our compression mechanism. The main reason is that the monitoring
noise is diÔ¨Äerent due to the diÔ¨Äerent behavior in buÔ¨Äer Ô¨Çushing.
Figure 4 shows three timelines depicting half a second of execution of Gromacs (Y-axis
processes and X-axis time). Figure 4a is the original trace, Ô¨Ågure 4b is a reconstructed trace
using 16 EZW passes, and Ô¨Ågure 4c is another trace reconstructed using 12 EZW passes. As
it can be seen in the picture with Ô¨Ågure 4c, the traces degrade as we introduce more error in
the compression, that is, as we use a smaller number of EZW passes. Nevertheless, it can be
seen that it is almost impossible to see any visual diÔ¨Äerence between the original trace and the
trace reconstructed using 16 EZW passes. Even at this level of zoom, where events happen
with nanoseconds of diÔ¨Äerence. We continue our analysis from now on only with the original
trace and the trace with 16 EZW passes.
Table 1 shows various metrics about the state of the MPI processes for both traces. It shows
the average, maximum, minimum, and standard deviation percentage of total computing and
idle time across MPI tasks. The table shows that both traces have in general the same behavior,
being the percentages almost the same for both trace versions.
Now we are interested in comparing if the time between events is similar in both traces,
1502

Online MPI Trace Compression using EFGs and Wavelets

Aguilar X., F¬®
urlinger K., and Laure E.

(a) Original trace.

(b) 16 EZW-passes reconstructed trace.

(c) 12 EZW-passes reconstructed trace.

Figure 4: Timeline zooms for one original trace and two reconstructions.

(a) Real trace.

(b) Trace reconstructed with 16 EZW passes.

Figure 5: 2D histogram on the distribution of computational bursts in Gromacs.

that is, the computation between MPI calls and the MPI call duration. Figure 5 shows a 2D
histogram with the distribution of computational times across MPI ranks. By computational
time we refer to a burst of computation between two MPI calls. Every row in the Y-axis is an
MPI task and every column in the X-axis is a range of burst duration. Cells in the left of the
plot correspond to shorter computational bursts whereas cells in the right are longer bursts.
The histogram cells are colored (from green to blue) regarding the amount of total time spent
in that speciÔ¨Åc range. In brief, the plot depicts for each process the distribution of computation
times. As we can see in the picture, both traces share the same distribution of times. In other
words, the distances between their MPI events are similar.
Finally, we perform the same analysis with the MPI calls. We check that the distribution
1503

Online MPI Trace Compression using EFGs and Wavelets

(a) Real trace.

Aguilar X., F¬®
urlinger K., and Laure E.

(b) Trace reconstructed with 16 EZW passes.

Figure 6: 2D histogram on the distribution of MPI call duration in Gromacs.
of times in MPI is the same for both traces. As it can be seen in Ô¨Ågure 6, MPI call duration
and their distribution is also very similar in both traces.
In summary, we have seen that the reconstructed trace behaves as the original trace, that is,
it would allow us to extract the same analysis conclusions than with the real trace. In addition,
the volume of compressed data that we need to store to reconstruct such a trace is almost 7
times smaller than the original trace.

5

Related Work

The huge performance data volumes generated by massive parallel applications is one of the
main challenges that tools developers face nowadays. Therefore, several tool frameworks include
data reduction techniques.
Score-P [16] and CrayPat [10], can Ô¨Ålter out highly frequent small functions. Other tracing
tools such Extrae, as well as Score-P, can turn on and oÔ¨Ä the tracing by inserting certain hooks
in the code. Extrae also allows to use burst clustering in order to detect application structure
online and trace only a few iterations, limiting thereby the size of the trace [17]. Our approach
does not Ô¨Ålter out any data but compress everything that the monitoring system produces.
In addition, it does not require any previous knowledge about the application as some of the
previous methodologies.
Other approaches are aimed at trace compression of parallel applications. ScalaTrace [19]
provides on-the-Ô¨Çy lossless trace compression of MPI event traces. Kn¬®
upfer et al. [15] uses Complete Call Graphs (CCGs) to compress post-mortem recurring patterns in traces. In contrast,
our approach provides online lossy compression of numerical data (timestamps) at a Ô¨Åne-grain
level
Previous research in performance monitoring has used wavelets together with performance
data. Casas et al. [6] used post-mortem wavelet analysis with trace data to detect application
structure and select a few iteration from the trace. This approach, however, requires the full
uncompressed trace and is performed post-mortem, whereas our method generates directly
a compressed trace and is performed online. The work of Gamblin et al. [11] is the closest
work to ours. The authors use the discrete wavelet transform with EZW-encoding to compress
1504

Online MPI Trace Compression using EFGs and Wavelets

Aguilar X., F¬®
urlinger K., and Laure E.

performance data collected from deÔ¨Åned regions within an application. The main diÔ¨Äerence
with our approach is how the compression is performed. Their mechanism compresses the
collected data at the end of the run, whereas ours is executed several times during application
execution, and therefore, is best suited for long application runs. In addition, our algorithm
is performed locally on each process without any communication involved. In any case, both
works demonstrate that the use of wavelets can be very beneÔ¨Åcial in the Ô¨Åeld of performance
monitoring.
Wavelets have been widely used in other research areas such as the medical Ô¨Åeld, digital imaging, or engineering. In the medical Ô¨Åeld, wavelets are extensively used to compress
biomedical signals, e.g., electrocardiogram (ECG) signals [14, 22, 1]. In image processing and
analysis, wavelets are used for denoising pictures [7], and image compression, among other
things. Wavelets and the EZW-encoding are part of the JPEG-2000 standard [21].

6

Conclusions and Future Work

Understanding the performance behavior of parallel applications is essential in order to use
high-performance computer infrastructures eÔ¨Éciently. However, the amount of performance
data that can be generated from those massively parallel machines makes such performance
analysis infeasible.
In this paper, we presented a new approach that combines event Ô¨Çow graphs together with
wavelet transforms and EZW-coding to compress performance traces online while the application runs. Our methodology has various advantages. First, by compressing data in memory
while the application runs, we diminish the number of data Ô¨Çushes to disk. Thereby, we reduce
the perturbation introduced into the application due to costly I/O operations. And second, we
generate timestamped traces orders of magnitude smaller, allowing us to monitor applications
at a Ô¨Åne-grain level for a longer period of time.
We demonstrated our approach with two in-production applications: Gromacs and MILC.
We showed that our framework is capable of eÔ¨Éciently compressing the data of these two large
applications while they run. In addition, we proved using a performance visualizing tool that
our reconstructed traces capture the same behavior than an uncompressed original trace.
We want to couple this new tracing approach with our previous research on automatic detection of application structure with event Ô¨Çow graphs. Thereby, we could monitor applications
without any constrains on the job duration. Iterative applications usually exhibit repetitive
performance behavior, and thus, they produce a lot of redundant performance data. By using
our structure detection mechanism we could Ô¨Ålter out those redundant phases and only apply
the wavelet compression when the application suÔ¨Äers a change in its performance. Furthermore,
we want to take advantage of the structure detection to automatically adjust the number of
EZW passes used across program iterations to Ô¨Åt error thresholds deÔ¨Åned by the user.

References
[1] Mohammed Abo-Zahhad. ECG signal compression using discrete wavelet transform. DISCRETE
WAVELET TRANSFORMS THEORY AND APPLICATIONS, page 143, 2011.
[2] Xavier Aguilar, Karl F¬®
urlinger, and Erwin Laure. MPI trace compression using event Ô¨Çow graphs.
In Euro-Par 2014 Parallel Processing, pages 1‚Äì12. Springer, 2014.
[3] Xavier Aguilar, Karl F¬®
urlinger, and Erwin Laure. Automatic on-line detection of MPI application
structure with event Ô¨Çow graphs. In Euro-Par 2015: Parallel Processing, pages 70‚Äì81. Springer,
2015.

1505

Online MPI Trace Compression using EFGs and Wavelets

Aguilar X., F¬®
urlinger K., and Laure E.

[4] Xavier Aguilar, Karl F¬®
urlinger, and Erwin Laure. Visual MPI performance analysis using event
Ô¨Çow graphs. Procedia Computer Science, 51:1353‚Äì1362, 2015.
[5] Manuel Blanco-Velasco, Fernando Cruz-Rold¬¥
an, J Ignacio Godino-Llorente, Joaqu¬¥ƒ±n BlancoVelasco, Carlos Armiens-Aparicio, and Francisco L¬¥
opez-Ferreras. On the use of prd and cr parameters for ECG compression. Medical Engineering & Physics, 27(9):798‚Äì802, 2005.
[6] Marc Casas, Rosa M Badia, and Jes¬¥
us Labarta. Automatic phase detection and structure extraction of MPI applications. International Journal of High Performance Computing Applications,
24(3):335‚Äì360, 2010.
[7] S Grace Chang, Bin Yu, and Martin Vetterli. Adaptive wavelet thresholding for image denoising
and compression. Image Processing, IEEE Transactions on, 9(9):1532‚Äì1546, 2000.
[8] The MIMD Lattice Computation (MILC) Collaboration. Milc code version 7, 2013. http://www.
physics.utah.edu/~detar/milc/milc_qcd.html.
[9] Ingrid Daubechies. The wavelet transform, time-frequency localization and signal analysis. Information Theory, IEEE Transactions on, 36(5):961‚Äì1005, 1990.
[10] Luiz DeRose, Bill Homer, Dean Johnson, Steve Kaufmann, and Heidi Poxon. Cray performance
analysis tools. In Tools for High Performance Computing, pages 191‚Äì199. Springer, 2008.
[11] Todd Gamblin, Bronis R De Supinski, Martin Schulz, Rob Fowler, Daniel Reed, et al. Scalable
load-balance measurement for spmd codes. In High Performance Computing, Networking, Storage
and Analysis, 2008. SC 2008. International Conference for, pages 1‚Äì12. IEEE, 2008.
[12] Berk Hess, Carsten Kutzner, David Van Der Spoel, and Erik Lindahl. Gromacs 4: algorithms for
highly eÔ¨Écient, load-balanced, and scalable molecular simulation. Journal of chemical theory and
computation, 4(3):435‚Äì447, 2008.
[13] David A HuÔ¨Äman et al. A method for the construction of minimum redundancy codes. Proceedings
of the IRE, 40(9):1098‚Äì1101, 1952.
[14] CHEN Jie and Takeshi HASHIMOTO. ECG data compression by using wavelet transform. IEICE
TRANSACTIONS on Information and Systems, 76(12):1454‚Äì1461, 1993.
[15] Andreas Kn¬®
upfer and Wolfgang E Nagel. Construction and compression of complete call graphs
for post-mortem program trace analysis. In Parallel Processing, 2005. ICPP 2005. International
Conference on, pages 165‚Äì172. IEEE, 2005.
[16] Andreas Kn¬®
upfer, Christian R¬®
ossel, Dieter an Mey, Scott BiersdorÔ¨Ä, Kai Diethelm, Dominic Eschweiler, Markus Geimer, Michael Gerndt, Daniel Lorenz, Allen Malony, et al. Score-p: A joint
performance measurement run-time infrastructure for periscope, scalasca, tau, and vampir. In
Tools for High Performance Computing 2011, pages 79‚Äì91. Springer, 2012.
[17] German Llort, Juan Gonzalez, Harald Servat, Judit Gimenez, and Jes¬¥
us Labarta. On-line detection
of large-scale parallel application‚Äôs structure. In Parallel & Distributed Processing (IPDPS), 2010
IEEE International Symposium on, pages 1‚Äì10. IEEE, 2010.
[18] Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 11(7):674‚Äì693, 1989.
[19] Michael Noeth, Prasun Ratn, Frank Mueller, Martin Schulz, and Bronis R de Supinski. Scalatrace:
Scalable compression and replay of communication traces for high-performance computing. Journal
of Parallel and Distributed Computing, 69(8):696‚Äì710, 2009.
[20] Jerome M Shapiro. Embedded image coding using zerotrees of wavelet coeÔ¨Écients. Signal Processing, IEEE Transactions on, 41(12):3445‚Äì3462, 1993.
[21] Athanassios Skodras, Charilaos Christopoulos, and Touradj Ebrahimi. The jpeg 2000 still image
compression standard. Signal Processing Magazine, IEEE, 18(5):36‚Äì58, 2001.
[22] G¬®
ulay Tohumoglu and K Erbil Sezgin. ECG signal compression by multi-iteration EZW coding
for diÔ¨Äerent wavelets and thresholds. Computers in Biology and Medicine, 37(2):173‚Äì182, 2007.

1506

