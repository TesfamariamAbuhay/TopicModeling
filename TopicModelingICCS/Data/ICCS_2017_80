Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This space
isComputer
reservedScience
for the
header, do not use it
Procedia
108CProcedia
(2017) 715â€“724
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Data Mining Approach for Feature Based Parameter
Data
Mining
Approach
for Feature
Based Parameter
Tunning
forApproach
Mixed-Integer
Programming
Solvers
Data
Mining
for Feature
Based Parameter
Tunning for Mixed-Integer Programming Solvers
1
1
Tunning
Programming
Matheus
G. Vilasfor
BoasMixed-Integer
, Haroldo G. Santos
, Rafael de S. O.Solvers
Martins2 , and
3
H. C.
Matheus G. Vilas Boas11 , Luiz
Haroldo
G.Merschmann
Santos11 , Rafael
de S. O. Martins22 , and
3
Matheus G. Vilas
Boas
,
Haroldo
G.
Santos
,
Rafael
de
S. O. Martins , and
1
LuizofH.
C.Preto,
Merschmann
Federal University
Ouro
Department
3 of Computing,
Luiz Ouro
H. C.Preto,
Merschmann
Brazil
1

Federal University of Ouro Preto, Department of Computing,
matheusgueedes91@gmail.com,
haroldo.santos@gmail.com
Federal University ofOuro
OuroPreto,
Preto,
Department of Computing,
Brazil
Federal University
of
Ouro
Preto,
Department
of Information Systems and Computing,
Ouro Preto,haroldo.santos@gmail.com
Brazil
matheusgueedes91@gmail.com,
JoaÌƒoDepartment
Monlevade,
matheusgueedes91@gmail.com,
haroldo.santos@gmail.com
Federal University
of Ouro Preto,
ofBrazil
Information Systems and Computing,
martins.rso@gmail.com
Federal University
of
Ouro
Preto,
Department
of
Information Systems and Computing,
JoaÌƒo
Monlevade,
Brazil
3
Federal University martins.rso@gmail.com
of
Lavras,
Department
JoaÌƒo
Monlevade,
Brazilof Computer Science,
Lavras,
Brazil
3
Federal Universitymartins.rso@gmail.com
of Lavras, Department of Computer Science,
3
luiz.hcm@dcc.ufla.br
Federal University of
Lavras,
Department
of Computer Science,
Lavras,
Brazil
1

2
2
2

Lavras, Brazil
luiz.hcm@dcc.ufla.br
luiz.hcm@dcc.ufla.br

Abstract
Abstract
Integer
Programming (IP) is the most successful technique for solving hard combinatorial optiAbstractproblems. Modern IP solvers are very complex programs composed of many different
mization
Integer Programming (IP) is the most successful technique for solving hard combinatorial optiprocedures
whose execution
embedded
the generic
Branch
& Bound
framework.
actiInteger
Programming
(IP) isisIP
the
most successful
technique
for solving
hard
combinatorial
optimization
problems.
Modern
solvers
areinvery
complex
programs
composed
of manyThe
different
vation
of
these
procedures
as
well
the
definition
of
exploration
strategies
for
the
search
tree
can
mization
problems.
Modern
IP
solvers
are
very
complex
programs
composed
of
many
different
procedures whose execution is embedded in the generic Branch & Bound framework. The actibe
done
by
setting
different
parameters.
Since
the
success
of
these
procedures
and
strategies
in
procedures
whose
execution
is
embedded
in
the
generic
Branch
&
Bound
framework.
The
activation of these procedures as well the definition of exploration strategies for the search tree can
improving
the
performance
of
IP
solvers
varies
widely
depending
on
the
problem
being
solved,
vation
of
these
procedures
as
well
the
definition
of
exploration
strategies
for
the
search
tree
can
be done by setting different parameters. Since the success of these procedures and strategies in
the
usualbyapproach
for discovering
a good
set of
parameters
considering
average
is not
be
done
setting
different
parameters.
Since
the
success
of these
procedures
andresults
strategies
in
improving
the
performance
of IP solvers
varies
widely
depending
on
the problem
being
solved,
ideal.
In
this
work
we
propose
a
comprehensive
approach
for
the
automatic
tuning
of
Integer
improving
the
performance
of
IP
solvers
varies
widely
depending
on
the
problem
being
solved,
the usual approach for discovering a good set of parameters considering average results is not
Programming
solverswe
the acharacteristics
instances for
are
considered.
Computational
exthe
usual
approach
forwhere
discovering
a good set ofofparameters
considering
average
results
is not
ideal.
In this
work
propose
comprehensive
approach
the
automatic
tuning
of Integer
periments
in
a
diverse
set
of
308
benchmark
instances
using
the
open
source
COIN-OR
CBC
ideal.
In
this
work
we
propose
a
comprehensive
approach
for
the
automatic
tuning
of
Integer
Programming solvers where the characteristics of instances are considered. Computational exsolver
wereinperformed
withofthe
different
parameter
and
the
results
processed
by CBC
data
Programming
of sets
instances
arethe
considered.
Computational
experiments
asolvers
diversewhere
set
308characteristics
benchmark
instances
using
openwere
source
COIN-OR
mining
algorithms.
The
results
were
encouraging:
when
trained
with
a
portion
of
the
database
periments
in
a
diverse
set
of
308
benchmark
instances
using
the
open
source
COIN-OR
CBC
solver were performed with different parameter sets and the results were processed by data
the
algorithms
were The
ablewith
to predict
parameters
for
the
instances
84%
the
solver
were
performed
different
parameter
sets
and
theremaining
results
processed
byofdata
mining
algorithms.
results
werebetter
encouraging:
when
trained
with awere
portion
ofin
the
database
cases.
The
selection
of
a
single
best
parameter
setting
would
provide
an
improvement
in
only
mining
algorithms.
The
results
were
encouraging:
when
trained
with
a
portion
of
the
database
the algorithms were able to predict better parameters for the remaining instances in 84% of the
56%
of The
instances,
showing
that
great
improvements
can
be
our approach.
the
algorithms
were
able
predict
parameters
forwould
theobtained
remaining
instances
in 84%inofonly
the
cases.
selection
of ato
single
bestbetter
parameter
setting
providewith
an
improvement
cases.
The
selection
of
a
single
best
parameter
setting
would
provide
an
improvement
in
only
56%
ofThe
instances,
showing by
that
greatB.V.
improvements can be obtained with our approach.
Â©
2017
Authors.
Elsevier
Keywords:
feature Published
based parameter
tunning, mixed-integer programming, data mining, regression
56% of instances,
showing
that
great
improvements
be obtained
with on
our
approach. Science
Peer-review
under responsibility
of
the scientific
committee of thecan
International
Conference
Computational
algorithms,
cut
Keywords: coin-or
feature branch
based and
parameter
tunning, mixed-integer programming, data mining, regression
Keywords:
feature
based
parameter
algorithms, coin-or branch and cut tunning, mixed-integer programming, data mining, regression
algorithms, coin-or branch and cut

1
1877-0509 Â© 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.286

1
1

716	

Feature Based Parameter Tunning
M.Boas
G. Vilas
H. Computer
G. Santos,
R. O.108C
Martins,
H. C. Merschmann
Matheus G. Vilas
et al. / Boas,
Procedia
Science
(2017)L.
715â€“724

1

Introduction

A mixed-integer linear program (MIP) is an optimization problem in which a linear objective
function is minimized subject to linear constraints over real and integer valued variables. A
large number of relevant problems can be modeled in this format, from production planning to
prediction of protein structures. Integer programming problems are notoriously hard to solve[8].
In fact, no efficient general algorithm is known for their solution. Nevertheless, the dramatic
solver performance improvements occurred in the last decades[6, 10], allowed operations research
practitioners to compute the exact solution of large scale models.
Modern MIP solvers are complex programs: performance improvements were obtained by
enhancing the classical Branch & Bound[12] algorithm with the inclusion of many additional
procedures which can be executed in the search tree. These procedures are basically cutting
planes, devised to improve the linear programming relaxation and heuristics, created to speedup
the production of high quality feasible solutions. Since these procedures involve additional
computations, their activation does not always improves the overall solverâ€™s performance. Thus,
MIP solvers typically have more than 50 parameters which can be tuned to improve their
performance when solving different problems. A possible approach for this problem is the use
of automated techniques to discover a good parameter setting considering average results in
a representative set of problems. As we show later, this approach is severely limited due to
the large diversity of problem types that can be modeled as MIPs: different composition of
variables (e.g.: integer/continuous), different constraint types (e.g.: knapsack, cardinality and
flow) and coefficient matrix characteristics (e.g. density of non-zeros and range of coefficients).
Thus, smarter approaches can benefit from this problem diversity and perform Feature Based
Parameter Tuning (FBPT), i.e. different best parameter settings can be defined for different
classes of problems. These classes can be defined previously or discovered in the tuning process.
A successful FBPT is described in [5] in the context of heuristics for one specific application. Authors approached the curriculum-based course timetabling problem using Simulated
Annealing. Parameter selection is performed using data mining (regression methods). Promising results were obtained, reaching 95.5% of accuracy in the classification algorithm. Similar
application specific works are [1, 9, 11] and [3].
Considering MIP solvers, FBPT tools are scarce. To the best of our knowledge it was only
attempted in [4] where machine learning methods were used to predict the best parameter
settings for a small set of instances. In [4], the metric used to evaluate the performance of
different parameters settings is the total time used to prove the optimality. This metric is
insensitive to solver executions which finished with different optimality gaps. It is not sensitive
also to executions were the first feasible solution was produced in very different times. A much
better metric is employed in [14] to tune the SCIP MIP solver: the whole history of the search
process is considered, i.e. every improvement in the the incumbent solution and its time is
computed so that the objective is to discover a parameter setting which is better than others
at any time. Our metric is equivalent to this last one, since all relevant events in the search
process (improvement in lower or upper bounds) and their times are considered.
In this paper we address the following questions: can instance features of Mixed Integer
Programs be used to determine better parameter settings for groups of instances considering
a comprehensive performance evaluation? Additionally, can these groups be determined automatically using machine learning? To answer these questions we built a dataset with 308
problem instances and evaluated the open source MIP solver COIN-OR CBC[13, 7] using 49
different parameter settings. Three regression methods were trained in a subset of instances and
evaluated for predicting the best parameter settings for new instances considering its features.
2

	

Feature Based Parameter Tunning
M.Boas
G. Vilas
H. Computer
G. Santos,
R. O.108C
Martins,
H. C. Merschmann
Matheus G. Vilas
et al. / Boas,
Procedia
Science
(2017)L.
715â€“724

This paper is organized as follows: Section 2 describes the instances and the parameter sets
used in this paper and how data mining (more specifically regression algorithms) was used to
approach this problem. Our experiments are described in Section 3. Finally, in Section 4, the
obtained results and final remarks are discussed.

2

Methodology

In this paper, we consider a comprehensive metric to evaluate the performance of a given MIP
solver execution: since these solvers deliver a sequence of improved solutions (primal bounds)
and dual bounds over time, our metric is defined to select the parameter set more suited to
obtain good results at any time. This metric is described in Subsection 2.1. A given execution is
composed of a given problem (or instance) and a set of predefined values for solver parameters.
The set of instances and the parameter sets evaluated will be presented in the Subsections 2.3
and 2.4, respectively.
An overview of our approach is proposed in Figure 1. Given a set of instances and a set
of parameter settings, each instance is tentatively solved with each parameter setting. Each
execution concludes when a time limit is reached or optimality is proved. By inspecting solver
output messages all relevant events (improved primal and/or dual bounds) are stored with their
respective execution times and are used to compute an overall performance metric that will be
described in Section 2.1. Part of this database is then used for training the regression algorithm
which will recommend parameters for the remaining records. The positive (or negative) impact
of these recommendations can be evaluated considering the performance variation w.r.t. default
parameters.

Figure 1: Overview of the proposed approach to the Feature Based Parameter Tunning Problem

2.1

A comprehensive metric for evaluating MIP solvers executions

A key decision when optimizing parameter settings is the metric used to evaluate different
results. In this paper we propose a metric capable of precisely evaluate solversâ€™ performance
regarding: (i ) time needed to produce the first feasible solution/dual bound (ii) evolution
primal and dual bounds in time and, finally (iii) time to prove the optimality, which is a
direct consequence of the first two items. Given a time limit T the performance of a solver
s running with with a parameter setting p to solve instance i, considering a set of Es,T,P,i =
{e0 , . . . , enâˆ’1 } events, indicating any improved lower (be ) or upper bound (be ) produced at time
te and considering previous event p(e), the performance is computed as:
3

717

Feature Based Parameter Tunning
G. et
Vilas
Boas, H.Computer
G. Santos,
R. O.
Martins,
H. C. Merschmann
Matheus G. VilasM.
Boas
al. / Procedia
Science
108C
(2017) L.
715â€“724

s,T,p,i =



eâˆˆE\{e0 }

[(te âˆ’ tp(e) ) Ã— (bp(e) + bp(e) )]

To avoid comparing bounds in different scales, be + be are the relative gaps considering,
for each instance i, the best known upper/lower bounds ui and li , respectively, as follows:
be = min(100, [(ue âˆ’ ui )/(ui + )] Ã— 100)1 and be = min(100, [(li âˆ’ le )/(li + )] Ã— 100) for absolute
upper/lower bounds ue and le for event e, respectively. For events occurring before the first
feasible solution is found the upper bound is âˆž, which would make s,T,P,i not comparable
for executions where only the lower bound improves. Thus, a penalty be = 200 is used for any
event e where no feasible solution was found. The same penalty is used when no dual bound is
produced yet.
Figure 2 depicts the evolution of upper and lower bounds in an execution of the CBC solver
processing instance protfold from MIPLIB using a parameter set named zhifmvnodedowndepth
in a time limit of one hour. Respective events are shown in the accompanying Table and
how they contribute to the final value of cbc,3600,zhif mvnodedowndepth,protf old =600970.71. In
this execution CBC produced the first feasible solution at 2435.60 seconds and concluded the
execution with a lower bound relative gap of 16.76% and an upper bound relative gap of 25.81%.
200
lower bound gap
upper bound gap
optimal gap

190
180
170
160
150
140
lower (upper) bound gap

718	

130
120
110
100
90
80
70
60
50
40
30
20
10
0
-10
-20
0

500

1000

1500

2000

2500

3000

3600

time (seconds)

E
e0
e1
e2
e3
e4
e5
e6
e7

t
0.00
0.19
34.05
1986.82
2435.60
2436.87
2544.87
2610.98

b
200.00
26.12
21.81
21.80
21.80
21.80
21.80
21.80

b
200.00
200.00
200.00
200.00
70.97
64.52
61.29
32.26

sum (event)
0.00
76.00
7656.42
433143.91
99539.40
117.82
9322.56
5493.08


E
e8
e9
e10
e11
e12
e13
e14
e15

t
2633.81
3187.71
3258.46
3341.16
3421.23
3500.26
3582.45
3600.00

b
21.80
19.44
18.30
17.71
17.41
17.01
16.76
16.76

b
25.81
25.81
25.81
25.81
25.81
25.81
25.81
25.81

sum (event)
1234.19
26371.18
3201.44
3647.90
3484.65
3415.68
3519.38
747.10
600970.71

Figure 2: Execution of CBC on instance protfold with parameter set zhifmvnodedowndepth (
metric)
1

4

is a small constant to avoid division by zero in the case of having an upper/lower bound with this value

	

Feature Based Parameter Tunning
G. et
Vilas
Boas, H.Computer
G. Santos,
R. O.
Martins,
H. C. Merschmann
Matheus G. VilasM.
Boas
al. / Procedia
Science
108C
(2017) L.
715â€“724

2.2

Evaluating parameter settings

The performance of a MIP solver can be evaluated with different parameter settings, defined in
a set denoted here as P. One of these settings is the default setting, denoted here as d âˆˆ P. Our
main objective is the discovery of improved parameter settings w.r.t. to the default settings.
Given a set of instances I, the objective of the proposed feature based parameter tuning method
is to define a function f : P â†’ I such that every instance i âˆˆ I has a selected parameter setting
p âˆˆ P with the largest improvement w.r.t. to the default parameter setting d. For a solver s
running in time limit T using parameter setting p in instance i if s,T,p,i < s,T,d,i we have
s,T ,p,i
. Conversely, if s,T,p,i > s,T,d,i we have a performance
an improvement rs,T,p,i = 1 âˆ’ s,T
,d,i
degradation computed as rs,T,p,i = 1 âˆ’

2.3

s,T ,d,i
s,T ,p,i .

Instances

Computational experiments were performed on a diverse set of 308 benchmark instances including MIPLIB 3, 2003 and 2010[10] benchmark set. Furthermore, scheduling, timetabling and
rostering instances were also included. Based on our previous experience in solving MIPs, we
observed that variable types and coefficient matrix coefficients appeared to be specially influential to describe different groups of instances. Table 1 presents a distribution of these instances
according to these 2 features.
â€¢ Pure binary instances: This set contains instances that have only binary variables.
â€¢ Pure integer instances: This set includes instances that contain only binary or integer
variables.
â€¢ Mixed-integer linear instances: This set includes instances that have at least 10% of
continuous variables.
â€¢ Matrix min aij â‰¥ 0 and max aij â‰¤ 1: This set includes all instances where all coefficients
in the constraint matrix are in the range [0, 1].
â€¢ Matrix min aij â‰¥ âˆ’1000 and max aij â‰¤ 1000: Includes all instances where coefficients
in the constraint matrix are in the range [âˆ’1000, 1000].
â€¢ Pure binary instances with matrix min aij â‰¥ 0 and max aij â‰¤ 1: This set contains
pure binary instances with coefficients in the constraint matrix in the range [0, 1].
â€¢ Pure binary instances with matrix min aij â‰¥ âˆ’1000 and max aij â‰¤ 1000: This
set contains pure binary instances with coefficients in the constraint matrix in the range
[âˆ’1000, 1000].
Other important instance features considered in our dataset are the number of constraint of
different types included, such as covering, partitioning, packing, cardinality, invariant knapsack,
knapsack and flow constraints[10]. The frequency histogram of Figure 3 depicts how often these
constraints appear in our instance set for problems of different groups.

2.4

Parameter settings

The parameter space of modern MIP solvers, including CBC, is large and hard to explore
in depth due to the many possible combinations. Furthermore, some parameters appear to
5

719

Feature Based Parameter Tunning
M. G. Vilas Boas, H. G. Santos, R. O. Martins, L. H. C. Merschmann
Matheus G. Vilas Boas et al. / Procedia Computer Science 108C (2017) 715â€“724

Table 1: Distribution of instances according variable types and coefficient matrix
Type
Pure binary problems

Binary

Integer

Continuous

|Instances|
159

Pure integer problems

56

Mixed-integer linear problems

49
4
14

Matrix min aij â‰¥ 0 and max aij â‰¤ 1

39
1
2
142
47
46
4
12

Matrix min aij â‰¥ âˆ’1000 and max aij â‰¤ 1000

instance types vs. constraint types

180

all instances

160

pure binary instances

140

mixed-integer linear instances

pure integer instances

matrix minaij >= 1 and maxaij <= 0 instances

120

matrix minaij >= -1000 and maxaij <= 1000 instances

100

pure binary instances with matrix minaij >= -1000 and maxaij <= 1000

number of instances

720	

pure binary instances with matrix minaij >= 1 and maxaij <= 0

80
60
40
20
0

partitioning

packing

covering

cardinality

inv.knp.
constraints

knp.

bï¬‚ow

iï¬‚ow

mï¬‚ow

Figure 3: Histogram of groups of instances and types of constraints
be much more influential than other and their tunning deserves special attention. Thus, to
guide the construction of our initial set of parameter settings we contact CBC developers and
experienced users. Bellow there is a list of parameters which were tuned in this work. Each
parameter is represented by its name, abbreviation, description, default value (highlighted in
bold) and used values in the experiments.
Branch-and-Bound
â€“ NodeStrategy (node): Strategy to select nodes do be explored. Values: fewest;
upfewest; updepth; hybrid; downdepth; downfewest; depth.
â€“ StrongBranching (str): Number of fractional variables which pseudo costs will be
examined before proceeding in the search tree. Values: 5; 10.
6

	

Feature Based Parameter Tunning
M.Boas
G. Vilas
H. Computer
G. Santos,
R. O.108C
Martins,
H. C. Merschmann
Matheus G. Vilas
et al. / Boas,
Procedia
Science
(2017)L.
715â€“724

â€“ Expensive (xpsve): Extra strong branching. Values: 0; 1; 2.
â€“ TrustPseudoCosts (tr): After computing pseudo costs many times for a variable trust
previously computed pseudo costs and do not perform strong branching anymore
(needs to be greater than StrongBranching). Values: 5; 20.
Cut Generation
â€“ Cuts (cts): CBC cut generators can be configured to be applied only at root node
or in the entire search tree. Values: on; off.
â€“ Lagomory (lgm): This simplification just uses original constraints while modifying
objective using other cuts. Values: off ; endonly.
â€“ ReduceAndSplitCuts (rdc): Switches on reduce and split cuts (either at root or in
entire tree). Values: off ; ifmove.
â€“ Latwomir (ltm): A lagrangean relaxation for TwoMir cuts. Values: off ; endonly;
endonlyroot.
â€“ PassTreeCuts (passt): Number of cut in tree. Values: 1; 2.
â€“ Zero (z): Switches on zero-half cuts. Values: off ; ifmove.
â€“ ResidualCapacityCuts (residual): Use Residual Capacity cuts. Values: off ; ifmove.
Heuristics
â€“ Heuristic (hr).: Enables/disables heuristics. Values: on; off.
â€“ PassFeasibilityPump (passf): Indicates the maximum number of passes for the Feasibility Pump heuristic. Values: 30; 45; 20.
â€“ Pumptune (pptn): Fine tunes Feasibility Pump. Values: 0; 5005043.
â€“ Combine2 (cbn2): Switches on a heuristic which does branch and cut on the problem
obtained by fixing variables which appeared in two or more solutions. Values: on;
off.
â€“ RoundingHeuristic (rnd): Switches on a simple rounding heuristic at each node of
tree. Values: on; off.
â€“ Rins (rns): Controls the activation of RINS heuristic. Values: on; off.
â€“ ProximitySearch (prxt): Whether to do proximity search heuristic. Values: on; off.
â€“ DiveOpt (dvo): Diving options. Values: 2; 7.
â€“ MultipleRootPasses (multiple): Do multiple root passes to collect cuts and solutions.
Values: 0; 2; 3; 4.

2.5

Data Mining Approach

In order to define which parameter setting should be used for a given instance, the regression
data mining task was applied. Its main goal is to build a model by analyzing a set of known
class examples (training set) for the purpose of using it to predict the class value of unlabeled
examples. In this work, the objective is to predict the  metric (a continuous value) from the
attribute set that describes an example, composed of attributes that characterize each instance
(see Subsection 2.3) and attributes related to solver parameters (see Subsection 2.4).
As our objective is to discover which solver parameter setting (among 49 different settings)
is more suitable for a given instance, the predictive model built from a training set is used
7

721

722	

Feature Based Parameter Tunning
M.Boas
G. Vilas
H. Computer
G. Santos,
R. O.108C
Martins,
H. C. Merschmann
Matheus G. Vilas
et al. / Boas,
Procedia
Science
(2017)L.
715â€“724

to predict the  metric for each example constructed merging the attributes related to the
instance characteristics and those regarding solver parameters. After, the parameter setting
associated with the example containing the best (smaller)  metric predicted is returned as the
recommended parameter setting for the given instance.
Multiple algorithms are available in the literature for regression model construction. In this
work, we employed three commonly used techniques: Locally Weighted Learning (LWL) [2],
Support Vector Machine (SVM) [16] and Random Committee (RC) [17].
Locally Weighted Learning is a lazy learning method in which the processing of the training
data is postponed until a query example needs to be answered. Instead of building a global
model to fit all of the training data, the basic idea behind LWL is to create a local model
based on neighboring data of the query example. In this way, each neighbor example becomes
a weighting factor which define its influence for the prediction.
Support Vector Machine (SVM), first introduced by Vladimir Vapnik [16], can be applied to classification and regression problems. In this paper we address the SVM regression problem (Support Vector Regression â€“ SVR [15]). Considering a set of training data
{(x1 , y1 ), (x2 , y2 ), . . . , (xk , yk )}, where each xi âŠ‚ Rn and yi is the corresponding target value,
yi âŠ‚ R, for i = 1, . . . , k, the basic idea of the SVR is to find a function f (x) that deviates from
yi by a value no greater than Îµ for every xi , and at the same time is as flat as possible. The
goal is to determine a function that can approximate future values accurately.
Random Committee builds an ensemble of base predictors and averages their predictions.
Each predictor is built from the same data but using a different random number seed. In this
work, Random Tree, a method that constructs a tree considering k randomly chosen attributes
at each node, was adopted as base predictor.

3

Experiments

In all, 49 different parameter settings have been evaluated for each one of the 308 benchmark
instances using the open source COIN-OR CBC solver. Regression algorithms were evaluated,
embedded in Weka2 . Several algorithms were evaluated and used to predict which parameter setting should be used for solve a given instance. We selected three regression algorithms
which performed better in previous experiments to be included. In this work, Random Tree
was adopted as base predictor. An algorithm has been implemented, called Random, that
randomly chooses one of 49 parameter setting each time an instance is run by the solver, just
to illustrate how a â€œblindâ€ classifier would perform. This randomized algorithm was executed
10000 times and the average results of these runs was used to evaluate it. The main results are
reported in Table 2, considering all instances. The results obtained by executing the solver with
different parameter settings proposed by the regression algorithms are compared with the values obtained by executing the solver with its default parameter setting. The first two columns
indicate average values of improvement (r) and performance degradation (r) considering parameter settings suggested by the regression algorithms on all instances. A time limit T = 3600
seconds was considered for solving each instance. The remaining columns show simple metrics
which are commonly used in other works: Eval. time indicates the overall processing time of
all instances, which can decrease if optimality is proved earlier. Column best shows how in how
many instances each algorithm indicated better parameter settings and, finally, column Eval
. Opt. shows the ratio of improvement in the number of instances where the optimality was
proved.
2 Weka
3 We

8

is a collection of machine learning algorithms implemented for data mining tasks.
consider exponent = 0.1, which will provide an adequate classification performance divergence.

Feature Based Parameter Tunning
M. G. Vilas Boas, H. G. Santos, R. O. Martins, L. H. C. Merschmann
Matheus G. Vilas Boas et al. / Procedia Computer Science 108C (2017) 715â€“724

Table 2: Results for all instances
Algorithm
Random
LWL
SMOReg3
RandomCommittee

Improvement
13.09%
28.41%
2.60%
24.24%

Degradation
19.84%
31.19%
18.97%
27.55%

Eval. Time
1.46%
-2.77%
-0.91%
-2.62%

Best
134
157
258
177

Eval. Opt.
1.11%
5.00%
1.11%
3.89%

recommended conï¬guration [LWL]
recommended conï¬guration [SMOReg0.1]
recommended conï¬guration [RandomCommittee]
default conï¬guration
optimal gap

60

lower (upper) bound gap

	

30

0

3

6

9

12

15

18

21

24

27

30

33

36

39

42

45

48

51

54

57

60

time (minutes)

Figure 4: Average out for lower (upper) bound gap for time intervals (all instances)
The obtained results are quite promising: better results using regression algorithms were
obtained always in more than a half of the instances, when comparing with default parameter
settings. In addition, we can always prove optimality in more instances and reduce the total time
spent processing all instances. Considering the LWL algorithm, we achieved an improvement
of approximately 29%, in the value of the  metric, for the 157 instances that were better with
the parameter settings recommended by this algorithm.
A different view of these results is presented in Figure 4. In this graphic the evolution of
the average upper and lower bounds when considering parameter settings suggested by each
regression algorithm is depicted. As it can be seen, the bounds of the parameters proposed by
the data mining algorithms are smaller than the default values most of the time, with a notable
exception at the beginning of the search process. This can be explained by the activation of
more expensive procedures which slow down the start of the search but improve considerably
the results as the search progresses.

4

Discussion and conclusions

In this paper, we present a comprehensive metric to evaluate the performance of MIP solvers and
regression algorithms are used to predict which parameter setting should be used when solving
instances with specific characteristics. We were able to obtain better results in 258 instances
9

723

724	

Feature Based Parameter Tunning
M.Boas
G. Vilas
H. Computer
G. Santos,
R. O.108C
Martins,
H. C. Merschmann
Matheus G. Vilas
et al. / Boas,
Procedia
Science
(2017)L.
715â€“724

(84% of all instances) when comparing to the performance obtained with the standard solver
configuration. A customized selection of parameters considering instance features appears to
be a very promising trend for MIP solvers: considering our set of experiments, the selection of
a single â€œbest in averageâ€ parameter setting would improve the result in only 56% of instances.

References
[1] Pablo Achard and Erik De Schutter. Complex Parameter Landscape for a Complex Neuron Model.
PLOS Computational Biology, 2(7):1â€“11, 07 2006.
[2] Christopher G. Atkeson, Andrew W. Moore, and Stefan Schaal. Locally Weighted Learning.
Artificial Intelligence Review, pages 11â€“73, 1997.
[3] Charles Audet and Dominique Orban. Finding Optimal Algorithmic Parameters Using DerivativeFree Optimization. SIAM Journal on Optimization, 17(3):642â€“664, 2006.
[4] Mustafa Baz, Brady Hunsaker, P Brooks, and Abhijit Gosavi. Automated Tuning of Optimization
Software Parameters. Technical report, Technical Report TR2007-7, University of Pittsburgh,
Department of Industrial Engineering, 2007.
[5] Ruggero Bellio, Sara Ceschia, Luca Di Gaspero, Andrea Schaerf, and Tommaso Urli. Feature-based
Tuning of Simulated Annealing applied to the Curriculum-based Course Timetabling Problem.
Computers & Operations Research, 65:83 â€“ 92, 2016.
[6] Robert E Bixby, Mary Fenelon, Zonghao Gu, Edward Rothberg, and Robert Wunderling. Mixed
Integer Programming: A Progress Report. In Martin GroÌˆtschel, editor, The sharpest cut: the
impact of Manfred Padberg and his work, chapter 18, pages 309â€“325. SIAM, 2004.
[7] John Forrest and Robin Lougee-Heimer. Cbc User Guide. In Emerging Theory, Methods, and
Applications, pages 257â€“277. INFORMS, 2005.
[8] Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co., New York, NY, USA, 1979.
[9] Joseph Haas, Maxim Peysakhov, and Spiros Mancoridis. Ga-based Parameter Tuning for Multiagent Systems. In Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation, GECCO â€™05, pages 1085â€“1086, New York, NY, USA, 2005. ACM.
[10] Thorsten Koch, Tobias Achterberg, Erling Andersen, Oliver Bastert, Timo Berthold, Robert E
Bixby, Emilie Danna, Gerald Gamrath, Ambros M Gleixner, Stefan Heinz, et al. MIPLIB 2010.
Mathematical Programming Computation, 3(2), 2011.
[11] Ron Kohavi and George H. John. Automatic Parameter Selection by Minimizing Estimated Error.
In In Proceedings of the Twelfth International Conference on Machine Learning, pages 304â€“312.
Morgan Kaufmann, 1995.
[12] A.H. Land and A.G. Doig. An Automatic Method for Solving Discrete Programming Problems.
Econometrica, 28:497â€“520, 1960.
[13] R. Lougee-Heimer. The Common Optimization Interface for Operations Research: Promoting
Open-source Software in the Operations Research Community. IBM Journal of Research and
Development, 47(1):57â€“66, 2003.
[14] Manuel Lpez-Ibez and Thomas Sttzle. Automatically Improving the Anytime Behaviour of Optimisation Algorithms. European Journal of Operational Research, 235(3):569 â€“ 582, 2014.
[15] S. K. Shevade, S. S. Keerthi, C. Bhattacharyya, and K. R. K. Murthy. Improvements to the SMO
Algorithm for SVM Regression. IEEE Transactions on Neural Networks, 11(5), 2000.
[16] Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA, 1995.
[17] Ian H. Witten, Eibe Frank, and Mark A. Hall. In Ian H. Witten, Eibe Frank, and Mark A. Hall,
editors, Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann,
Boston, 3rd edition, 2011.

10

