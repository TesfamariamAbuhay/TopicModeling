Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1475 â€“ 1484

International Conference on Computational Science, ICCS 2013

Markov Chain Analysis of Agent-based Evolutionary Computing
in Dynamic Optimization
Aleksander Byrskia , Robert Schaefera
a AGH

University of Science and Technology, Al. Mickiewicza 30, 30-059 Krakow, Poland

Abstract
In this paper a Markov model for Evolutionary Multi-Agent System is recalled. The model allows to study dynamic features of
the computation and increases understanding the considered classes of systems by e.g., proving the ergodicity of the Markov
chain modelling EMAS. This feature may be considered as a reason to use such complex techniques, as following the Michael
Voseâ€™s approach, similar feature is proven for EMAS, showing that this system is able to reach any possible state of the system
space (including of course optima sought). The main contribution of the paper is showing possibilities of applying the already
proposed model to dynamic optimization problems. The impact of these enhancements on the ergodicity feature is also discussed.

Â© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
c 2012 The
Published
Elsevier B.V.of the organizers of the 2013 International Conference on Computational
Selection
andAuthors.
peer review
underby
responsibility
Selection and/or peer-review under responsibility of the 2013 International Conference on Computational Science.
Science
Keywords: multi-agent systems; Markov chain modelling; ergodicity; dynamic optimization

1. Introduction
When looking for an optimal heuristics to solve a certain problem, one must recall the famous â€œno free lunchâ€
theorem [1], stating, that there are no algorithm that will be equally good for all possible problems. On the
other hand, it would be good to be sure, if a certain algorithm is at least capable of solving the considered task,
especially, when constructing complex metaheuristics. This is important, because complex search methods may
aï¬€ect the ability to ï¬nd all possible answers to the given problem, therefore, formal proving of certain features of
the computation becomes an important argument in the discussion of applicability of certain search methods.
Handling dynamic optimization problems, where the ï¬tness value changes over the time, requires special
approaches to constantly maintain or even refresh the diversity of the population. The methods known as hypermutation (increasing mutation rate after the change of the ï¬tness function) or random immigrants (introducing
randomly generated individuals into the population) were developed [2]. An interesting approach solution of dynamic optimization problems may also be performed by employing diï¬€erential evolution [3]. Other techniques,
as employing dominance and diploidy were also considered [4]. Some other approaches try to predict the changes
in the optima position [5].
âˆ— Corresponding author. Tel.: +48-12-328-33-30 ; fax: +48-12-617-51-72 .
E-mail address: olekb@agh.edu.pl.

1877-0509 Â© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.315

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

	






	








	










			


	


	

	










	







	


	







	
	


	
	







	


	

	
	







	
	





	
	





1476





	
	




 
	







(a) Evolutionary multi-agent system (EMAS)



(b) Scheme of the synchronisation mechanism

Fig. 1: EMAS structure and behaviour and the synchronisation mechanism
There are few formal models prepared for analysing of stochastic features of population-based heuristics, and
virtually none covering dynamic optimization. Several of the most known ones are cited in this paragraph. The
model presented by Vose [6] proves in the most simple, yet eï¬€ective way, asymptotic guarantee of success, i.e.
â€œability to ï¬nd all local maximizers (minimizers) with probability 1 after inï¬nite number of epochsâ€ [7, 8, 9]
in the analysis of the Simple Genetic Algorithm (SGA) behaviour, formally conï¬rming the possibility of using
SGA for global optimisation. Formal models for genetic algorithms have also been proposed by other researchers,
providing a deeper insight into the long term, steady state behaviour of large population EAs [10, 11, 12] or
modelling speciï¬c features of EAs such as selection, genetic drift, niching etc. [13, 14, 15].
In this paper, basic features of the stochastic Markov models already introduced in the works of Byrski,
Schaefer et al. (e.g. [16, 17, 18, 19, 20]) are recalled. Formulas giving e.g., description of system state space
and the dynamics of the model are given, however it is carried out only in order to build a background for the
considerations regarding the extending of EMAS. These works were devoted to modelling EMAS (Evolutionary
Multi-Agent Systems) introduced by Cetnarowicz [21], being a general optimisation system leveraging paradigms
of evolutionary computation and agency (that can be counted to computational intelligence solutions [22, 23]).
The main contribution of the paper is an analysis of possibility of proving asymptotic features (e.g. asymptotic
guarantee of success) of EMAS applied to dynamic optimization problem. The full formal proof of ergodicity has
been carried out in the work of Byrski, Schaefer, SmoÅ‚ka and Cotta [24].
The paper starts with an overview of EMAS then the Markov chain model of EMAS is presented (including
deï¬nition of the system state and transition functions). Then, the adaptation of the presented model for dynamic
optimization problem and a discussion of the impact of this adaptation on the possibility of proving the ergodicity
are shown.
2. Evolutionary Multi-Agent System in dynamic optimization
EMAS is a general-purpose optimisation system leveraging paradigms of evolutionary computation and agency,
(following work of Cetnarowicz [21]) that has already proven its eï¬ƒciency for certain class of problems (see e.g.,
[25, 26, 27, 28, 29]).
Fig. 1a shows the simplest possible model of an evolutionary multi-agent system, with one type of agents and
one resource (called energy) deï¬ned. This energy becomes a base for distributed selection mechanismâ€”the more
energy agent possesses, the more likely it may reproduce, on the other hand, when its energy falls below certain
level, the agent is removed from the system.
The agent possesses genotype that represents feasible solutions to the problem. The energy is transferred
between agents in the process of evaluation. When the agent ï¬nds out that one of its neighbours (e.g. randomly
chosen), has lower ï¬tness, it takes a part of its neighbourâ€™s energy, otherwise it passes part of its own energy

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

1477

to the evaluated neighbour. The level of life energy triggers the actions of Reproduction (performed when the
agentâ€™s energy raises above a certain level, followed by production of a new individual in cooperation with one
of its neighbours, with genotype based on parentsâ€™ genotypes (crossed over and mutated) and part of energy also
taken from its parents), Death (the agent is removed from the system when its energy falls below a certain level,
the remaining energy is distributed among its neighbours), Migration (the agent may migrate when its energy
rises above a certain level, then it is removed from one evolutionary island and moved to another according to
predeï¬ned topology).
Each action is attempted randomly with a certain probability, and it is performed only when their basic preconditions are met (e.g., an agent may attempt to perform the action of reproduction, but it will reproduce only if
its energy rises above certain level and it meets an appropriate neighbour).
To derive the deï¬nition of dynamic optimization, let us start from the deï¬nition of stationary global optimization. This problem is deï¬ned by the closed search space U and a quality function (ï¬tness) F : U â†’ R. The task
is to determine the set of extrema X âŠ† U, deï¬ned as:
X = {gen âˆˆ U; âˆ€gen âˆˆU F(gen) â‰¥ F(gen )}

(1)

In the discussed case, U is a ï¬nite genetic universum #U = r < +âˆ [8]. Of course the minimization problem may
be easily changed into maximization by negating the ï¬tness value.
A dynamic optimization problem is deï¬ned by the search space U, a set of quality functions F (t) : U â†’ R, t âˆˆ
N0 . The goal of this task is to determine the set of all extrema X(t) âŠ‚ U(t âˆˆ N0 ), deï¬ned as:
X(t) = {gen âˆˆ U; âˆ€gen âˆˆU F (t) (gen) â‰¥ F (t) (gen )}

(2)

Again in this case U is a ï¬nite genetic universum #U = r < +âˆ [30].
Handling such dynamic optimization problems, requires special approaches to constantly maintain or even
refresh the diversity of the population. The approaches known as hypermutation (increasing mutation rate after
the change of the ï¬tness function) or random immigrants (introducing randomly generated individuals into the
population) were developed [2]. The allopatric niching techniqueâ€”island model of evolutionâ€”is also considered
as a valuable approach to this problem.
Covering such problem in the considered Markov chain EMAS model consists mainly in appropriately proposing the ï¬tness function and changing only one important element of the model, that utilizes the ï¬tness function:
the function that compares two agents [17] in order to transfer the energy from the worse one to the better one.
Following this modiï¬cation, popular mechanism supporting dynamic optimization may be easily implemented,
i.e.: allopatric speciation has been covered in EMAS right from the start of the research on this heuristics, the parameters of the variation operators (e.g., mutation) may be also changed, in order to perform hypermutation,
randomly generated agents may be introduced into the system using the cloning action coupled with hypermutation. The impact of these changes on the Markovian model of EMAS introduced by Byrski et al. [16, 17, 19] will
be discussed later in this paper, along with possibilities of proving the feature of ergodicity in this case.
3. EMAS state
In this section the description of the EMAS state given in [19, 31] will be recalled. The full EMAS state
description is given in [24]
Computational EMAS agents belong to the predeï¬ned ï¬nite set Ag one-to-one mapped on set U Ã— P, where
P = {1, . . . , p} and p is assumed to be the maximum number of agents contain the same genotype, so each agent
aggen,n âˆˆ Ag is uniquely represented by its signature (gen, n) âˆˆ U Ã— P.
Agents are assigned to locations Loc = {1, . . . , s}. The locations are linked by channels along which agents
may migrate from one location to another. The topology of channels is determined by the symmetric relation
T op âŠ‚ Loc2 . We assume that the connection graph Loc, T op is coherent and does not change during the
system evolution. Each agent possesses a variable parameter called energy, its value is quantized and belongs to
{0, Î”e, 2 Â· Î”e, 3 Â· Î”e, . . . , m Â· Î”e}. The current value of the energy exhibits the maturity of agent in solving the
optimization problem, aï¬€ecting its abilities (reproduction, cloning, migration) (see [32]).

1478

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

Let us consider the set of three-dimensional, incidence and energy matrices x âˆˆ X with s layers (corresponding
to all locations) x(i) = {x(i, gen, n), gen âˆˆ U, n âˆˆ P}, i âˆˆ Loc. The layer x(i) will contain energies of agents in
i-th location. In other words, x(i, gen, k) > 0 means that the k-th clone of the agent containing the gene gen âˆˆ U is
active, its energy equals x(i, gen, k) and it is located in i-th location.
The following coherency conditions are assumed:
â€¢ each layer x(i) contains at most qi values greater than zero, which denotes the maximum capacity of the i-th
location, moreover, the quantum of energy Î”e is lower or equal than total energy divided by the maximal
number of individuals that may be present in the system Î”e â‰¤ s1 qi what allows to achieve maximal
i=1
population of agents in the system,
s
qi . We assume that p =
â€¢ reasonable values of p should be greater or equal to 1 and less or equal to i=1
s
q
which
assures
that
each
conï¬guration
of
agents
in
locations
is
available,
respecting
the constrained
i=1 i
s
qi . Increasing p over this value does not enhance the descriptive power
total number of active agents i=1
of the presented model,
â€¢ (Â·, j, k)-th column contains at most one value greater than zero, which expresses that the agent with k-th
copy of j-th genotype may be present in only one location at a time, whereas other agents containing copies
of j-th genotype may be present in other locations,
â€¢ entries in the incidence and energy matrices are non-negative x(i, j, k) â‰¥ 0, âˆ€ i = 1, . . . , s, j = 1, . . . , r, k =
p
s
r
1, . . . , p and i=1
j=1 k=1 x(i, j, k) = 1, which means that the total energy contained in the whole system
is constant, equal to 1.
Gathering all these conditions, the set of three-dimensional incidence and energy matrices was described in
the following way:
X = x âˆˆ {0, Î”e, 2 Â· Î”e, 3 Â· Î”e, . . . , m Â· Î”e} sÂ·rÂ·p , Î”e Â· m = 1,
s

r

p

r

p

x(i, j, k) = 1 and âˆ€ i = 1, . . . , s
i=1 j=1 k=1

[x(i, j, k) > 0] â‰¤ qi
j=1 k=1

s

and âˆ€ j = 1, . . . , r, k = 1, . . . , p

[x(i, j, k) > 0] â‰¤ 1

(3)

i=1

where [Â·] denotes the value of the logical expression contained in the parentheses.
Note that the formula (3) implies that there must exist at least one agent in the system i.e. at least one location
is non-empty at a time.
EMAS may be modeled as the following tuple:
< U, Loc, T op, Ag, {agseli }iâˆˆLoc , locsel, {LAi }iâˆˆLoc , MA, Ï‰, Act >

(4)

where:
MA (master agent) is used to synchronize the work of the locations; it allows to perform actions in particular
locations. This agent is also used to introduce necessary synchronization into the system.
locsel : X â†’ M(Loc) is the function used by MA to determine which location should be allowed to perform the
next action.
LAi (local agent) is assigned to each location; it is used to synchronize the work of computational agents present
in its location, LAi chooses the computational agent and lets it evaluate a decision and perform the action,
at the same time asking MA whether this action may be performed.
agseli : X â†’ M(U Ã— P) is a family of functions used by local agents to select the agent that may perform the
action, so every location i âˆˆ Loc has its own function agseli . The probability agseli (x)(gen, n) vanishes
when the agent aggen,n is inactive in the state x âˆˆ X or it is present in other than i-th location,

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

1479

Ï‰ : X Ã— U â†’ M(Act) is the function used by agents for selecting actions from the set Act; both these symbols
will be described later.
Act is a predeï¬ned, ï¬nite set of actions.
Here and later M(Â·) stands for the space of probabilistic measures.
Generally speaking, in order to appropriately model a system using a Markov chain, a synchronization mechanism must be developed, to control the changes of the state by the system actions. The agent-based synchronization mechanism taking into account parallel executions of some actions is explained in details e.g., in [16, 17].
Shortly speaking, the agents situated in diï¬€erent levels of hierarchy (computational agents and local agents) ask
their predecessor for the permission to perform certain task (see Fig. 1b) and conduct their work only in the case,
when they receive a permission. In the considered case of dynamic optimization, it is assumed that all actions
performed in the system are considered global. Though, the presented synchronization mechanism is similar to
the one presented in e.g., [16], assuming that the probability of performing global action is equal to 1.
The population of agents is initialized by using introductory sampling. It may be explained as a one-time
sampling from X according to the predeï¬ned probability distribution (possibly uniform) from M(X). Every agent
starts its work in EMAS immediately after being activated. At every observable moment a certain agent on each
location gains the possibility of changing the state of the system by executing its action.
The function agseli is used by the Local Agent LAi to determine which agent present on i-th location will be
the next one to interact with the system. After being chosen, the agent aggen,n chooses one of the possible actions
according to the probability distribution Ï‰(x, gen). Notice the relationship of this probability distribution with the
concept of ï¬ne-grain schedulers introduced in the syntactic model for memetic algorithms in [33].
Next, the agent applies to LAi for the permission to perform this action. When the permission is granted, aggen,n
checks whether the associated condition is true, and if so, the agent performs the action. The agent suspends its
work in the system after performing the action which brings its energy to zero.
Master agent MA manages the activities of LAi allowing them to grant permissions for their agents (thus
relating to coarse-grain schedulers in [33]). Each action Î± âˆˆ Act is the pair of families of random functions
}genâˆˆU,nâˆˆP and {Ï‘gen,n
}genâˆˆU,nâˆˆP where
{Î´gen,n
Î±
Î±
: X â†’ M({0, 1})
Î´gen,n
Î±

(5)

(1) by agent aggen,n in state x âˆˆ X i.e.
will denote the decision. The action Î± is performed with probability Î´gen,n
Î±
when the decision is undertaken. Moreover
: X â†’ M(X)
Ï‘gen,n
Î±

(6)

deï¬nes the non-deterministic state transition caused by the execution of action Î± by agent aggen,n . The trivial state
transition
Ï‘null : X â†’ M(X)
(7)
such that for all x âˆˆ X
Ï‘null (x)(x ) =

1
0

if x = x
otherwise

(8)

(x)(0), i.e. when decision Î´Î± is not undertaken (Î´gen,n
(x) is evaluated as zero).
is performed with probability Î´gen,n
Î±
Î±
The value of the probability transition function for action Î± for the agent containing the n-th copy of genotype
gen being in the location l
gen,n
: X â†’ M(X)
(9)
Î±
for the arbitrary current state x âˆˆ X and the next one x âˆˆ X is given by:
gen,n
(x)(x
Î±

) = Î´gen,n
(x)(0) Â· Ï‘null (x)(x ) + Î´gen,n
(x)(1) Â· Ï‘gen,n
(x)(x )
Î±
Î±
Î±

(10)

Notice ï¬nally that it is formally possible to consider a very large (yet ï¬nite) set Act, comprising all actions
up to a certain description length (using a GÂ¨odel numbering or any appropriate encoding). This implies that this
set may be implicitly deï¬ned by such an encoding, allowing much ï¬‚exibility in the set of actions available (a
connection can be drawn with multimeme algorithms [34]).

1480

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

4. EMAS dynamics
In this section, the description of dynamics of Markov chain EMAS model will be recalled after [19]. The full
description of the dynamics will is given in [24]. As it was stated before, it is assumed, that all actions identiï¬ed
in the system applied to dynamic optimization problem are considered global.
At the observable moment at which EMAS takes state x âˆˆ X all agents in all locations notify their local
agents their intent to perform an action, all local agents choose an agent using the distribution given by the
agseli (x), i âˆˆ Loc function and then notify the master agent of their intent to let perform an action by one of their
agents. The master agent chooses the location using the random function locsel(x).
The transition function for the whole system looks as follows:
â›
ââ
â›
p
âœâœâœ
âŸâŸâŸâŸâŸâŸ
âœâœâœ
gen,n
âœ
âœ
locsel(x)(i) âœâœâ
agseli (x)(gen, n)Â· âœâœâœâ
Ï‰(x, gen)(Î±) Â· Î± (x)(x )âŸâŸâŸâŸâ âŸâŸâŸâŸâ 
(11)
Ï„(x)(x ) =
iâˆˆLoc

Î±âˆˆActgl

genâˆˆU n=1

It is easy to see that
Observation 1. The stochastic state transition of EMAS given by formula (11) satisï¬es the Markov condition.
Moreover, the Markov chain deï¬ned by these functions is stationary.
5. Adaptation of the model to dynamic optimization case
Adaptation of the model to cover dynamic optimization problem requires introducing appropriate deï¬nition
of ï¬tness function. In this section, certain type of ï¬tness dynamism will be presented, that has enough descriptive
power to support dynamic optimization problems. Generally speaking, the modiï¬cation of the ï¬tness function
consists in assuming, that the values returned by the original ï¬tness may be modiï¬ed (perturbed) by the random
variable of a given distribution. Let {[lgen , hgen ]}genâˆˆU be the assumed ranges of ï¬tness perturbations. Let us denote
by
IPF = { f : U â†’ R; f (gen) âˆˆ [lgen , hgen ], âˆ€gen âˆˆ U}
(12)
The perturbation will be described as the following random function:
Per : X â†’ M(IPF).

(13)

Fitness will be now represented by the following random function:
Fitness : X â†’ M(F + IPF)

(14)

where â€œ+â€ stands for the set translation operator and F stands for ï¬tness function in stationary case.
Let I âŠ‚ R be any measurable set. Now,
âˆ€ gen âˆˆ U, x âˆˆ X (Fitness(x)(I))(gen) = Per(x)((I âˆ© ([lgen , hgen ] + F(gen))) âˆ’ F(gen)).

(15)

In particular case, when the perturbations are deï¬ned by the family of pairwise independent functions, the following random function vector may be deï¬ned:
pergen : X â†’ M([lgen , hgen ])

(16)

in such way, that for any measurable sets Agen âŠ‚ [lgen , hgen ], gen âˆˆ U and the sets of functions: A = { f âˆˆ
IPF; f (gen) âˆˆ Agen }:
Per(x)(A) =
pergen (x)(Agen ).
(17)
genâˆˆU

Let us assume the sequel of EMAS states x , x , . . . , x(tâˆ’1) , x(t) , . . . then F (t) may be deï¬ned as the eï¬€ect of
sampling Fitness(x(tâˆ’1) ) such that
(0)

(1)

F (t) (gen) = F(gen) + pergen (x(tâˆ’1) ), âˆ€gen âˆˆ U

(18)

where the underlining denotes the eï¬€ect of sampling the underlined random variable or function.
Further implementation of selected techniques supporting dynamic optimization may require redeï¬nition of
MUT and CROS S functions, by simply making them dependent on the state of the system, therefore allowing to
change the parameters of these distributions based on, e.g., number of evolutionary generations passed etc.

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

1481

6. EMAS actions
Let us consider a sample EMAS with the following set of actions: Act = {get, repr, clo, migr}
The ï¬tness value and their changes do not aï¬€ect of all actions expecting get, so they are set exactly the same
as in [19]. The action get will be partially redeï¬ned in order to cover dynamic optimization with the model.
Due to space limitations we describe the actions informally, underlining only the necessary conditions for the
subsequent analysis of the systemsâ€™s ergodicity in the dynamic optimization case. Complete formal descriptions
of these actions leading to the probability transition functions (5) and (6) may be found in [16] and will is also
given in [24].
In the following (gen, n) stands for the signature of a generic agent that attempts to execute the following
actions:
6.1. Energy transfer action â€œgetâ€
Decision Î´gen,n
get for energy transfer is positive when there is at least one agent more on the same location. Agent
chooses randomly one of its neighbours and during the meeting, the energy is exchanged between agents, what
may be considered somewhat as a tournament (see tournament selection [35]). The direction of the energy ï¬‚ow is
determined by a probability distribution CMP : X Ã— U Ã— U â†’ M({0, 1}) dependent on agentsâ€™ ï¬tnesses and the
current state of the system, that will be described in details later in this section. It is to note, that the comparing
function CMP returns 0 if the ï¬rst agent passed as a parameter is better in the means of ï¬tness value from the
second one, and 1 otherwise. In the next state one of the agents receives a predeï¬ned part of energy Î”e from its
s
neighbour, which is assumed to satisfy Î”e â‰¤ ( i=1
qi )âˆ’1 .
Utilising the dynamic ï¬tness function deï¬nition given by (18) the following implementation of CMP function
may be proposed. Let us introduce the auxiliary random function:
W(x, gen1 , gen2 ) =

1âˆ’

if Fitness(x)(gen1 ) < Fitness(x)(gen2 )
otherwise

(19)

where âˆˆ (0, 1) is the arbitrary small constant. We assume the probability of returning 0 by CMP(x, gen1 , gen2 )
is W(x, gen1 , gen2 ), and returning 1 by 1 âˆ’ W(x, gen1 , gen2 ).
The probability of the event Fitness(x)(gen1 ) < Fitness(x)(gen2 ) might be computed in the following way.
Let us denote by Ii = [lgeni , hgeni ] + F(geni ), i = 1, 2 and by m = min{lgen1 , lgen2 }, m = max{hgen1 , hgen2 }. Then
Pr(Fitness(x)(gen1 ) < Fitness(x)(gen2 )) =

m

per1 (x)(m, Î±) per2 (x)(Î±, m)dÎ±.

(20)

m

It is clear, that the probability of returning zero or 1 is bounded from below by Î¹CMP = min{ , 1 âˆ’ } for any
state x âˆˆ X and any pair of genes gen1 , gen2 âˆˆ U.
6.2. Reproduction action â€œreprâ€
Decision Î´gen,n
repr for reproduction is positive when the energy of the agent performing the action is greater than
a reproduction threshold erepr and there is at least one agent more in the same location satisfying the same energy
condition. We assume that erepr â‰¤ 2Î”e. These agents create an oï¬€spring agent based on their solutions using a
predeï¬ned mixing operator. Part of the parentsâ€™ energy (e0 = n0 Â· Î”e, n0 is even) is passed to the oï¬€spring.
In order to introduce new individuals into the system, the action employs crossover and mutation probability
distributions, in order to localize new individuals in the system state space based on their parents. Assuming that
the distributions imposed by these two functions may change over time (being adapted in an arbitrary way), one
can follow a similar approach as in the case of get action, making them dependent on the current system state:
CROS S : X Ã— U Ã— U â†’ M(U), MUT : X Ã— U Ã— U â†’ M(U)
This delivers enough descriptive power to cover the possibilities of adapting the actual distributions imposed
by these two functions. Moreover, random immigrants may be introduced in the system (cf. Section 2), still
by modiï¬cation of the MUT probability distribution, that in certain cases may introduce completely random
individuals, instead of the ones based on parents genotypes.

1482

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

6.3. Cloning action â€œcloâ€
Decision Î´gen,n
for cloning is based on checking the amount of agentâ€™s energy only. An agent with enough
clo
energy strictly greater than Î”e, creates an oï¬€spring agent based on its solution (applying a predeï¬ned mutation
operator MUT ). Also in this case, this parameter considers the current state of the system, that makes possible
adaptation of the probability distribution imposed: MUT : X Ã— U â†’ M(U). Part of the parentâ€™s energy Î”e is
passed to the oï¬€spring.
In this case, similar to the deï¬nition of repr function, random immigrants may be introduced in the system
(cf. Section 2), still by modiï¬cation of the MUT probability distribution, that in certain cases may introduce
completely random individuals, instead of the ones based on parents genotypes.
6.4. Migration action â€œmigrâ€
Decision Î´gen,n
migr is positive when an agent has enough energy greater than emigr and there exists a location that
is able to accept it (the number of agents there is lower than its capacity). When these conditions are met the agent
is moved from its location to another. We assume, that emigr < sâˆ’1 .
Introducing of this action, along with supporting the allopatric speciation in EMAS model allows to support
dynamic optimization per se, by increasing the diversity of the population [2].
7. Ergodicity
Now we pass to study the ergodicity of the Markov chain imposed by the EMAS solving dynamic global
optimization problem deï¬ned by the ï¬tness introduced in Section 5. We will utilize the following theorem which
was formulated (see [19] and [17]) and proved (see [24]) for the more general case of EMAS that allow for parallel
execution of selected actions.
Theorem 1. (see Theorem 1 in [19]) Given the following assumptions:
1. The capacity of every location is greater than one, qi > 1, i = 1, . . . , s.
2. The graph of locations is connected.
3. Each active agent can be selected by its local agent with strictly positive probability, so
âˆƒ Î¹agsel > 0; âˆ€ i âˆˆ Loc, âˆ€ gen âˆˆ U, âˆ€ n âˆˆ P, âˆ€ x âˆˆ {y âˆˆ X; y(i, gen, n) > 0},
agseli (x)(gen, n) â‰¥ Î¹agsel .
4. The families of probability distributions being the parameters of EMAS have the uniform, strictly positive
lower bounds:
âˆƒ Î¹Ï‰ > 0; âˆ€ x âˆˆ X, gen âˆˆ U, Î± âˆˆ Act, Ï‰(gen, x)(Î±) â‰¥ Î¹Ï‰ ,
âˆƒ Î¹CMP > 0; âˆ€ gen, gen âˆˆ U, âˆ€x âˆˆ X CMP(x, gen, gen ) â‰¥ Î¹CMP ,
âˆƒ Î¹mut > 0; âˆ€gen, gen âˆˆ U, âˆ€x âˆˆ X MUT (x, gen)(gen ) â‰¥ Î¹mut ,
âˆƒ 0 < Î¹locsel < 1; âˆ€ x âˆˆ X, âˆ€ j âˆˆ Loc, locsel(x)( j) â‰¥ Î¹locsel .
We can construct a ï¬nite sequence of transitions between two arbitrarily chosen system states which may be passed
with strictly positive probability. Moreover we can deliver the upper bound of the number of such transitions,
which can be eï¬€ectively computed based on the systemâ€™s parameters.
It is easy to observe, that all assumptions of this theorem hold for the EMAS case discussed in this paper.
First, the case of the Markov model describing execution of mutually excluded actions only can be obtained by
setting the probability of parallel actions processing Î¶ loc â‰¡ 0 in the formula deï¬ning the probability transition
(see (19) in [19]). Moreover we have already proved, that the probability of agent comparing CMP is uniformly
bounded from below by the strictly positive constant (see Section 6.1), that satisï¬es the second inequality of the
assumption 4 of Theorem 1. Other assumptions are also satisï¬ed, because the EMAS under consideration does
cover all remaining features of the one presented in [19].
Remark 1. The above Theorem 1 makes all states containing the extrema reachable in a ï¬nite number of states,
thus EMAS satisï¬es an asymptotic guarantee of success [8], [9]. Moreover the Markov chain modelling EMAS
solving dynamic global optimization problem under consideration is ergodic.

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

1483

The possible impact of the Markov chain ergodicity on the dynamic optimization problem is twofold. First,
it is easy to see, that independent on the ï¬tness changes all the possible system states are reachable from every
arbitrarily chosen state. In other words, two arbitrarily chosen states may be transferred one to another in the ï¬nite
number of steps. This brings the second possible impact on dynamic optimization. In the extended version of
the paper giving full formal proof for the stationary optimization case [24] upper bounds of the number of steps
needed to connect any two states are computed. Therefore, this number may be used as an estimate of adaptation
capabilities, and be used along with the prediction-based approaches [5] to analyse the possibilities and the time
of reaching the vicinity of the changing optimum.
8. Conclusions
In this paper, a formal model for EMAS presented in [16, 17, 19] has been recalled. The structure and behaviour of EMAS modelled by Markov chain were shown. The model was appropriately modiï¬ed to cover the
dynamic optimization problem by changing some of its features (mutation, crossover and evaluation probability
distributions) to support selected ways of solving these problems: hypermutation, allopatric speciation and random immigrants. Actual experiments conducted in a system constructed according to the described model were
presented in [36]. One of the most important matters was the detailed deï¬nition (with an example) of the random
function CMP used to compare the agents in the course of action get.
One of the main implications of the analysis conducted here is the formulation and proof draft of Theorem
1 stating that the Markov chain based model of EMAS is stationary and ergodic. This will lead to an important
conclusion, namely that EMAS possesses the feature of asymptotic guarantee of success. The assumed way of
proving this theorem was also sketched out. The detailed description of the EMAS Markov model along with
complete proof of ergodicity have been already formulated in [24].
The possible impact of the Markov chain ergodicity on the dynamic optimization problem analysis is the
observation, that regardless the ï¬tness changes all the possible system states are reachable from every arbitrarily
chosen state. The estimate of upper bound of the number of steps connecting two arbitrarily chosen states of the
system space in the full paper [24] may allow to further research the features of dynamic optimizing EMAS, e.g.,
help in implementing prediction based approaches.
The presented extension of the formal framework may be adapted to model other simulation or computing
problems. Selected possible examples are: evolutionary programming in multi-robot environment [37], hierarchic
genetic search [38] or co-evolutionary optimization [39].
Acknowledgements
The research presented here was partially supported by the grants â€œBiologically inspired mechanisms in planning and management of dynamic environmentsâ€ No. N N516 500039, â€œParallel hierarchical adaptive algorithm
for solving challenging inverse problemsâ€ No. N N519 447739, and â€œMulti-model, multi-criterial and multiadaptive strategies for solving the inverse tasksâ€, No. DEC-2011/03/B/ST6/01393, all funded by the Polish National Science Centre.
References
[1] D. H. Wolpert, W. G. Macready, No free lunch theorems for optimization, IEEE Transactions on Evolutionary Computation 1 (1) (1997)
67â€“82.
[2] Y. Jin, J. Branke, Evolutionary optimization in uncertain environmentâ€”a survey, IEEE Transactions on Evolutionary Computation 9
(2005) 303â€“317.
[3] R. Storn, K. Price, Diï¬€erential evolutionâ€”a simple and eï¬ƒcient heuristic for global optimization over continuous spaces, Journal of
Global Optimization 11.
[4] D. Goldberg, R. Smith, Nonstationary function optimization using genetic algorithms with dominance and diploidy, in: Proc. of the
Second International Conference on Genetic Algorithms, 1987, pp. 59â€“68.
[5] A. Simoes, E. Costa, Improving prediction in evolutionary algorithms for dynamic environments, in: Proc. of the 2009 Genetic and
Evolutionary Computation Conference, 2009, pp. 875â€“888.
[6] M. Vose, The Simple Genetic Algorithm: Foundations and Theory, MIT Press, Cambridge, MA, USA, 1998.
[7] R. Schaefer, Foundations of global genetic optimization, Springer Verlag, 2007.

1484

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 â€“ 1484

[8] R. Horst, P. Pardalos, Handbook of Global Optimization, Kluwer, 1995.
[9] A. Rinnoy Kan, G. Timmer, Stochastic global optimization methods, Mathematical Programming 39 (1987) 27â€“56.
[10] T. E. Davis, J. C. Principe, A simulated annealing like convergence theory for the simple genetic algorithm, in: Proc. of the Fourth
International Conference on Genetic Algorithms, San Diego, CA, 1991, pp. 174â€“181.
[11] J. Suzuki, A Markov Chain Analysis on a Genetic Algorithm, in: S. Forrest (Ed.), Proc. of the 5th ICGA, Morgan Kaufmann, 1993, pp.
146â€“154.
[12] G. Rudolph, Massively parallel simulated annealing and its relation to evolutionary algorithms, Evolutionary Computation 1 (1994)
361â€“383.
[13] D. Goldberg, P. Segrest, Finite Markov chain analysis of genetic algorithms, in: Proceedings of the Second International Conference on
Genetic Algorithms on Genetic algorithms and their application, L. Erlbaum Associates Inc., Hillsdale, NJ, USA, 1987, pp. 1â€“8.
[14] S. Mahfoud, Finite Markov Chain Models of an Alternative Selection Strategy for the Genetic Algorithm, Complex Systems 7 (1991)
155â€“170.
[15] J. Horn, Finite Markov Chain Analysis of Genetic Algorithms with Niching, in: Proceedings of the Fifth International Conference on
Genetic Algorithms, Morgan Kaufmann, 1993, pp. 110â€“117.
[16] A. Byrski, R. Schaefer, Stochastic model of evolutionary and immunological multi-agent systems: Mutually exclusive actions, Fundamenta Informaticae 95 (2-3) (2009) 263â€“285.
[17] R. Schaefer, A. Byrski, M. SmoÅ‚ka, Stochastic model of evolutionary and immunological multi-agent systems: Parallel execution of
local actions, Fundamenta Informaticae 95 (2-3) (2009) 325â€“348.
[18] A. Byrski, R. Schaefer, M. SmoÅ‚ka, Asymptotic features of parallel agent-based immunological system, in: T. BurczyÂ´nski, J. KoÅ‚odziej,
A. Byrski, M. Carvalho (Eds.), Proc. of 25th European Conference on Modelling and Simulation, 2011.
[19] A. Byrski, R. Schaefer, M. SmoÅ‚ka, C. Cotta, Asymptotic analysis of computational multi-agent systems, in: R. S. et al. (Ed.), PPSN
2010 Proceedings, Vol. 3256 of LNCS, Springer-Verlag, 2010.
[20] A. Byrski, R. Schaefer, Formal model for agent-based asynchronous evolutionary computation, in: IEEE Congress of Evolutionary
Computation, Trondheim, Norway (submitted to), IEEE, 2009.
[21] K. Cetnarowicz, M. Kisiel-Dorohinicki, E. Nawarecki, The application of evolution process in multi-agent world (MAW) to the prediction system, in: M. Tokoro (Ed.), Proc. of the 2nd Int. Conf. on Multi-Agent Systems (ICMASâ€™96), AAAI Press, 1996.
[22] M. Kisiel-Dorohinicki, G. Dobrowolski, E. Nawarecki, Agent populations as computational intelligence, in: L. Rutkowski, J. Kacprzyk
(Eds.), Neural Networks and Soft Computing: Proc. of 6th International Conference on Neural Networks and Soft Computing Location:
ZAKOPANE, POLAND Date: JUN 11-15, 2002, Advances in Soft Computing, Springer-Verlag, 2003, pp. 608â€“613.
[23] A. Byrski, M. Kisiel-Dorohinicki, Agent-based model and computing environment facilitating the development of distributed computational intelligence systems, in: Proc. of ICCS 2009, Vol. 5545 of Computational Science, Springer, 2009, pp. 865â€“874.
[24] A. Byrski, R. Schaefer, M. SmoÅ‚ka, C. Cotta, Asymptotic guarantee of success for multi-agent memetic systems, Bulletin of the Polish
Academy of Sciences: Technical Sciences, accepted for printing.
[25] A. Byrski, M. Kisiel-Dorohinicki, E. Nawarecki, Agent-Based Evolution of Neural Network Architecture, in: M. Hamza (Ed.), Proc. of
the IASTED Int. Symp.: Applied Informatics, IASTED/ACTA Press, 2002.
[26] A. Byrski, M. Kisiel-Dorohinicki, Immunological selection mechanism in agent-based evolutionary computation, in: Proc. of IIS:
IIPWM â€™05 conference : Gdansk, Poland, Advances in Soft Computing, Springer, 2005.
[27] A. Byrski, M. Kisiel-Dorohinicki, Agent-based evolutionary and immunological optimization, in: Computational Science - ICCS 2007,
7th International Conference, Beijing, China, May 27 - 30, 2007, Proceedings, Springer, 2007.
[28] A. Byrski, M. Kisiel-Dorohinicki, Immune-based optimization of predicting neural networks, in: V. Sunderam, G. Van Albada, P. Sloot
(Eds.), Proc. of 5th International Conference on Computational Science (ICCS 2005) Location: Atlanta, GA Date: MAY 22-25, 2005,
Vol. 3516 of Lecture Notes in Computer Science, Springer-Verlag, 2002, pp. 703â€“710.
[29] A. Byrski, M. Kisiel-Dorohinicki, M. Carvalho, A crisis management approach to mission survivability in computational multi-agent
systems, Computer Science 11 99â€“113.
[30] D. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Massachusetts: Addison-Wesley, 1989.
[31] A. Byrski, R. Schaefer, Formal model for agent-based asynchronous evolutionary computation, in: Proc. of IEEE Congress on Evolutionary Computation Location: Trondheim, NORWAY Date: MAY 18-21, 2009, 2009.
[32] M. Kisiel-Dorohinicki, Agent-Oriented Model of Simulated Evolution, in: W. I. Grosky, F. Plasil (Eds.), SofSem 2002: Theory and
Practice of Informatics, Vol. 2540 of LNCS, Springer-Verlag, 2002.
[33] N. Krasnogor, J. Smith, A tutorial for competent memetic algorithms: Model, taxonomy, and design issues, IEEE Transactions on
Evolutionary Computation 9 (5) (2005) 474â€“488. doi:10.1109/TEVC.2005.850260.
[34] N. Krasnogor, S. Gustafson, A study on the use of â€œself-generationâ€ in memetic algorithms, Natural Computing 3 (2004) 53â€“76.
[35] Z. Michalewicz, Genetic Algorithms Plus Data Structures Equals Evolution Programs, Springer-Verlag New York, Inc., Secaucus, NJ,
USA, 1994.
[36] M. Kisiel-Dorohinicki, Agent-based evolutionary computing in dynamic optimization problems, Computing and Informaticsâ€”
Submitted for review X (X).
[37] A. KubÂ´Ä±k, Distributed genetic algorithm: a case-study of evolution by direct exchange of chromosomes, Computing and Informatics
23 (6) (2004) 575â€“596.
[38] P. Jojczyk, R. Schaefer, Global impact balancing in the hierarchic genetic search, Computing and Informatics 28 (2).
[39] R. DreË™zewski, Co-evolutionary multi-agent system with speciation and resource sharing mechanisms, Computing and Informatics 25 (4).

