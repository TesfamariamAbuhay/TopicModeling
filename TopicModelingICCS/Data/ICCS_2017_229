Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 535‚Äì544

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it
space is on
reserved
for the Procedia
header,
not use
it June 2017,
International This
Conference
Computational
Science,
ICCSdo2017,
12-14
Zurich, Switzerland

Replicated Synchronization for Imperative BSP Programs
Replicated Synchronization for Imperative BSP Programs
1,2 , FreÃÅdeÃÅric Dabrowski2 , Wadoud Bousdira2 ,
Arvid Jakobsson
Replicated
Synchronization
for Imperative
BSP Programs
1,2
2
2
3 , and Gaetan
Arvid Jakobsson
FreÃÅdeÃÅric Dabrowski
, Wadoud
Hains1 Bousdira ,
FreÃÅdeÃÅric ,Loulergue
3
1,2 ,Loulergue
2 , Wadoud
, and Gaetan
Hains1 Bousdira2 ,
FreÃÅdeÃÅric
Arvid Jakobsson
FreÃÅdeÃÅric Dabrowski
1
Huawei Technologies3 France Research Center 1
1
and Gaetan
Hains
FreÃÅdeÃÅric
Loulergue
firstname.lastname@huawei.com
Huawei
Technologies ,France
Research Center
2

Univ. OrleÃÅans,
Centre Val de Loire, LIFO EA 4022, OrleÃÅans, France
firstname.lastname@huawei.com
1 INSA
Huawei Technologies France Research Center
firstname.lastname@univ-orleans.fr
Univ. OrleÃÅans, INSA
Centre Val de Loire, LIFO EA 4022, OrleÃÅans, France
firstname.lastname@huawei.com
School2 of Informatics, Computing
and Cyber Systems, Northern Arizona University, USA
firstname.lastname@univ-orleans.fr
Univ. OrleÃÅans, INSA Centre Val de Loire, LIFO EA 4022, OrleÃÅans, France
frederic.loulergue@nau.edu
School of Informatics, Computing
and Cyber Systems, Northern Arizona University, USA
firstname.lastname@univ-orleans.fr
frederic.loulergue@nau.edu
School of Informatics, Computing and Cyber Systems, Northern Arizona University, USA
frederic.loulergue@nau.edu
2

3
3
3

Abstract
Abstract
The
BSP model (Bulk Synchronous Parallel) simplifies the construction and evaluation of parThe
BSP
model (Bulk
Synchronous
simplifies
the construction
evaluation
of parallel
algorithms,
with its
simplified Parallel)
synchronization
structure
and cost and
model.
Nevertheless,
Abstract
allel algorithms,
with its simplified
synchronization
structureerrors.
and cost
model. with
Nevertheless,
imperative
BSP
programs
can
suffer
from
synchronization
Programs
textually
The BSP model (Bulk Synchronous Parallel) simplifies the construction and evaluation of parimperative
BSPare
programs
can
suffer
from
errors.
Programs
with textually
aligned
barriers
freeitsfrom
such
errors,
andsynchronization
this structure
eases
program
comprehension.
We
allel algorithms,
with
simplified
synchronization
structure
and
cost model.
Nevertheless,
aligned barriers
are free
from such of
errors,
andinference
this structure
eases
program
comprehension.
We
propose
a
simplified
formalization
barrier
as
data
flow
analysis,
which
verifies
statimperative BSP programs can suffer from synchronization errors. Programs with textually
propose
a simplified
formalization
of
barrierhas
inference
as data
flow analysis,which
whichisverifies
statically
whether
an
imperative
BSP
program
replicated
synchronization,
a
sufficient
aligned barriers are free from such errors, and this structure eases program comprehension. We
ically whether
an imperative
BSP program has replicated synchronization, which is a sufficient
condition
textual
barrier alignment.
propose a for
simplified
formalization
of barrier inference as data flow analysis, which verifies statcondition for textual barrier alignment.
ically
whether
an
imperative
BSP
program
has replicated
synchronization,
which inference
is a sufficient
Keywords:
ParallelPublished
programming,
bulk
synchronous
parallelism,
static analysis, barrier
¬©
2017 The Authors.
by Elsevier
B.V.
Peer-review
responsibility
of the
scientific
committee ofparallelism,
the International
Conference
Computational
Science
condition under
for
textual
barrier
alignment.
Keywords:
Parallel
programming,
bulk synchronous
static
analysis,onbarrier
inference
Keywords: Parallel programming, bulk synchronous parallelism, static analysis, barrier inference

1 Introduction
1 Introduction
Parallel architectures are ubiquitous, and the number of processing elements keeps increasing.
1
Introduction
Parallelare
architectures
are ubiquitous,
and the number
processing
elements
keeps
There
existing architectures
for embedded
systemsofwith
hundreds
of cores
[4]. increasing.
Therefore
There are
existing
architectures
for for
embedded
systems
withare
hundreds
of cores [4].
Therefore
models
programming
libraries
scalable
parallelism
necessary.
Parallel and
architectures
are ubiquitous,
and
the number
of processing
elementsBulk
keepssynchronous
increasing.
models
and(BSP)
programming
libraries
for scalable
parallelism
are
necessary.
Bulk synchronous
parallelism
is
such
a
model
[12].
BSP
provides
a
high
degree
of
abstraction
like
PRAM
There are existing architectures for embedded systems with hundreds of cores [4]. Therefore
parallelism
(BSP)
is such
a model
[12].
BSP provides
a high degree
ofgeneral
abstraction
like parallel
PRAM
models
and
yet
allows
portable
and
predictable
performance
on
any
purpose
models and programming libraries for scalable parallelism are necessary. Bulk synchronous
models
and
yet
allows
portable
and
predictable
performance
on
any
general
purpose
parallel
architecture.
BSPlib
[6] is aa model
C API[12].
proposal
BSP programming
mode.like PRAM
parallelism (BSP)
is such
BSP for
provides
a high degree in
of direct
abstraction
architecture.
BSPlibis[6]a ispotential
a C APIsource
proposal
for BSP
programming
in direct
mode.BSPlib proSynchronization
of
errors
in
imperative
BSP
programs.
models and yet allows portable and predictable performance on any general purpose
parallel
Synchronization
source
of errors
in programming
imperative
BSP
programs.
programs
interleave
the iscode
which
handles
local
computation
and code
handles
synchronarchitecture.
BSPlib
[6]a ispotential
a C API
proposal
for
BSP
inwhich
direct
mode.BSPlib
grams interleave
the code which
handles
local computation
and but
codehard
which
handles
synchronization.
As
a
consequence,
incorrect
programs
are
easy
to
write
to
debug.
Synchronization is a potential source of errors in imperative BSP programs. BSPlib proization.
As a consequence,
incorrect
programs
are easy
to write
but hard
debug.is becoming
In response
evolving
architectures,
embedded
software
developed
at to
Huawei
grams
interleavetothe
code which
handles local
computation
and
code which
handles
synchronIn
response
to
evolving
architectures,
embedded
software
developed
at
Huawei
is becoming
increasingly
dependent
on
exploiting
parallelism
efficiently
and
safely.
Engineers
with
little or
ization. As a consequence, incorrect programs are easy to write but hard to debug.
increasingly
dependent
on exploiting
parallelism
efficiently
andwith
safely.
Engineers
with
little
or
no
prior
experience
in
writing
parallel
codes
must
be
provided
languages
and
tools
which
In response to evolving architectures, embedded software developed at Huawei is becoming
no prior experience
incomplexity
writing parallel
codesprogramming.
must be provided
with languages
and
tools which
alleviate
the
inherent
of
parallel
We
propose
a
subset
of
BSPlib
that
increasingly dependent on exploiting parallelism efficiently and safely. Engineers with little or
alleviate the inherent complexity of parallel programming. We propose a subset of BSPlib that
no prior experience in writing parallel codes must be provided with languages and tools which
1
alleviate
theAuthors.
inherent
complexity
of parallel
1877-0509
¬© 2017 The
Published
by Elsevier
B.V. programming. We propose a subset of BSPlib that
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.123

1

536	

Replicated Synchronization forArvid
Imperative
BSP
Programs
Arvid Jakobsson et al.
Jakobsson
et al.
/ Procedia Computer Science 108C (2017) 535‚Äì544

ensures safe synchronization and codifies Software Engineering best practices, as well as a sound
static analysis that verifies whether any given BSPlib-program is in this subset.
Indeed, safe synchronization can be guaranteed by writing programs that have textually
aligned barriers. In such programs, each barrier is the result of a synchronization request from
the same source code location in all processes. It has already been noted that quality parallel
code has strict synchronization patterns, and there are analyses to enforce them [2, 13], but
our review of BSPlib programs in the wild suggests that the stricter convention of textually
aligned barriers is sufficient to capture a large majority of correct programs. Additionally, such
programs are conceptually simpler which help steer program design toward correctness, and
ease other validation steps such as code review. Our static analysis verifies that a program
has replicated synchronization: a statically inferable property which implies textually aligned
barriers and safe synchronization as a result. Therefore, it reconstitutes the backbone of a BSP
algorithm, which is its synchronization.
This static analysis poses higher requirements on the analyzed source code than existing
analyses such as Barrier Inference [2]. For this reason, it is defined directly on the syntax of
our language, assumes structured control flow and allows fewer synchronization patterns. This
is intentional, since our purpose is to carve out a stricter subset of the language that complies
with best practices and which nudges programmers toward correctness.
The main contribution of this paper is an adaptation of the Barrier Inference static analysis [2], formalized and proven, which verifies whether any given program has the replicated
synchronization, and which thus is in the safe subset of BSPlib programs. The synchronizationbackbone frames further static reasoning on the program, by giving knowledge of its degree of
parallelism. Thus, this analysis is an initial building block in our envisioned set of formally
justified static analyses for BSPlib programs as plug-ins for Frama-C [9], a framework for static
and dynamic analysis of C programs.
The paper is organized as follows. First we present the BSP model (Section 2) and a small
imperative BSP language, BSPlite (Section 3) with its operational semantics and describe the
safe language subset with textually aligned barriers. Section 4 is devoted to the static analysis
of this language. We discuss related work in Section 5 before concluding in Section 6.

2

The BSP Model

The BSP model offers an abstract model of parallel architectures, a model of parallel algorithms,
and a cost (performance) model. A BSP computer is a distributed memory machine. It has a set
of p processor-memory pairs, interconnected in such a way that point-to-point communications
are possible. It has also a global synchronization unit. This model is abstract in the sense that
any general purpose architecture can be seen as a BSP computer. For example, a cluster can
be seen as a BSP computer with global synchronization provided by software.
A BSP program is a sequence of super-steps. Each super-step proceeds in three phases. In
the local computation phase, each processor computes using only the data from local memory.
In the communication phase, processors can request and send data from other processors. The
synchronization phase ends a super-step. It is guaranteed that the data requested or sent during
the communication phase has reached its destination at the end of the synchronization barrier.
This constrained form of parallelism allows a realistic, yet simple, performance (or cost) model.
We omit discussing it for the sake of conciseness.
BSPlib [6] provides a small set of primitives for direct mode bulk synchronous parallel
programming. BSPlib follows the Single Program Multiple Data (SPMD) paradigm. It gives
access to the process identifier, through a function bsp pid(). It allows to write only one
2

	

Replicated Synchronization forArvid
Imperative
BSP
Programs
Arvid Jakobsson et al.
Jakobsson
et al.
/ Procedia Computer Science 108C (2017) 535‚Äì544

program for all the processors, but the program can behave differently depending on the process
identifier. If we think of an SPMD program P as taking the process identifier as argument (i.e.
the value returned by bsp pid()), informally the parallel behavior is: P (0)‚à•P (1)‚à• . . . ‚à•P (p ‚àí 1)
where p (returned by bsp nprocs()) is the number of processors of the BSP machine that runs
the program and ‚à• denotes asynchronous parallel composition.
For the communication phase, BSPlib provides two styles of communication: message
passing and direct remote memory access. We do not present them as they are logically independent from the synchronization barriers. The synchronization phase occurs when all the
processors call bsp sync(). Under correct dynamic use, bsp sync() generates the sequence of
super-steps that is the structure of a BSP computation.
Yet it is quite easy to write an incorrect SPMD program from the perspective of synchronization. For example, the one-line program: if (2*bsp pid() < bsp nprocs()) bsp sync();
is incorrect because only half of the processors are calling the synchronization barrier. The proposed analysis statically verifies whether a program is free from such errors. This development
forms an initial step towards a secure and statically verified basis for BSPlib programming.

3

The BSPlite Language

BSPlite is a small imperative language with concurrency primitives. Communication and local
errors are not modelled, since the focus is on synchronization. Expressions and instructions are
defined by the following grammar:
expr
bexpr
cmd

‚àã e ‚à∂‚à∂= nprocs ‚à£ pid ‚à£ x ‚à£ n ‚à£ e + e ‚à£ e ‚àí e ‚à£ e √ó e
‚àã b ‚à∂‚à∂= true ‚à£ false ‚à£ e < e ‚à£ e = e ‚à£ b or b ‚à£ b and b ‚à£ !b
‚àã c ‚à∂‚à∂= x ‚à∂= e ‚à£ skip ‚à£ sync ‚à£ c; c ‚à£ if b then c else c end ‚à£ while b do c end

where the symbolic expressions nprocs and pid denote the number of processors of the BSP
machine and the process identifier respectively, x ‚àà X (the set of variables) and n is a numeral
denoting a natural number.
The semantics of BSPlite models BSPlib, but is simplified to exclude communication and
local errors such as division by zero. The semantics is parameterized by the number p > 0 of
processors of the underlying BSP machine. The set of processors identifiers is P = {0, . . . , p ‚àí 1}.
The semantics of numerical and boolean expressions, given by ‚ãÖi , is standard and omitted for
brevity, but we note that pid i = i and nprocsi = p.
The operational semantics for BSPlite-programs is divided into local and global rules operating on environment vectors. Intuitively, the local rules compute the new state of one component
in the environment vector, corresponding to one processor. An eventual continuation describes
the next step of local computation. The global rules compute the new state of a complete
environment vector by applying the local rules to each component and, after synchronization,
possibly performing any remaining computations by re-applying the global rules.
The semantics of local computation is given by the relation ‚Üíi , indexed by i ‚àà P:
‚Üíi ‚à∂ cmd √ó Œ£ √ó T √ó Œ£

Œ£=X‚ÜíN

T = {Ok} ‚à™ {W ait(c) ‚à£ c ‚àà cmd }

where T is the set of termination states, with Ok denoting end of computation and W ait(c) a
remaining computation to execute. The relation is defined by a set of inference rules (Figure 1).
The rules are read as implications: when the premises in the numerator holds, the conclusion in
the denominator defines a member of the relation. The set of memory states in Œ£ are mappings
from variables to values. We write ‚ü®c, œÉ‚ü© ‚Üíi ‚ü®t, œÉ ‚Ä≤ ‚ü© for (c, œÉ, t, œÉ ‚Ä≤ ) ‚àà ‚Üíi . The operational semantics
3

537

538	

Arvid Jakobsson et al. / Procedia Computer Science 108C (2017) 535‚Äì544
Replicated Synchronization for Imperative BSP Programs
Arvid Jakobsson et al.

‚ü®X ‚à∂= e, œÉ‚ü© ‚Üíi ‚ü®Ok, œÉ[X ‚Üê ei (œÉ)]‚ü©

‚ü®skip, œÉ‚ü© ‚Üíi ‚ü®Ok, œÉ‚ü©

‚ü®c1 , œÉ‚ü© ‚Üíi ‚ü®Ok, œÉ ‚Ä≤‚Ä≤ ‚ü©

‚ü®c2 , œÉ ‚Ä≤‚Ä≤ ‚ü© ‚Üíi t

‚ü®c1 ; c2 , œÉ‚ü© ‚Üíi t

bi (œÉ) = true

‚ü®sync, œÉ‚ü© ‚Üíi ‚ü®W ait(skip), œÉ‚ü©

‚ü®c1 , œÉ‚ü© ‚Üíi t

‚ü®if b then c1 else c2 end, œÉ‚ü© ‚Üíi t

bi (œÉ) = true ‚ü®c, œÉ‚ü© ‚Üíi ‚ü®Ok, œÉ ‚Ä≤ ‚ü©
‚ü®while b do c end, œÉ ‚Ä≤ ‚ü© ‚Üíi t
‚ü®while b do c end, œÉ‚ü© ‚Üí t
bi (œÉ) = true
i

‚ü®c1 , œÉ‚ü© ‚Üíi ‚ü®W ait(c‚Ä≤1 ), œÉ ‚Ä≤ ‚ü©

‚ü®c1 ; c2 , œÉ‚ü© ‚Üíi ‚ü®W ait(c‚Ä≤1 ; c2 ), œÉ ‚Ä≤ ‚ü©

bi (œÉ) = false

‚ü®c2 , œÉ‚ü© ‚Üíi t

‚ü®if b then c1 else c2 end, œÉ‚ü© ‚Üíi t
bi (œÉ) = false

‚ü®while b do c end, œÉ‚ü© ‚Üíi ‚ü®Ok, œÉ‚ü©
‚ü®c, œÉ‚ü© ‚Üí ‚ü®W ait(c‚Ä≤ ), œÉ ‚Ä≤ ‚ü©
i

‚ü®while b do c end, œÉ‚ü© ‚Üíi ‚ü®W ait(c‚Ä≤ ; while b do c end), œÉ ‚Ä≤ ‚ü©
Figure 1: Local operational semantics

‚àÄi ‚àà P, ‚ü®C i, E i‚ü© ‚Üíi ‚ü®W ait(C ‚Ä≤ i), E ‚Ä≤ i‚ü©
‚ü®C, E‚ü© ‚Üí S

‚àÄi ‚àà P, ‚ü®C i, E i‚ü© ‚Üí ‚ü®Ok, E i‚ü©
‚ü®C, E‚ü© ‚Üí E ‚Ä≤
i

‚Ä≤

‚ü®C ‚Ä≤ , E ‚Ä≤ ‚ü© ‚Üí S

‚àÄi ‚àà P, ‚ü®C i, E i‚ü© ‚Üíi ‚ü®T i, E ‚Ä≤ i‚ü©
‚àÉi ‚àà P, T i = Ok
‚àÉj ‚àà P, T j = W ait(c‚Ä≤ )
‚ü®C, E‚ü© ‚Üí ‚Ñ¶

Figure 2: Global operational semantics

of BSPlite programs is given by the relation ‚Üí:

‚Üí ‚à∂ (P ‚Üí cmd ) √ó (P ‚Üí Œ£) √ó ((P ‚Üí Œ£) ‚à™ {‚Ñ¶})

We write ‚ü®C, E‚ü© ‚Üí S for (C, E, S) ‚àà ‚Üí, and define this relation by the rules of Figure 2. The
third rule specifies that a synchronization error (‚Ñ¶) occurs when all processes terminate and
the termination state of at least two processes is incoherent. The goal of the static analysis in
Section 4 is to rule out this kind of error for a given program.
Textually aligned barriers A sufficient criteria for the absence of synchronization errors in a
program is having textually aligned barriers. A program has textually aligned barriers when
all processors agree on the evaluation of all guard-expressions in the program which lead up to
any sync-primitive. We say that these guard-expressions are uniform. As a consequence, either
all processors synchronize or none of them do. If they synchronize, all requests to synchronize
come from the same occurrence of the sync-primitive: same occurrence both in terms of source
code location and iteration count for sync-primitives in loops.
Note that some programs do not have textually aligned barriers, but still behave well in the
4

	

Arvid Jakobsson et al. / Procedia Computer Science 108C (2017) 535‚Äì544
Replicated Synchronization for Imperative BSP Programs
Arvid Jakobsson et al.

1,2

3

4

6

5

8
5m

7
Figure 3: Control flow graph of cnok

[I ‚à∂= 10]1 ; [X ‚à∂= pid ]2 ;
while [0 < I]3 do
[sync]4 ;
5m
if [X = 3]5 then [sync]6 else [I ‚à∂= 0]7 [end] ;
[I ‚à∂= I ‚àí 1]8
end
Figure 4: Example program cnok

operational semantics. For instance, in any environment with at least two processors who do
not agree on the value of b, the following program does not have textually aligned barriers:
c = if b then (sync) else (skip) end; if b then (skip) else (sync) end;
and so at least two processors will terminate local computation by calling distinct syncprimitives. The operational semantics will not return an error for this execution, since each
process will execute one of the sync-primitives, but the program does not have replicated synchronization and so will not be accepted by the analysis in the next section. However, from a
Software Engineering best practices point of view, this kind of synchronization pattern is discouraged, since divergence of control flow between processors impedes program comprehension.

4

Static Analysis

Textual alignment of barriers can be statically verified using an analysis inspired by the type
and effect-system of Aiken & Gay for Barrier Inference [2]. We refer to this static property
as replicated synchronization. We reformalize Barrier Inference as a data-flow analysis,
restricted to identify programs with replicated synchronization, and prove it correct with respect
to the semantics of BSPlite.
Data-flow analysis is a standard technique used in compilation and static analysis. Programs
are translated into a system of constraints on the properties to be calculated for each program
point. The solution of the system is found by fix-point iteration. Convergence is guaranteed by
requiring that the property space of the analysis is a complete lattice satisfying the Ascending
Chain Condition, and that the constraints form a monotone function over this space.
The analysis is divided into two phases: (1) a data-flow analysis which under-approximates
the set of ‚Äúpid -independent‚Äù variables at each program point ‚Äî variables which do not have a
data- nor control-dependency on the special pid -expression; (2) using the result of the data-flow
analysis, it is verified that each branching statement has a pid -independent guard-expression,
or that its sub-statements are syntactically synchronization-free.
The idea behind the first step is to identify statically those expressions which must be
uniform when the program is executed. Being independent on the pid -expression is a sufficient
criteria for this: except for the pid -expression and those who depend on it, all expressions
evaluate to the same value over all processors.
The program cnok in Figure 4 is used as running example. This program is erroneous, since
the sync at label 6 will only be executed by one processor if p ‚â• 4.

4.1

Pid-independence data-flow analysis

Control Flow Graph (CFG) and merge-nodes The analysis works on programs labelled
with elements  ‚àà L, with an additional label on the end of conditionals:

5

539

540	

Replicated Synchronization forArvid
Imperative
BSP
Arvid Jakobsson et al.
Jakobsson
et al.Programs
/ Procedia Computer Science 108C (2017) 535‚Äì544

A)

n()



x()

t()

n()
B)

f ()

c1


b()

C)

n()

t()

f ()

c1

t(m )

m

c2


f (m )

x(m )

A) c ‚àà {[skip] , [y ‚à∂= e] , [sync] } B) c = while [b] do c1 end C) c = if [b] then c1 else c2 [end]

m

Figure 5: CFG and edge-functions for instruction-, conditional- and loop-statements

if [b] then c1 else c2 [end] . The CFG of a program has one node per label in the program.
Since conditionals have two labels, an additional merge-node corresponds to the join of control
flow at the exit of the conditional. Figure 3 contains the CFG of cnok . Here, 5m is the mergenode of the if-statement 5. To refer to the in- and out-going edges of each node, we define
n, x, f, t, b ‚à∂ L ‚Üí L √ó L. They return, respectively, the entry-, exit-, false-, true-, and back-edge
of each node, as illustrated by Figure 5. For merge-nodes, true- and false-edges refer to the
incoming edges from each respective branch.
m

Analysis domain The data-flow analysis calculates an abstract state for each node in the
CFG of the program. An abstract state l = (V, p) is a tuple consisting of: (1) an underapproximation of the set of variables V considered pid -independent; (2) a path abstraction p.
To refer to the first and second component, œÄ 1 (l) = V and œÄ 2 (l) = p are used. The set of
abstract states is L.
A path in Seq(L) is associated with each location in a program. The path of a location is
a sequence of labels. It is obtained by collecting the label of each branching statement in the
abstract syntax tree on the path from the root-node to that location. Let LÃÑ be the set of marked
labels. The semantic path in Seq(L ‚à™ LÃÑ) at some location and step of an execution is the path
of the location, where each label is marked if the guard-expression was not pid -independent
when evaluated last in the execution. Marking is an idempotent operation, written ¬Ø for both
labels and sequences. The path abstraction in S = (Seq(L ‚à™ LÃÑ) ‚à™ {, ‚ä∫}) for a location is
a conservative over-approximation of all the semantic paths of that location, at any step and
execution. If a label is marked in any of those semantic paths, then it must be marked in the
path abstraction. To form a complete lattice,  and ‚ä∫ are added.
Consider the program cnok in Figure 4. The path at label 6 is 3.5. A process reaching this
label in the first iteration of the loop will have the semantic path 3.5ÃÑ, since the evaluation of the
guard-expression at 5 depends on pid . Thus a path abstraction at 6 must have label 5 marked.
Path abstractions reflect the program nesting of the program and the replication of guards.
Path abstractions are ordered by ‚™Ø. Intuitively, if p ‚™Ø p‚Ä≤ then both p and p‚Ä≤ are path
abstractions for the same program point, but p‚Ä≤ can be a more conservative over-approximation
than p, i.e., if a label is marked in p then it must also be in p‚Ä≤ (or p‚Ä≤ is ‚ä∫). The least upper
bound, written ‚äî, of two path abstractions referring to the same location is the path abstraction
where each label is marked when the corresponding label is marked in any of the arguments. If
the arguments do not refer to the same location, ‚ä∫ is their least upper bound. Now L forms a
complete lattice ordered by ‚äë = (‚äá, ‚™Ø), ensuring that the fix-point iteration will terminate.
Path abstractions are modified by concatenating labels to existing path abstractions (.) and
by merging path abstractions (‚ñΩ), defined in Figure 6. Merging removes the last label from
two path abstractions of the same location, and returns the least upper bound of the rest.

6

	

Replicated Synchronization forArvid
Imperative
BSP
Arvid Jakobsson et al.
Jakobsson
et al.Programs
/ Procedia Computer Science 108C (2017) 535‚Äì544

(.)

‚à∂

S √ó (L ‚à™ LÃÑ) ‚Üí S
(concat.)
‚éß
‚é™p ‚à∂  if p ‚àà Seq(L ‚à™ LÃÑ)
‚é™
p. = ‚é®
‚é™p
oth.
‚é™
‚é©
where (‚à∂) is concatenation for sequences

‚ñΩ‚à∂S √óS ‚ÜíS
‚ñΩ is the least
p. ‚ñΩ p‚Ä≤ .‚Ä≤
 ‚ñΩ p.
‚ñΩ
p ‚ñΩ p‚Ä≤

(merge)
commutative operator s.t.
= p ‚äî p‚Ä≤ if p. = p‚Ä≤ .‚Ä≤
= p
= 
= ‚ä∫
oth.

Figure 6: Concatenation (.) and merge (‚ñΩ) of path abstractions.

œÜd (e, V ) = pid ‚àà/ exprs(e) ‚àß f v(e) ‚äÜ V œÜc (p)
‚éß
‚é™
‚é™ if œÜd (e, V )
vdep((V, p), e, x) =
cdep(, e, V ) = ‚é® ¬Ø
‚é™
 oth.
‚é™
‚é©

= p ‚àà Seq(L) ‚à™ {}
‚éß
‚é™
‚é™V ‚à™ {x} if œÜd (e, V ) ‚àß œÜc (p)
‚é®
‚é™
V ‚àñ {x} oth.
‚é™
‚é©

Figure 7: The predicates œÜd and œÜc and the functions cdep and vdep

Data-flow equations The equation system P I(c) defines the abstract state in each node,
given by In ‚à∂ L ‚Üí L, in terms of the transfer-function of each edge, given by Out ‚à∂ L √ó L ‚Üí L.
Before defining P I(c), we define the predicates œÜd (pid data-independent) and œÜc (pid
control-independent) and the functions cdep and vdep (Figure 7) where exprs(e) gives the subexpressions of an expression e and f v(e) is the set of free variables. The function cdep(, e, V )
is used at branching statements and marks the label  unless the guard-expression e is dataindependent on pid , as determined by the predicate œÜd (e, V ). This predicate holds if e does
not contain the symbolic expression pid nor any variable which depends on it. The function
vdep((V, p), e, x) is used at assignments to conditionally add (respectively remove) the assigned
variable x to the set of pid -independent variables V , when the assigned expression e is (respectively may not be) data- and control-independent on pid as determined by the predicates
œÜd (e, V ) and œÜc (p). The latter holds for a path abstraction when it contain any marked labels.
For each node labelled  in the CFG, depending on the type of command c‚Ä≤ it belongs to,
the equation system P I(c) defines the functions In and Out in terms of each other (Figure 8).
Each equation defines the abstract state for the corresponding node or edge. The set of pid independent variables at a node are those which are pid -independent on all incoming edges.
Outgoing edges on nodes of assignments adds or removes the variable to the set using vdep.
The data-flow equations define the path abstraction of each node so that it follows the
nesting structure of the program. Accordingly, instruction-nodes do not change the path abstractions. At conditionals, we concatenate the label to outgoing edges. The label is marked
unless the guard-expression is pid -independent, as handled by the cdep-function. Each conditional in the control flow graph has a corresponding merge-node. At merge-nodes we restore the
path abstraction at the entry of the conditional by merging the two incoming path abstractions.
At while-statements we merge the path abstraction of the incoming edge (with the statements
label concatenated) with that of the back-edge, and concatenate the label of the node (possibly marked by cdep) on the outgoing value for the true-edge. The equations for branching
statements ensure that the path abstraction at their exit is the same as at their entry.
The solution to this equation system can be found by standard fix-point iteration, thanks
7

541

542	

Jakobsson
et al.Programs
/ Procedia Computer Science 108C (2017) 535‚Äì544
Replicated Synchronization forArvid
Imperative
BSP
Arvid Jakobsson et al.

In() = (X, )
if init(c) = , otherwise:
In() = Out(n())
c‚Ä≤ = [skip] or c‚Ä≤ = [sync]
Out(x()) = In()
In() = Out(n())
c‚Ä≤ = [y ‚à∂= e]
Out(x()) = (vdep((V, p), e, y), p) where (V, p) = In()
In() = Out(n())
c‚Ä≤ = if [e] then c1 else c2 [end]
Out(t()) = Out(f ()) = (V, p.cdep(, e, V )) where (V, p) = In()

In() = (Vt ‚à© Vf , pt ‚ñΩ pf )
c‚Ä≤ = if [e] then c1 else c2 [end]
where ((Vt , pt ), (Vf , pf )) = (Out(t()), Out(f ()))
Out(x()) = In()
In() = (Vn ‚à© Vb , pn . ‚ñΩ pb )
c‚Ä≤ = while [e] do c1 end
where ((Vn , pn ), (Vb , pb )) = (Out(n()), Out(b()))
Out(t()) = (V, p.cdep(, e, V ))) where (V, p) = In()
Out(f ()) = In()
Figure 8: Data-flow equations

Node 
or Edge (, ‚Ä≤ )

1
(1,2)
(2,3)
3
(3,4)
(4,5)
(5,6)
(5,7)
(6, 5m )
(7, 5m )
5m
(5m , 8)
(8,3)
 ‚àà {2, 4, 5, 6, 7}

In() or Out((, ‚Ä≤ ))

({I, X}, )
(vdep(In(1), (10), I), œÄ 2 (In(1)))
(vdep(2, (pid), X), œÄ 2 (In(2)))
(Vn ‚à© Vb , pn .3 ‚ñΩ pb ) where ((Vn , pn ), (Vb , pb )) = (Out(n(3)), Out(b(3)))
(œÄ 1 (In(3)), œÄ 2 (In(3)).cdep(3, (I), œÄ 1 (In(3)))
In(4)
(œÄ 1 (In(5)), œÄ 2 (In(5)).cdep(5, (X = 3), œÄ 1 (In(5))))
(œÄ 1 (In(5)), œÄ 2 (In(5)).cdep(5, (X = 3), œÄ 1 (In(5))))
In(6)
(vdep(In(7), (0), I), œÄ 2 (In(7)))
(Vt ‚à© Vf , pt ‚ñΩ pf ) where ((Vt , pt ), (Vf , pf )) = (Out(t(5m )), Out(f (5m )))
In(5m )
(vdep(In(8), (I - 1), I), œÄ 2 (In(8)))
Out(n())

Figure 9: Equation system P I(cnok ) with its solution

Solution
({I, X}, )
({I, X}, )
({I}, )
(‚àÖ, )
(‚àÖ, 3ÃÑ)
(‚àÖ, 3ÃÑ)
(‚àÖ, 3ÃÑ.5ÃÑ)
(‚àÖ, 3ÃÑ.5ÃÑ)
(‚àÖ, 3ÃÑ.5ÃÑ)
(‚àÖ, 3ÃÑ.5ÃÑ)
(‚àÖ, 3ÃÑ)
(‚àÖ, 3ÃÑ)
(‚àÖ, 3ÃÑ)
-

to the lattice structure of L. Figure 9 contains the equation system of cnok and its solution.
This section has described a data-flow analysis which under-approximates the set of pid independent variables in a program. The correctness of this analysis has been formally proven:
for each possible execution in the BSPlite semantics, we show that each pid -independent variable
in the final abstract state computed by the analysis has the same value in all processors in the
final concrete state. We refer to our coming research report [7] for the details.

4.2

Replicated synchronization analysis

With the result of the data-flow analysis pi of a program c, it is simple to verify that a program has replicated synchronization by verifying that all sync-primitives are guarded by pid independent conditions. Let the sf ‚ôØ -predicate hold for programs which do not (syntactically)
contain the sync-primitive. Then, replicated synchronization RS is defined by the rules in
Figure 10.
A program c is has replicated synchronisation when we can derive the property RS(c, pi). It
8

	

Jakobsson
et al.Programs
/ Procedia Computer Science 108C (2017) 535‚Äì544
Replicated Synchronization forArvid
Imperative
BSP
Arvid Jakobsson et al.

RS(c1 , pi) RS(c2 , pi)
RS(c1 ; c2 , pi)

(replicated synchronization)

RS([skip] , pi)
(V, p) = pi()

RS([sync] , pi)

¬¨œÜd (b, V ) ‚áí sf ‚ôØ (c1 ) ‚àß sf ‚ôØ (c2 )

RS([X ‚à∂= e] , pi)

RS(c1 , pi)

RS(if [b] then c1 else c2 [end] , pi)
(V, p) = pi() ¬¨œÜd (b, V ) ‚áí sf ‚ôØ (c) RS(c, pi)


RS(c2 , pi)

m

RS(while [b] do c end, pi)

Figure 10: Replicated synchronization analysis
is then guaranteed to be free from synchronization errors in any initially replicated environment.
As expected, the property RS cannot be derived for cnok . No rule applies to the loop and
conditional at label 3 and 5, since they contain sync and their conditions are not replicated.
The correctness of the second part of the analysis is formally proven: in a program with
replicated synchronization, there is no execution in the operational semantics reaching the error
state. We refer to our forthcoming research report [7] for the details.

5

Related Work

Synchronization analysis of the parallel programs has been extensively studied for the purposes
of deadlock and data-race detection and optimization. Our survey covers static techniques.
The most relevant previous work on verifying barrier synchronization is proposed by Aiken
and Gay [2]. The authors propose a system for SPMD programs that verifies their synchronization pattern, based on structural correctness and single-valued expressions. This analysis has
been extended inter-procedurally and with barrier matching [13]. In [1], the author exploits that
barriers in Titanium (a Java-dialect) are statically guaranteed to be textually aligned. They
construct a May-Happen-in-Parallel relation and use it for data-race detection.
In terms of formal semantics, several works propose formal semantics for BSP languages.
The first one is [8] and gives algebraic laws for transformation and derivation. LOGS [3] is
an algebraic framework to reason about BSP programs. Allan Stewart et al. [11] explored
the semantics of BSP program both as a sequential composition of super-steps and a parallel
composition of processes. BSPA [10] is a process algebra with which one can describe BSP
programs, with a notion of enumerated parallel vectors and global synchronization. Another
approach is simulating imperative BSP programs by sequential WhyML-programs to prove
their correctness [5]. Our semantics differs from previous ones in that it is explicitly designed
to model the features of BSPlib most relevant to synchronization.

6

Conclusion and Future Work

We have proposed a formalization of the parallel BSPlite language, a simplified version of C
with BSPlib, with its operational semantics. We have formalized a restricted version of the
Barrier Inference static analysis, which identifies programs with the replicated synchronization
property. This property is a marker of good code quality, and a sufficient condition for the
9

543

544	

Jakobsson
et al.
/ Procedia Computer Science 108C (2017) 535‚Äì544
Replicated Synchronization forArvid
Imperative
BSP
Programs
Arvid Jakobsson et al.

absence of synchronization errors, to which BSPlib-programs are susceptible. Additionally, the
simplified synchronization pattern of such programs leads us to suspect that future program
analyses will be simplified as a result.
We have implemented the analysis as a plug-in to the Frama-C analysis platform. Initial
experimentation suggests that the analysis performs well, with results in seconds for programs
ranging up to hundreds of lines. There is no combinatorial explosion due to thread interleavings:
the size of the abstract domain of the analysis is a function of the size of the program, and the
result of the analysis is true for any number of processors executing the program.
We are currently working on extending the formalism and implementation to handle pointers
and interprocedurality, in the goal of analysing realistic BSPlib programs. This plug-in is part
of an envisioned suite of Frama-C plug-ins dedicated to analysis of BSPlib-programs written in
C, for a secure, statically verified basis of BSPlib programming. A full version of this article,
including proofs, will be available as a research report [7].

References
[1] K. Y. A. Kamil. Concurrency analysis for parallel programs with textually aligned barriers. In
International Workshop on Languages and Compilers for Parallel Computing. ACM, 2005.
[2] A. Aiken and D. Gay. Barrier inference. In 25th ACM SIGPLAN Symposium on Principles of
Programming (POPL), pages 342‚Äì354. ACM, 1998.
[3] Y. Chen and J. W. Sanders. Logic of global synchrony. ACM Trans Program Lang Syst, 26(2):221‚Äì
262, 2004.
[4] B. D. de Dinechin, R. Ayrignac, P. E. Beaucamps, P. Couvert, B. Ganne, P. G. de Massas,
F. Jacquet, S. Jones, N. M. Chaisemartin, F. Riss, and T. Strudel. A clustered manycore processor
architecture for embedded and accelerated applications. In 2013 IEEE High Performance Extreme
Computing Conference (HPEC), pages 1‚Äì6, Sept 2013.
[5] J. Fortin and F. Gava. From BSP routines to high-performance ones: Formal verification of a
transformation case. In International Conference on Computational Science (ICCS), volume 1,
pages 155‚Äì164, 2010.
[6] J. M. D. Hill, B. McColl, D. C. Stefanescu, M. W. Goudreau, K. Lang, S. B. Rao, T. Suel,
T. Tsantilas, and R. Bisseling. BSPlib: The BSP Programming Library. Parallel Computing,
24:1947‚Äì1980, 1998.
[7] A. Jakobsson, F. Dabrowski, W. Bousdira, F. Loulergue, and G. Hains. Replicated Synchronization
for Imperative BSP programs. Technical Report RR-2017-02, LIFO, University of OrleÃÅans, 2017.
[8] H. Jifeng, Q. Miller, and L. Chen. Algebraic laws for BSP programming. In L. BougeÃÅ, P. Fraigniaud,
A. Mignotte, and Y. Robert, editors, Euro-Par‚Äô96. Parallel Processing, number 1123‚Äì1124 in
LNCS, Lyon, August 1996. LIP-ENSL, Springer.
[9] F. Kirchner, N. Kosmatov, V. Prevosto, J. Signoles, and B. Yakobowski. Frama-C: A software
analysis perspective. Formal Asp. Comput., 27(3):573‚Äì609, 2015.
[10] A. Merlin and G. Hains. A bulk synchronous process algebra. Computer Languages, Systems and
Structures, 33(3-4):111‚Äì133, 2007.
[11] A. Stewart, M. Clint, and J. GabarroÃÅ. Axiomatic Frameworks for Developing BSP-Style Programs.
Parallel Algorithms and Applications, 14:271‚Äì292, 2000.
[12] L. G. Valiant. A bridging model for parallel computation. Commun. ACM, 33(8):103, 1990.
[13] Y. Zhang and E. Duesterwaki. Barrier matching for programs with textually unaligned barriers.
In 12th ACM SIGPLAN symposium on Principles and practice of parallel programming (PPoPP),
pages 194‚Äì204. ACM, 2007.

10

