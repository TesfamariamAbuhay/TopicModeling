Procedia Computer
Science
Procedia Computer
Procedia
ComputerScience
Science001(2010)
(2012)1‚Äì10
2629‚Äì2638

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Enabling HMMER for the Grid
with COMP Superscalar
Enric Tejedora,b , Rosa M. Badiab,c , Romina Royob,d , Josep L. Gelp¬¥ƒ±b,d,e
a Universitat Polit`
ecnica de Catalunya
Jordi Girona, 34. 08034 Barcelona, Spain
b Barcelona Supercomputing Center
Jordi Girona, 29. 08034 Barcelona, Spain
c Consejo Superior de Investigaciones Cient¬¥ƒ±Ô¨Åcas
Serrano, 117. Madrid E-28006, Spain
d National Institute of Bioinformatics
e Dept of Biochemistry and Molecular Biology (Universitat de Barcelona)
& Institute of Research on Biomedicine

Abstract
The continuously increasing size of biological sequence databases has motivated the development of analysis
suites that, by means of parallelization, are capable of performing faster searches on such databases. However, many
of these tools are not suitable for execution on mid-to-large scale parallel infrastructures such as computational Grids.
This paper shows how COMP Superscalar can be used to eÔ¨Äectively parallelize on the Grid a sequence analysis
program. In particular, we present a sequential version of the HMMER hmmpfam tool that, when run with COMP
Superscalar, is decomposed into tasks and run on a set of distributed resources, not burdening the programmer with
parallelization eÔ¨Äorts.
Although performance is not a main objective of this work, we also present some test results where COMP
Superscalar, using a new pre-scheduling technique, clearly outperforms a well-known parallelization of the hmmpfam
algorithm.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
‚Éù

Keywords: Parallel programming models, Bionformatics applications

1. Introduction
Over the last years the size of biological sequence databases has grown exponentially, now containing millions of
genes and proteins that are freely available to researchers over the Internet. Examples of such datasets are GenBank
and RefSeq, which can be both accessed on the NCBI web site [1]. In order to process these biological data many
programs have appeared so far. One of the most used is the HMMER suite [2], a set of tools that provide diÔ¨Äerent
functionalities for protein sequence analysis.
Email addresses: etejedor@ac.upc.edu (Enric Tejedor), rosa.m.badia@bsc.es (Rosa M. Badia), romina.royo@bsc.es (Romina
Royo), gelpi@bsc.es (Josep L. Gelp¬¥ƒ±)

c 2012 Published by Elsevier Ltd.
1877-0509 ‚Éù
doi:10.1016/j.procs.2010.04.296

Open access under CC BY-NC-ND license.

2630

E. Tejedor/ Procedia
et al. / Procedia
Computer
Science
(2012) 2629‚Äì2638
Computer
Science 00
(2010) 11‚Äì10

2

Along with the proliferation of sequence data, a need has arisen for techniques that are able to analyse these large
databases in a reasonable amount of time. According to the current trends in the computer industry [3], the response
to this need is no longer to improve the performance of a single CPU, but to make use of multi-core architectures and
distributed-memory infrastructures: the inherent concurrency of sequence-search computations must be exploited.
In this sense, the COMP Superscalar framework (COMPSs) [4] can easily bring serial sequence-search tools to a
parallel scenario with minimal eÔ¨Äort from the programmer. COMPSs is basically composed of a programming model
and an execution runtime. In the COMPSs programming model, the user selects the application methods to be run on
one of the available resources instead of locally. When executing the application, the COMPSs runtime is responsible
from automatically detecting calls to these methods and processing them as remote tasks, i.e. checking their data
dependencies and scheduling their concurrent execution on distributed parallel resources (e.g. Grids).
To exemplify the usage of COMPSs, this paper presents a Java application for protein sequence comparison that is
based on hmmpfam, a widely used tool included in the HMMER analysis suite. From the relationship between the Life
Sciences department of the Barcelona Supercomputing Center (BSC) and the European Bioinformatics Institute (EBI),
it came to our knowledge that EBI was interested in performing huge runs of hmmpfam, which is computationally
expensive, on MareNostrum, a supercomputer available at BSC. In order to speed up that computation, we developed a
Java version of hmmpfam that was intended to be run with COMPSs. Here we describe its design and implementation,
how the tasks are selected for remote execution and the performance that is achieved.
The aim of this work is to demonstrate how COMPSs, i.e. its programming model and runtime, can help program
and parallelize a real application in a straightforward way. For that purpose, we present a parallelization of the
hmmpfam tool following the guidelines of the COMPSs programming model. Besides, we show how we can achieve
satisfactory results for huge tests on a large-scale infrastructure like the MareNostrum supercomputer. However,
our objective here is not to reach an optimal performance; as will be discussed later, there exist several previous
approaches that achieve better speedups, but unlike COMPSs they are tuned for a speciÔ¨Åc parallel environment or
modify the original hmmpfam code to make it more eÔ¨Écient. Finally, an important contribution of this work is the
addition of a pre-scheduling technique to the COMPSs runtime and the study of its beneÔ¨Åts.
The remainder of this paper is organized as follows. Section 2 discusses some related work. Section 3 gives an
overview of the COMP Superscalar framework. Section 4 presents the HMMER analysis suite and the hmmpfam tool.
Section 5 describes the basic steps followed to program a Java hmmpfam that runs with COMPSs. Section 6 shows
performance test results. Section 7 concludes the paper.
2. Related work
Several solutions have been proposed to accelerate the computation of the HMMER suite.
Some of these approaches are based on the usage of a particular hardware. [5] modiÔ¨Åes the most time consuming
part of the hmmpfam and hmmsearch programs with SIMD instructions of the Intel SSE2 instruction set. ClawHMMER [6] is a transformation of the original hmmsearch Viterbi algorithm into a streaming algorithm which runs
eÔ¨Éciently in a single GPU and even in a small cluster of GPUs. DeCypherHMM [7] achieves excellent performance
by means of FPGAs. DiÔ¨Äerently from all these examples, which have little or no portability, COMPSs does not
require any especial hardware platform to run. Moreover, such kind of solutions normally have huge development
times, whereas COMPSs oÔ¨Äers a simple programming model which facilitates the development process.
There has also been some work to parallelize HMMER over commodity distributed-memory infrastructures. The
HMMER suite comes with a PVM (Parallel Virtual Machine [8]) implementation for hmmpfam and other tools.
Another example is the already discussed MPI-HMMER [9]. However, none of these applications is able to reach the
same performance as COMPSs-HMMPfam.
A closely related work to MPI-HMMER is [10], where the authors present an enhanced version that improves
its scalability by means of a parallel Ô¨Åle system and techniques like database caching. Similarly, another port of
hmmpfam to MPI is described in [11]; this time, the implementation is tuned for a massively parallel architecture,
the IBM BlueGene/L, and it follows a multiple-master model to avoid master bottlenecks. Unlike COMPSs, these
approaches optimize the execution of HMMER for a speciÔ¨Åc parallel environment. Therefore, they cannot handle the
heterogeneity of Grids like COMPSs does. Moreover, neither of them report results of huge tests that take several
hours, as we do.

E. Tejedor et al./ Procedia
/ Procedia
Computer
Science
1 (2012)
Computer
Science
00 (2010)
1‚Äì102629‚Äì2638

2631
3

SledgeHMMER [12] is a web server that allows to run a parallelized version of hmmpfam on the Pfam HMM
database. It also has a cache of pre-calculated entries to speed up the searches. The whole database is read into
memory once per node, which is a drawback if the database is too large to Ô¨Åt in it. HMMPfam-COMPSs, on the
contrary, is able to segment both the query sequences Ô¨Åle and the database to prevent such situation.
Regarding other bioinformatic tools, the most important family of sequence-database search algorithms, BLAST [13],
has originated many diverse parallel implementations, e.g. mpiBLAST [14] or G-BLAST [15]. An analogous work to
ours can be found in [16], which describes the implementation of the fastDNAml application with GRID superscalar
and demonstrates the validity of our programming model for this kind of applications.
3. COMP Superscalar overview
COMP Superscalar (COMPSs) is a new version of a framework called GRID superscalar [17], and it diÔ¨Äers from
its predecessor in two main aspects. First, COMPSs oÔ¨Äers a programming model that is especially tailored for Java
applications. Second, the runtime of COMPSs is formed by a set of components, where each of them is concerned
with a diÔ¨Äerent functionality and collaborates to run the application on the Grid.
COMPSs makes use of two technologies: ProActive [18] and JavaGAT[19]. The former is a library that COMPSs
uses to build the components of the runtime. The latter is a uniform interface which allows to access numerous types
of Grid middleware; it is used by COMPSs to submit jobs to remote resources and transfer Ô¨Åles between them. On
top of these technologies, COMPSs provides many key features, e.g. application instrumentation, automatic task
generation, task dependency analysis, task scheduling and Ô¨Åle management.
3.1. Programming model
The main objective of COMPSs is to facilitate the development of Grid-unaware applications, i.e. those that have
no prior knowledge about the Grid. By means of COMPSs, such applications are able to transparently exploit the
Grid resources. For that purpose, COMPSs provides a straightforward programming model which only requires the
user to perform two steps: Ô¨Årst, selecting the parts of the application that will be executed on the Grid (the so-called
tasks) and, second and optionally, using very few API methods.
More precisely, the Ô¨Årst step consists in specifying which tasks must be taken into account by COMPSs, that
is, which methods called from the application code will be actually executed on the Grid. This selection is done
by providing a Java interface which declares these methods, along with Java annotations [20] that specify necessary
metadata about the tasks.
Concerning the second step, it involves making the application invoke the runtime of COMPSs. The runtime
must be started and stopped, receive the tasks to submit to the Grid and know about the Ô¨Åles that the application
accesses. This can be accomplished by calling up to 3 API methods from the application code, but the user also has
the possibility to leave the application completely unchanged, while COMPSs does all the work. This is possible
through a custom Java class loader which instruments the application and, on the Ô¨Çy, inserts the necessary calls in the
original code. For instance, whenever the application calls a selected method, such call is replaced by an invocation
to the COMPSs API which will create a remote task for that method.
The programming model of COMPSs will be illustrated with the Java hmmpfam application in Section 5.
3.2. Runtime Execution
The runtime of COMPSs is in charge of optimizing the perfomance of the application by exploiting its inherent
concurrency. In short, the runtime receives the tasks from the application, checks the dependencies between them and
decides which ones can be run at every moment and where, considering task constraints and performance aspects.
As explained above, the application is instrumented at execution time, replacing local invocations to the userselected methods by the creation of COMPSs tasks. The runtime of COMPSs receives these task creation requests,
and for each one it creates a node on a task dependency graph. Besides, it discovers the data dependencies between
each task and all previous ones. Such dependencies are the edges of the graph, and must be respected when running
the tasks.
Tasks with no dependencies pass to the next step: the scheduling. In this phase, a task is assigned to one of the
available resources (the workers). This decision is made according to a scheduling algorithm that takes into account

2632

E. Tejedor/ Procedia
et al. / Procedia
Computer
Science
(2012) 2629‚Äì2638
Computer
Science 00
(2010) 11‚Äì10

4

data locality and task constraints. Next, the input Ô¨Åles for the scheduled task are sent to the destination host. Once this
is done, the task can be submitted for remote execution. Once a task Ô¨Ånishes, the task dependency graph is updated,
possibly resulting in newly dependency-free tasks to send for scheduling.

Task
Analyser

Task
Scheduler

Job
Manager

File Manager
File
Information
Provider

File
Transfer
Manager

Figure 1: COMPSs runtime components. Each of the components encompasses a particular functionality that contributes to the overall execution
of the application on the Grid. These functionalities are task dependency analysis, task scheduling, job management and Ô¨Åle management.

3.2.1. Task Pre-Scheduling
One important new feature that the COMPSs runtime implements is pre-scheduling: tasks are pre-scheduled on
processors that are already running a task at a given moment, so that the transfers for the pre-assigned tasks are
triggered beforehand; later, when the processor gets free, the next task can be submitted immediately, without having
to wait for the necessary transfers.
This technique aims to overlap computation and communication as much as possible, and to distribute the load of
the master (i.e. the COMPSs runtime) all along the execution, avoiding ‚Äòhot spots‚Äô when many transfers have to be
performed. Its beneÔ¨Åts will be evaluated in Section 6.
4. The HMMER analysis suite and hmmpfam
The HMMER suite contains several tools for protein sequence analysis. It is based on proÔ¨Åle hidden Markov
models (proÔ¨Åle HMMs) [21], which are statistical models of multiple sequence alignments. Each HMM represents a
protein family, and captures position-speciÔ¨Åc information about how conserved each column of the alignment is and
which residues are likely.
One of the most important programs in the HMMER collection is hmmpfam. This tool reads a sequence Ô¨Åle and
compares each sequence in it, one at a time, against a database of HMMs, searching for signiÔ¨Åcantly similar sequence
matches with each model. The work performed by hmmpfam is computationally intensive and embarassingly parallel,
which makes it a good candidate to beneÔ¨Åt from COMPSs.
5. HHMPfam: a Java implementation of hmmpfam
This section presents HMMPFam, a Java application that helps to parallelize the hmmpfam computation by segmenting both the sequences and the database Ô¨Åles. Next subsections describe the main algorithm, illustrate the basic
steps of the COMPSs programming model by applying them to HMMPfam and give some details of the internal task
processing in the COMPSs runtime.
5.1. The sequential HMMPfam algorithm
The algorithm of HMMPfam is divided in three main phases. It is worth noting that this algorithm was programmed in a totally sequential fashion, that is to say, no parallelization code was written at all.
‚Ä¢ Segmentation: the query sequences Ô¨Åle, the database Ô¨Åle or both are split, depending on the number of processors
and memory available.

E. Tejedor et al./ Procedia
/ Procedia
Computer
Science
1 (2012)
Computer
Science
00 (2010)
1‚Äì102629‚Äì2638

2633
5

‚Ä¢ Execution: hmmpfam is run for each pair of sequence-database fragments.

‚Ä¢ Reduction: the partial outputs for each pair of sequence-database fragments are merged into a Ô¨Ånal result Ô¨Åle.
5.2. Applying the COMP Superscalar programming model
HMMPfam was intended to be run with COMPSs, which would be in charge of parallelizing it on the Grid. Next
we explain how we applied the two basic steps of the COMPSs programming model to HMMPfam, following the
guidelines already presented in Section 3.
public interface HMMPfamItf {
@ClassName("hmmer.HMMPfamImpl")
void hmmpfam(
@ParamMetadata(type = Type.STRING, direction
String hmmpfamBin,
@ParamMetadata(type = Type.FILE, direction =
String seqFile,
@ParamMetadata(type = Type.FILE, direction =
String dbFile,
@ParamMetadata(type = Type.FILE, direction =
String resultFile
);

= Direction.IN)
Direction.IN)
Direction.IN)
Direction.OUT)

@ClassName("hmmer.HMMPfamImpl")
void merge(
@ParamMetadata(type = Type.FILE, direction = Direction.INOUT)
String resultFile1,
@ParamMetadata(type = Type.FILE, direction = Direction.IN)
String resultFile2
);
}
Figure 2: Annotated interface for HMMPfam. The two declared methods, hmmpfam and merge, are selected to be executed on the Grid.

5.2.1. Selecting the tasks
For the Ô¨Årst step, a Java interface which declares the methods to be executed remotely (i.e. the future tasks) must
be provided. Figure 2 shows such Java interface for HMMPfam.
Given an application, the most suitable methods to be run on the Grid are the computationally expensive ones. In
this sense, we selected in the Ô¨Årst place the hmmpfam method, which calls the HMMER hmmpfam binary for a given
pair of sequence-database fragments. Moreover, some metadata is speciÔ¨Åed as Java annotations: at method level, the
name of the class that contains the implementation of the method and, at parameter level, the type and direction of
each parameter. Concerning the merge method, it merges the output Ô¨Åles from two hmmpfam tasks. We chose these
two methods for remote execution to parallelize as much as possible the reduction phase, and to avoid it causing a
bottleneck in the master node when a lot of Ô¨Åles have to be merged.
5.2.2. The application code
The second step involves making the HMMPfam application invoke the COMPSs runtime. For that purpose, we
chose to keep the application code completely unchanged, and let the COMPSs class loader do all the work. As programmers, we did not have to bother about including any Grid-related invocations in the code nor about parallelization
at all.
Consequently, COMPSs is able to take the original HMMPfam, which is totally sequential, and automatically run
the selected parts on the Grid while exploiting their concurrency.

2634

E. Tejedor/ Procedia
et al. / Procedia
Computer
Science
(2012) 2629‚Äì2638
Computer
Science 00
(2010) 11‚Äì10

6

5.3. Running HMMPfam with COMP Superscalar
When launching the execution of HMMPfam with COMPSs, the application is instrumented and the necessary
calls to the runtime are inserted. After that, the COMPSs runtime begins to receive requests for the creation of new
remote tasks. As a result to such requests, the runtime generates a task dependency graph that looks like the one
depicted in Figure 3. The nodes represent the invocations of methods that have been selected as remote (the tasks),
and the edges are the data dependencies between them.
The segmentation phase is entirely done in the local host, so it does not generate any task. The uppermost part of
the graph in Figure 3 corresponds to the execution phase, with a column of totally parallel tasks that run hmmpfam
for each pair of database-sequence fragments. Below, the reduction phase merges all the outputs into a single result
Ô¨Åle.
DB fragment 1

hmmpfam

hmmpfam

merge

DB fragment 2

hmmpfam

hmmpfam

hmmpfam

hmmpfam

merge

merge

merge

hmmpfam

hmmpfam

merge

merge

merge

Figure 3: Example of a task dependency graph generated by HMMPfam when running it with COMPSs. In this case, the database is split in two
fragments and the query sequences Ô¨Åle in four parts. This creates eight independent tasks that run hmmpfam on a given pair of database-sequence
fragments. After that, there are three levels of reduction tasks, the last one merging the results from the two diÔ¨Äerent database fragments.

In order to improve the performance of the application, the tasks are scheduled on resources so that the parallelism
exhibited by the dependency graph is exploited as much as possible.
6. Tests
This section evaluates the performance of HMMPfam-COMPSs. All the tests were carried out in the MareNostrum
supercomputer, equipped with IBM PowerPC 970MP processors at 2.3 GHz, which are organised in JS21 blades of
4 processors, 8 GB RAM and 36 GB of local storage. 280 TB of additional shared storage are provided via a Global
Parallel File System (GPFS). The interconnection network used was a Gigabit Ethernet.
6.1. Speedup and scalability
In these series of tests we measure the speedup and scalability of COMPSs running HMMPfam. Besides, we run
the same experiments with a reference parallel implementation of hmmpfam, included in the MPI-HMMER suite [9].
This MPI implementation is also based on a master-worker paradigm for work dispatching. Each worker must have a
copy of the HMM database available either locally or via a network storage. The master distributes to the workers the
sequences and the indexes of the HMMs to compute, and post-processes the results from all workers.
Concerning the common execution parameters, we used Superfamily as the HMM database and a set of 5000
sequences produced for benchmarking by our partners of the EBI [22]. The mean sequence length is similar to that of
the whole UniParc database [23].

E. Tejedor et al./ Procedia
/ Procedia
Computer
Science
1 (2012)
Computer
Science
00 (2010)
1‚Äì102629‚Äì2638

2635
7

Speedup

In the case of COMPSs, we used the SSH JavaGAT [19] adaptor to perform job submission and Ô¨Åle transfer operations. In every execution of HMMPfam-COMPSs, both the HMM database and the sequences Ô¨Åle are initially located
on the shared storage (GPFS); when starting the execution, the database and/or the sequences Ô¨Åle are segmented by
HMMPfam and the fragments are put in the local storage of the master. From that moment on, the fragments are
transferred via SSH from the master to the local storage of a worker or between workers, depending on the scheduling
of the tasks, which takes into account data locality. We did not store the (fragments of) database or sequences in
GPFS because of performance issues when a number of nodes is accessing the same shared GPFS Ô¨Åle. Besides, the
I/O-bound nature of hmmpfam would make the problem worse and would prevent HMMPfam from scaling. For the
same reason, in the case of MPI-HMMER we pre-distributed the HMM database Ô¨Åle to the local store of each worker
node before the execution.
Regarding the segmentation strategy in HMMPfam-COMPSs, the application produced fragments of the sequences Ô¨Åle, with a segmentation factor adjusted on the basis of previous experiments, in order to obtain a good
tradeoÔ¨Ä between the overhead of processing more tasks and the load balancing that is done among the resource nodes.
140
120
100
80
60
40
20
0

COMPSs
MPI-HMMER

8

16

32

64

128 256

Number of worker processors

Figure 4: Performance comparison between HMMPfam-COMPSs and MPI-HMMER hmmpfam

Figure 4 compares the performance of HMMPfam-COMPSs and MPI-HMMER hmmpfam. The baseline of the
speedups is a sequential run of the hmmpfam binary.
On the one hand, Figure 4 shows how MPI-HMMER hmmpfam scales poorly beyond 64 nodes, mainly because
of the excessive communication between the master and the workers. This was previously stated in [10] and [11],
which propose enhanced versions of the HMMER suite that achieve better performance than the original one. We did
not choose any of these works to establish a comparison with COMPSs because they use a particular infrastructure
to improve their results, e.g. a parallel Ô¨Åle system. On the contrary, COMPSs relies on standard I/O, makes use of
commodity hardware and it is not tuned for any concrete platform; therefore, we found that the comparison with
MPI-HMMER was more adequate.
On the other hand, COMPSs-HMMPfam exhibits a good scalability up to 256 worker processors, which contrasts
with the results of MPI-HMMER hmmpfam. This is even more remarkable if we take into account that COMPSs
transfers the database Ô¨Åle, which is about 370 MB, to all the worker nodes during the execution. On the contrary, the
times of MPI-HMMER hmmpfam does not include the necessary pre-distribution of the database to all the workers
before execution.
Concerning the speedup of COMPSs-HMMPfam, the results are satisfactory, achieving a maximum speedup of
about 100 with 256 workers. It is worth noting that, in a recent parallelization of MPI-HMMER [10], the authors only
reach a speedup of 80 without modifying the hmmpfam binary to cache the database. The reason why the speedup
gets worse beyond 64 workers is the overload that the master experiences, especially for the 256-workers case, due
to the limitations of a single-master model. For larger numbers of workers, the amount of computation per worker
is smaller; in this scenario, the workers get underused because the master is unable to provide them with tasks/Ô¨Åles
quickly enough. Moreover, the lower number of tasks per CPU makes the execution more susceptible to unbalance.
This issue was addressed in [11], and could be solved by incorporating several masters in a COMPSs run.

2636

E. Tejedor/ Procedia
et al. / Procedia
Computer
Science
(2012) 2629‚Äì2638
Computer
Science 00
(2010) 11‚Äì10

8

6.2. BeneÔ¨Åts of Pre-Scheduling

50

No Pre-sched
Pre-sched

Percentage

40
30
20
10
0

	

One of the key factors to achieve good performance results in COMPSs is the pre-scheduling technique that it
features, already described in Section 3.2.1. During the execution of a COMPSs application, the master becomes idle
when all the dependency-free tasks have been scheduled and submitted to their destination host. Later, when a task
Ô¨Ånishes, the master leaves its idle state to update the dependency graph and possibly send a new task to the freed
resource, along with its input Ô¨Åles.
Pre-scheduling makes the most of the master inactivity periods, assigning tasks to busy resources and pre-transferring
the Ô¨Åles that they need to these resources. Thus, the overlapping of computation and communication in the workers is
maximized.























	


8

16 32 64 128 256

Number of worker processors

Figure 5: Percentage of Idle+Transferring time in the workers,
with respect to the total of Idle+Transferring+Computing, with
and without pre-scheduling (Pre-sched). When applying prescheduling, the I+T percentage is generally lower.

 !"#
 !"#
 !"#

 !"#
 !"#
 !"#

Figure 6: Number of concurrent transfers that the COMPSs is performing during the Ô¨Årst 500 seconds of several executions, varying the number of worker
processors (16, 64, 256) and applying pre-scheduling (Pre-sched) or not. Prescheduling keeps the master busy (transferring) longer, except in case of overload.

Figure 5 shows how pre-scheduling contributes to reduce the weight of non-computational periods in the workers,
for several runs of COMPSs-HMMPfam with diÔ¨Äerent numbers of worker processors. During a COMPSs execution,
a worker processor can be in three states:
‚Ä¢ Idle (I): the worker is inactive, i.e. it is not running any task nor receiving any Ô¨Åle.
‚Ä¢ Transferring (T): the worker is receiving one or more input Ô¨Åles for a task.
‚Ä¢ Computing (C): the worker is running a task.
On Figure 5, the percentage of non-computational time in the workers (I+T) is calculated with respect to the total
time (I+T+C). The lower is the I+T percentage, the higher is the utilization of the workers. As expected, the weight
of I+T increases along the X axis due to two factors: Ô¨Årst, the more worker processors, the least data locality is
achieved and the more transfers are necessary; second, a higher number of worker processors also increases the load
of the master, which causes larger idle periods on workers that have Ô¨Ånished a task and are waiting for the next one.
However, when pre-scheduling is activated the I+T percentage is smaller. This happens because more transfers are
overlapped with computation, and a processor that gets free can receive sooner a new (pre-scheduled) task, without
having to wait for any transfer at that point. Such statement is not true for the case of 256 workers, when the overload
of the master prevents it from applying pre-scheduling: it continuously has newly idle workers to which transfer Ô¨Åles
and submit tasks.

E. Tejedor et al./ Procedia
/ Procedia
Computer
Science
1 (2012)
Computer
Science
00 (2010)
1‚Äì102629‚Äì2638

2637
9

Figure 6 illustrates how pre-scheduling helps to distribute the load of the master more uniformly all along the
execution of COMPSs-HMMPfam. It depicts, for diÔ¨Äerent numbers of worker processors, the average number of
concurrent transfers that the master is handling during the 500 Ô¨Årst seconds of the execution, both for pre-scheduling
and no pre-scheduling. For 16 workers, the pre-schedule line falls about 50 seconds later than the other one, due to
the pre-transfers that the master performs. The diÔ¨Äerence between the two lines is more remarkable for 64 workers,
because the master has more workers to which transfer input Ô¨Åles of pre-scheduled tasks. In the case of 256 workers,
however, there is no noticeable diÔ¨Äerence between the two lines: no pre-scheduling is actually done, again because
the overloaded master is never idle.
7. Conclusions and future work
This paper has presented HMMPfam, a Java application that acts as a wrapper of the widely used hmmpfam, a
tool for protein sequence analysis that belongs to the HMMER suite. We have demonstrated how the sequential code
of HMMPfam can be easily parallelized with the help of the COMP Superscalar framework. COMPSs provides a
simple programming model that does not oblige the programmer to modify the application with Grid-related calls.
The only requirement is to select the methods of the application that will be run on the Grid, and COMPSs will take
care of improving its performance by means of concurrency exploitation.
Although the optimal performance is not a main objective of this work, the results of the tests on HMMPfam show
that it is able to achieve a good scalability up to a considerable number of processors, beating the reference parallel
implementation based on MPI for commodity clusters. Moreover, in comparison to other approaches, COMPSsHMMPfam has an excellent tradeoÔ¨Ä between programming productivity and performance obtained.
Besides the experiments reported here, COMPSs-HMMPfam has been used to perform huge tests, processing more
than 7 million protein sequences and involving nearly 10000 processors. This fact demonstrates that the application
is working and ready to target executions with large databases and query sequences Ô¨Åles.
Future work on this topic will include improving the speedup obtained with HMMPfam, in order to reduce the
execution times of our users in MareNostrum. Also, given the success achieved with this application, we are open to
support more applications, either from the bioinformatics Ô¨Åeld or others, that could be suitable to run with COMPSs.
Acknowledgment
The authors gratefully acknowledge the Ô¨Ånancial support of the Comisi¬¥on Interministerial de Ciencia y Tecnolog¬¥ƒ±a
(CICYT, Contract TIN2007-60625), the Generalitat de Catalunya (2009-SGR-980), the UPC Recerca grant and the
XtreemOS IP project funded by the EC.
References
[1] NCBI web site. URL: http://www.ncbi.nlm.nih.gov.
[2] HMMER: biosequence analysis using proÔ¨Åle hidden Markov models. URL: http://hmmer.janelia.org.
[3] K. Asanovic, R. Bodik, B. C. Catanzaro, J. J. Gebis, P. Husbands, K. Keutzer, D. A. Patterson, W. L. Plishker, J. Shalf, S. W. Williams,
K. A. Yelick, The Landscape of Parallel Computing Research: A View from Berkeley, Tech. Rep. UCB/EECS-2006-183, EECS Department,
University of California, Berkeley (Dec 2006).
URL http://www.eecs.berkeley.edu/Pubs/TechRpts/
2006/EECS-2006-183.html
[4] E. Tejedor, R. Badia, COMP Superscalar: Bringing GRID superscalar and GCM Together, in: 8th IEEE International Symposium on Cluster
Computing and the Grid, 2008.
[5] J. P. Walters, B. Qudah, V. Chaudhary, Accelerating the HMMER Sequence Analysis Suite Using Conventional Processors, in: AINA ‚Äô06:
Proceedings of the 20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA‚Äô06), IEEE
Computer Society, Washington, DC, USA, 2006, pp. 289‚Äì294. doi:http://dx.doi.org/10.1109/AINA.2006.68.
[6] D. R. Horn, M. Houston, P. Hanrahan, ClawHMMER: A Streaming HMMer-Search Implementation, in: SC ‚Äô05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing, IEEE Computer Society, Washington, DC, USA, 2005, p. 11.
doi:http://dx.doi.org/10.1109/SC.2005.18.
[7] TimeLogic biocomputing solutions. DeCypherHMM. URL: http://www.timelogic.com.
[8] Parallel Virtual Machine. URL: http://www.csm.ornl.gov/pvm/.
[9] J. P. Walters, MPI-HMMER. URL: http://www.mpihmmer.org.

2638

E. Tejedor/ Procedia
et al. / Procedia
1 (2012) 2629‚Äì2638
ComputerComputer
Science 00Science
(2010) 1‚Äì10

10

[10] J. P. Walters, R. Darole, V. Chaudhary, Improving mpi-hmmer‚Äôs scalability with parallel i/o, Parallel and Distributed Processing Symposium,
International 0 (2009) 1‚Äì11.
[11] K. Jiang, O. Thorsen, A. Peters, B. Smith, C. P. Sosa, An EÔ¨Écient Parallel Implementation of the Hidden Markov Methods for Genomic
Sequence-Search on a Massively Parallel System, IEEE Transactions on Parallel and Distributed Systems 19 (1) (2008) 15‚Äì23.
[12] G. Chukkapalli, C. Guda, S. Subramaniam, SledgeHMMER: a web server for batch searching the Pfam database, Nucleic Acids Research 32
(1 July 2004) W542‚ÄìW544(1).
[13] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, D. J. Lipman, Basic Local Alignment Search Tool, Journal of Molecular Biology 215 (3)
(1990) 403‚Äì410.
[14] A. E. Darling, L. Carey, W.-c. Feng, The Design, Implementation, and Evaluation of mpiBLAST, in: ClusterWorld Conference & Expo and
the 4th International Conference on Linux Cluster: The HPC Revolution 2003, San Jose, California, 2003.
[15] C.-T. Yang, T.-F. Han, H.-C. Kan, G-blast: a grid-based solution for mpiblast on computational grids, TENCON 2007 - 2007 IEEE Region
10 Conference (2007) 1‚Äì5doi:10.1109/TENCON.2007.4429009.
[16] V. Dialinos, R. M. Badia, R. Sirvent, J. M. Perez, J. Labarta, Implementing phylogenetic inference with grid superscalar, in: CCGRID
‚Äô05: Proceedings of the Fifth IEEE International Symposium on Cluster Computing and the Grid (CCGrid‚Äô05) - Volume 2, IEEE Computer
Society, Washington, DC, USA, 2005, pp. 1093‚Äì1100.
[17] R. M. Badia, J. Labarta, R. Sirvent, J. M. P¬¥erez, J. M. Cela, R. Grima, Programming Grid Applications with GRID superscalar, Journal of
GRID Computing 1 (2) (2003) 151‚Äì170.
[18] D. Caromel, W. Klauser, J. Vayssiere, Towards Seamless Computing and Metacomputing in Java, Concurrency Practice and Experience
10 (11‚Äì13) (1998) 1043‚Äì1061, http://proactive.inria.fr.
[19] G. Allen, K. Davis, T. Goodale, A. Hutanu, H. Kaiser, T. Kielmann, A. Merzky, R. van Nieuwpoort, A. Reinefeld, F. Schintke, T. Sch¬®utt,
E. Seidel, B. Ullmer, The Grid Application Toolkit: Towards Generic and Easy Application Programming Interfaces for the Grid, in: Proceedings of the IEEE, Vol. 93, 2005, pp. 534‚Äì550.
[20] Java annotations. URL: http://java.sun.com/j2se/1.5.0/docs/guide/language/annotations.html.
[21] S. R. Eddy, ProÔ¨Åle hidden Markov models, Bioinformatics 14 (9) (1998) 755‚Äì763.
[22] A. Quinn, S. Hunter, HMMer Benchmarking for ELIXIR.
URL http://www.ebi.ac.uk/seqdb/confluence/display/InterPro/HMMer+benchmarking+for+ELIXIR
[23] UniProt Archive. URL: http://www.ebi.ac.uk/uniparc/.

